{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pyautogui\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import constants.constants as const\n",
    "import constants.file_handler_constants as fh\n",
    "from constants.accommodation_constants import *\n",
    "\n",
    "from packages.accommodation.accommodation import *\n",
    "from packages.file_handler_package.file_handler import *\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv, dotenv_values \n",
    "\n",
    "from selenium.webdriver import Remote, ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.actions.wheel_input import ScrollOrigin\n",
    "from selenium.webdriver import ActionChains\n",
    "\n",
    "from seleniumwire import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from selenium.webdriver.edge.options import Options\n",
    "\n",
    "\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_accommodation_df(accommodation: Accommodation) -> pd.DataFrame:\n",
    "    accommodation_dict = {\n",
    "        'name' : [accommodation.get_name()],\n",
    "        'description' : [accommodation.get_description()],\n",
    "        'latitude' : [accommodation.get_latitude()],\n",
    "        'longitude' : [accommodation.get_longitude()],\n",
    "        'imgPath' : [accommodation.get_imgPath()],\n",
    "        'phone': [accommodation.get_phone()],\n",
    "        'website': [accommodation.get_website()],\n",
    "        'star': [accommodation.get_star()],\n",
    "        'facility': [accommodation.get_facility()],\n",
    "        'tag': [accommodation.get_tag()],\n",
    "        'type': [accommodation.get_type()],\n",
    "\n",
    "        # location\n",
    "        'address' : [accommodation.get_location().get_address()],\n",
    "        'province' : [accommodation.get_location().get_province()],\n",
    "        'district' : [accommodation.get_location().get_district()],\n",
    "        'subDistrict' : [accommodation.get_location().get_sub_district()],\n",
    "        'province_code' : [accommodation.get_location().get_province_code()],\n",
    "        'district_code' : [accommodation.get_location().get_district_code()],\n",
    "        'sub_district_code' : [accommodation.get_location().get_sub_district_code()],\n",
    "\n",
    "        # rating\n",
    "        'score' : [accommodation.get_rating().get_score()],\n",
    "        'ratingCount' : [accommodation.get_rating().get_ratingCount()],\n",
    "    }\n",
    "\n",
    "    accommodation_df = pd.DataFrame(accommodation_dict)\n",
    "    \n",
    "    return accommodation_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_url_by_page(link_to_accommodation: str, page: int) -> str:\n",
    "\n",
    "    if(page == 1):\n",
    "        return link_to_accommodation\n",
    "    \n",
    "    first_page_url_split = link_to_accommodation.split('-')\n",
    "    nth_count_page = 'oa%s' % ((page - 1) * 30)\n",
    "    first_page_url_split[-2] = nth_count_page\n",
    "    res_page_url =  \"-\".join(first_page_url_split)\n",
    "\n",
    "    return res_page_url\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_img(accommodation_page_driver: webdriver) -> list[str]:\n",
    "    \n",
    "    res_imgPath = []\n",
    "\n",
    "    # possible_click_img_xpath = [\n",
    "    #     '//*[@id=\"lithium-root\"]/main/span/div[4]/div/div[2]/div[3]/div/div[1]/div[1]/div/div[1]/div/div[5]/div'\n",
    "    #     '//*[@id=\"lithium-root\"]/main/span/div[4]/div/div[2]/div[3]/div/div[1]/div[1]/div/div/div/div[5]/div'\n",
    "    # ]\n",
    "    \n",
    "    # btn_img_xpath = \"\"\n",
    "    # for cur_xpath in possible_click_img_xpath:\n",
    "    #     try:\n",
    "    #         WebDriverWait(accommodation_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, cur_xpath)))\n",
    "    #         btn_img_xpath = cur_xpath\n",
    "    #         break\n",
    "        \n",
    "    #     except Exception as e:\n",
    "    #         pass\n",
    "    \n",
    "    # if(not len(btn_img_xpath)):\n",
    "    #     print(\"can't scrape img (no img ?)\")\n",
    "    #     return ['']\n",
    "\n",
    "    # find button and click\n",
    "    # to see modal then scrape image address\n",
    "    try:\n",
    "        # WebDriverWait(accommodation_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, btn_img_xpath)))\n",
    "        # click_img_btn = accommodation_page_driver.find_element(By.XPATH, btn_img_xpath)\n",
    "        \n",
    "        # print(\"p1\")\n",
    "        # WebDriverWait(accommodation_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'QXsnf')))\n",
    "        print(\"p2\")\n",
    "        click_img_btn = accommodation_page_driver.find_element(By.CLASS_NAME, 'QXsnf')\n",
    "        print(\"p3\")\n",
    "        click_img_btn.click()\n",
    "        print(\"p4\")\n",
    "        is_end_scrape_img = False\n",
    "        cnt_retry = 0\n",
    "        print(\"p5\")\n",
    "        while(not is_end_scrape_img):\n",
    "            if(cnt_retry == 10):\n",
    "                print(\"max retry for scrape image...\")\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                WebDriverWait(accommodation_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'cfCAA')))\n",
    "                all_img_elements = accommodation_page_driver.find_elements(By.CLASS_NAME, 'cfCAA')\n",
    "                print(\"find image element -> \", len(all_img_elements))\n",
    "                for cur_img_element in all_img_elements:\n",
    "                    cur_bgImg_val = cur_img_element.value_of_css_property('background-image')\n",
    "                    match = re.search(r'url\\(\"(.*?)\"\\)', cur_bgImg_val)\n",
    "                    if match:\n",
    "                        res_imgPath.append(match.group(1))\n",
    "\n",
    "                is_end_scrape_img = True\n",
    "\n",
    "            except Exception as e:\n",
    "                cnt_retry += 1\n",
    "                print(\"retry scrape img...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "            pass\n",
    "    \n",
    "\n",
    "    return res_imgPath.copy()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_location(accommodation_page_driver: webdriver, latitude: float, longitude: float, province_th: str) -> Location:\n",
    "\n",
    "    # find better address description on wongnai\n",
    "    # for example: \"991 ถนนพระราม 1 Pathum Wan, กรุงเทพมหานคร (กทม.) 10330 ไทย\"\n",
    "    address_tripAdvisor = \"\"\n",
    "    possible_address_xpath = [\n",
    "        '//*[@id=\"lithium-root\"]/main/span/div[4]/div/div[1]/div[3]/div/div[3]/div[1]/div[2]/span[2]/span',\n",
    "    ]\n",
    "\n",
    "\n",
    "    for cur_address_xpath in possible_address_xpath:\n",
    "        try:\n",
    "            WebDriverWait(accommodation_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, cur_address_xpath)))\n",
    "            address_element = accommodation_page_driver.find_element(By.XPATH, cur_address_xpath)\n",
    "            address_tripAdvisor = address_element.text\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "\n",
    "    # start scrape location\n",
    "    res_location = Location()\n",
    "    cnt_retry = 0\n",
    "    try:\n",
    "        while(True):\n",
    "            if(cnt_retry == 10):\n",
    "                print(\"max retry for scrape Google Map ...\")\n",
    "                break\n",
    "            \n",
    "            # set up new webdriver to work googlemap url(query for specific lat/long)\n",
    "            possible_addressGoogleMap_elements = []\n",
    "            try:\n",
    "                # set Chrome options to run in headless mode\n",
    "                # options = Options()\n",
    "                options = webdriver.ChromeOptions()\n",
    "                options.add_argument(\"start-maximized\")\n",
    "                # options.add_argument(\"--headless=new\")\n",
    "                options.add_experimental_option(\n",
    "                    \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "                )\n",
    "\n",
    "                google_map_driver = webdriver.Chrome(options=options)\n",
    "                \n",
    "                google_map_query = \"https://www.google.com/maps/search/?api=1&query=%s,%s\" % (latitude, longitude)\n",
    "                google_map_driver.get(google_map_query)\n",
    "                print(\"scrape location data for, \", google_map_query)\n",
    "                \n",
    "                WebDriverWait(google_map_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'DkEaL')))\n",
    "                possible_addressGoogleMap_elements = google_map_driver.find_elements(By.CLASS_NAME, 'DkEaL')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"retry  scrape Google Map..\")\n",
    "                cnt_retry += 1\n",
    "                google_map_driver.close()\n",
    "                continue\n",
    "\n",
    "\n",
    "            # after init new webdriver -> continure scrape location data\n",
    "\n",
    "            # if found some wiered place that doesn't even have its address\n",
    "            # skip this case for now...\n",
    "            if(not len(possible_addressGoogleMap_elements)):\n",
    "                return res_location\n",
    "\n",
    "            subStrDistrict = \"อำเภอ\"\n",
    "            subStrSubDistrict = \"ตำบล\"\n",
    "\n",
    "            if province_th == \"กรุงเทพมหานคร\":\n",
    "                subStrDistrict = \"เขต\"\n",
    "                subStrSubDistrict = \"แขวง\"\n",
    "\n",
    "            district = 0\n",
    "            subDirstrict = 0\n",
    "\n",
    "            # find location\n",
    "            useData = None\n",
    "            for cur_element in possible_addressGoogleMap_elements:\n",
    "                if province_th in cur_element.text and cur_element.text.find(subStrDistrict) != -1:\n",
    "                    useData = cur_element.text.replace(\",\",\"\").replace(\"เเ\",\"แ\")\n",
    "                    break\n",
    "           \n",
    "            if(useData != None):\n",
    "                # print(\"Full Address :\",useData)\n",
    "                # another brute force way in case of province 'กรุงเทพหมานคร' not have word 'แขวง' in address\n",
    "                if(province_th == 'กรุงเทพมหานคร' and useData.find(subStrSubDistrict) == -1):\n",
    "                    subAddress_split = useData.split(' ')\n",
    "                    cur_province_Idx = subAddress_split.index(province_th)\n",
    "                    district = subAddress_split[cur_province_Idx - 1].replace(\"เขต\",\"\")\n",
    "\n",
    "                else:\n",
    "                    start_address_index = useData.find(subStrDistrict)\n",
    "                    subAddress = useData[start_address_index:]\n",
    "                    district = subAddress[subAddress.find(subStrDistrict)+len(subStrDistrict):subAddress.find(province_th)].replace(\" \",\"\")               \n",
    "\n",
    "                if district == \"เมือง\":\n",
    "                    district = district+province_th\n",
    "\n",
    "                # filter row to find 'ISO_3166_code', 'zip_code', 'geo_code'\n",
    "                geo_code_df = pd.read_csv(fh.PATH_TO_GEOCODE)\n",
    "                filtered_rows = geo_code_df[\n",
    "                    (geo_code_df['province_th'] == province_th) & (geo_code_df['district_th'] == district)\n",
    "                ]\n",
    "                filtered_rows.reset_index(inplace=True, drop=True)\n",
    "                \n",
    "                if not filtered_rows.empty:\n",
    "                    print(\"found province :\",filtered_rows.loc[0, 'ISO_3166_code'], province_th)\n",
    "                    print(\"found District :\",filtered_rows.loc[0, 'zip_code'], district)\n",
    "\n",
    "                    res_location.set_address(address_tripAdvisor if len(address_tripAdvisor) else useData)\n",
    "                    res_location.set_province(province_th)\n",
    "                    res_location.set_district(district)\n",
    "                    res_location.set_sub_district(\"\")\n",
    "                    res_location.set_province_code(filtered_rows.loc[0, 'ISO_3166_code'])\n",
    "                    res_location.set_district_code(filtered_rows.loc[0, 'zip_code'])\n",
    "                    res_location.set_sub_district_code(0)\n",
    "\n",
    "                else:\n",
    "                    print(\"not found province :\", province_th)\n",
    "                    print(\"not found District :\", district)\n",
    "\n",
    "                    res_location.set_address(address_tripAdvisor if len(address_tripAdvisor) else useData)\n",
    "                    res_location.set_province(province_th)\n",
    "                    res_location.set_district(district)\n",
    "                    res_location.set_sub_district(\"\")\n",
    "                    res_location.set_province_code(0)\n",
    "                    res_location.set_district_code(0)\n",
    "                    res_location.set_sub_district_code(0)\n",
    "\n",
    "            google_map_driver.close()\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"can't scrape location data\")\n",
    "\n",
    "    return res_location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape lat/long, and openingHours (there are in another page of current accommodation)\n",
    "def scrape_location_latlong_openingHours(accommodation_page_driver: webdriver, link_to_adjust_page: str) -> tuple[float, float]:\n",
    "    lat = 0\n",
    "    long = 0\n",
    "\n",
    "    # create new webdriver to continue scrape lat/long, openingHours in adjust accommodation page\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape single accommodation ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "\n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        adjust_page_driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # retry in case of web restrictions, some elements not loaded\n",
    "        try:\n",
    "            print(\"scrape data in adjust accommodation page...\")\n",
    "            print(\"for link : \", link_to_adjust_page)\n",
    "            adjust_page_driver.get(link_to_adjust_page)\n",
    "\n",
    "            print(\"debug option of adjust page: \")\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'DiHOR')))\n",
    "\n",
    "            # find dropdown --> click display data below --> cick display lat/long input form\n",
    "            possible_target_btn = adjust_page_driver.find_elements(By.CLASS_NAME, 'DiHOR')\n",
    "            for cur_dropdown_btn in possible_target_btn:\n",
    "                cur_dropdown_text = cur_dropdown_btn.text\n",
    "                if(\"แนะนำการแก้ไขข้อมูลของสถานที่นี้\" in cur_dropdown_text):\n",
    "                    print(\"found target dropdown btn ...\")\n",
    "                    cur_dropdown_btn.click()\n",
    "                    WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/button')))\n",
    "                    # find button click to display lat/long input form\n",
    "                    display_lat_long_btn = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/button')\n",
    "                    display_lat_long_btn.click()\n",
    "\n",
    "        except Exception as e:\n",
    "            cnt_retry += 1\n",
    "            adjust_page_driver.quit()\n",
    "            print(\"retry adjust page...\")\n",
    "            continue\n",
    "\n",
    "      \n",
    "        # find lat/long\n",
    "        try:\n",
    "            WebDriverWait(adjust_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[1]/div/div[2]/div/div/div/span')))\n",
    "            WebDriverWait(adjust_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[2]/div/div[2]/div/div/div/span')))\n",
    "    \n",
    "            lat_input_container = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[1]/div/div[2]/div/div/div/span')\n",
    "            lat_input_element = lat_input_container.find_element(By.TAG_NAME, 'input')\n",
    "            lat = float(lat_input_element.get_attribute('value'))\n",
    "\n",
    "            long_input_container = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[2]/div/div[2]/div/div/div/span')\n",
    "            long_input_element = long_input_container.find_element(By.TAG_NAME, 'input')\n",
    "            long = float(long_input_element.get_attribute('value'))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find lat/long\")\n",
    "        \n",
    "        print(\"lat : \", lat)\n",
    "        print(\"long : \", long)\n",
    "\n",
    "        # **if can't find lat/long --> don't scrape this attaction\n",
    "        if(lat == 0 and long == 0):\n",
    "            print(\"in scrape_location_latlong_openingHours --> can't find lat/long --> 0, 0\")\n",
    "            return lat, long\n",
    "\n",
    "        adjust_page_driver.quit()\n",
    "        break\n",
    "\n",
    "    return lat, long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_accommodation(link_to_accommodation: str, province_th: str) -> Accommodation:\n",
    "    \n",
    "    accommodation = Accommodation()\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape single accommodation ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "\n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        accommodation_page_driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            print(\"******************************************************\")\n",
    "            print(\"scrape single accommodation...\")\n",
    "            print(\"for accommodation : \", link_to_accommodation)\n",
    "            accommodation_page_driver.get(link_to_accommodation)\n",
    "\n",
    "            print(\"debug scrape_single_accommodation: top info component section\")\n",
    "            WebDriverWait(accommodation_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/span/div[4]/div/div[1]/div[3]/div')))\n",
    "            top_info_container = accommodation_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/span/div[4]/div/div[1]/div[3]/div')\n",
    "\n",
    "            print(\"debug scrape_single_accommodation: bottom info component section\")\n",
    "            WebDriverWait(accommodation_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"ABOUT_TAB\"]')))\n",
    "            bottom_info_container = accommodation_page_driver.find_element(By.XPATH, '//*[@id=\"ABOUT_TAB\"]')\n",
    "\n",
    "            print(\"debug scrape_single_attraction: common component section\")\n",
    "            WebDriverWait(accommodation_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'IDaDx')))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"retry single accommodation case 1...\")\n",
    "            cnt_retry += 1\n",
    "            accommodation_page_driver.quit()\n",
    "            continue\n",
    "        \n",
    "        # convert accommodation url to adjust page url\n",
    "        # for example: from 'https://th.tripadvisor.com/Hotel_Review-g10804710-d586602-Reviews-Pacific_Club_Resort-Karon_Beach_Karon_Phuket.html' to 'https://th.tripadvisor.com/ImproveListing-d586602.html'\n",
    "        link_to_adjust_page = 'https://th.tripadvisor.com/ImproveListing-%s.html' % (link_to_accommodation.split('-')[2])\n",
    "\n",
    "        # ** find lat/long, location data and openingHours (there are in another page of current accommodation)\n",
    "        # ** if this accommodation not have lat/long\n",
    "        # ** don't continue to scrape\n",
    "        lat, long = scrape_location_latlong_openingHours(\n",
    "            accommodation_page_driver = accommodation_page_driver,\n",
    "            link_to_adjust_page = link_to_adjust_page\n",
    "        )\n",
    "        \n",
    "        # **if can't find lat/long --> don't scrape this attaction\n",
    "        if(lat == 0 and long == 0):\n",
    "            print(\"in scrape_single_accommodation --> can't find lat/long --> don't scrape this accommodation ...\")\n",
    "            accommodation_page_driver.quit()\n",
    "            return accommodation\n",
    "\n",
    "        # find name\n",
    "        name = \"\"\n",
    "        try:\n",
    "            WebDriverWait(accommodation_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'rRtyp')))\n",
    "            name_element = accommodation_page_driver.find_element(By.CLASS_NAME, 'rRtyp')\n",
    "            name = name_element.text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find name\")\n",
    "\n",
    "        print(\"name -> \", name)\n",
    "\n",
    "        # find description\n",
    "        # description = \"\"\n",
    "        # try:\n",
    "        #     WebDriverWait(accommodation_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"AR_ABOUT\"]/div[1]')))\n",
    "            \n",
    "        #     description_container = accommodation_page_driver.find_element(By.XPATH, '//*[@id=\"AR_ABOUT\"]/div[1]')\n",
    "        #     header_element = description_container.find_element(By.CLASS_NAME, 'biGQs')\n",
    "        #     header_text = header_element.text\n",
    "        #     if(header_text == 'ข้อมูล'):\n",
    "        #         description_element = accommodation_page_driver.find_element(By.CLASS_NAME, 'JguWG')\n",
    "        #         description = description_element.text\n",
    "                \n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(\"can't find description\")\n",
    "\n",
    "        # print(\"description -> \", description)\n",
    "        \n",
    "        # find\n",
    "        phone = \"\"\n",
    "        try:\n",
    "            WebDriverWait(accommodation_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/span/div[4]/div/div[1]/div[3]/div/div[3]/div[1]/div[3]/div[2]/div/a')))\n",
    "            phone_element = accommodation_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/span/div[4]/div/div[1]/div[3]/div/div[3]/div[1]/div[3]/div[2]/div/a')\n",
    "            phone_element_href = phone_element.get_attribute('href')\n",
    "            if(\"tel\" in phone_element_href):\n",
    "                phone = phone_element.text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find phone\")\n",
    "\n",
    "        print(\"phone --> \", phone)\n",
    "\n",
    "        # find rating\n",
    "        rating = 0\n",
    "        rating_count = 0\n",
    "        try:\n",
    "            WebDriverWait(accommodation_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'dGsKv')))\n",
    "            rating_container = accommodation_page_driver.find_element(By.CLASS_NAME, 'dGsKv')\n",
    "            \n",
    "            rating_element = rating_container.find_element(By.CLASS_NAME, 'kJyXc')\n",
    "            rating = float(rating_element.text)\n",
    "\n",
    "            rating_count_element = rating_container.find_element(By.CLASS_NAME, 'KxBGd')\n",
    "            rating_count = int(rating_count_element.text.replace(',', '').replace('รีวิว ', '').replace(' รายการ', ''))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find rating and rating_count\")\n",
    "\n",
    "        print(\"rating --> \", rating)\n",
    "        print(\"rating_count --> \", rating_count)\n",
    "\n",
    "        # find facilities, tags\n",
    "        facilities = []\n",
    "        tags = []\n",
    "        try:\n",
    "            possible_target_containers = bottom_info_container.find_elements(By.CLASS_NAME, 'Jevoh')\n",
    "            all_topic_elements = bottom_info_container.find_elements(By.CLASS_NAME, 'vqEpQ')\n",
    "\n",
    "            for i in range(len(possible_target_containers)):\n",
    "                cur_topic = all_topic_elements[i].text\n",
    "                print(\"cur_topic --> \", cur_topic)\n",
    "                if(cur_topic == \"สิ่งอำนวยความสะดวกของสถานที่ให้บริการ\" or cur_topic == \"สิ่งอำนวยความสะดวกในห้องพัก\"):\n",
    "                    all_facility_elements = possible_target_containers[i].find_elements(By.CLASS_NAME, 'gFttI')\n",
    "                    for cur_facility_element in all_facility_elements:\n",
    "                        cur_facility_text = cur_facility_element.text\n",
    "                        if(not len(cur_facility_text)):\n",
    "                            continue\n",
    "                        facilities.append(cur_facility_text)\n",
    "\n",
    "                elif(cur_topic ==  \"ประเภทห้องพัก\"):\n",
    "                    all_tag_elements = possible_target_containers[i].find_elements(By.CLASS_NAME, 'gFttI')\n",
    "                    for cur_tag_element in all_tag_elements:\n",
    "                        cur_tag_text = cur_tag_element.text\n",
    "                        if(not len(cur_tag_text )):\n",
    "                            continue                        \n",
    "                        tags.append(cur_tag_text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find facilities or tags\")\n",
    "\n",
    "        print(\"facilities --> \", facilities)\n",
    "        print(\"tags --> \", tags)\n",
    "\n",
    "        # find star\n",
    "        star = 0\n",
    "        try:\n",
    "            WebDriverWait(accommodation_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\":lithium-Riqkhaianknvlq:\"]')))\n",
    "            star_element = accommodation_page_driver.find_element(By.XPATH, '//*[@id=\":lithium-Riqkhaianknvlq:\"]')\n",
    "            star = star_element.text.split(' ')[0]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find star\")\n",
    "\n",
    "        print(\"star --> \", star)\n",
    "\n",
    "        # find img_path\n",
    "        img_path = scrape_img(accommodation_page_driver)\n",
    "        print(\"cur img path -> \", img_path)\n",
    "\n",
    "        # find location\n",
    "        location = scrape_location(\n",
    "            accommodation_page_driver = accommodation_page_driver,\n",
    "            latitude = lat,\n",
    "            longitude = long,\n",
    "            province_th = province_th\n",
    "        )\n",
    "        print(\"province :\", location.get_province_code(), location.get_province())\n",
    "        print(\"District :\", location.get_district_code(), location.get_district())\n",
    "        print(\"Address : \", location.get_address())\n",
    "\n",
    "        # set some of \"accommodation\" object properties\n",
    "        accommodation.set_name(name)\n",
    "        # accommodation.set_description(description)\n",
    "        accommodation.set_phone(phone)\n",
    "        accommodation.set_latitude(lat)\n",
    "        accommodation.set_longitude(long)\n",
    "        accommodation.set_imgPath(img_path)\n",
    "        accommodation.set_website(link_to_accommodation)\n",
    "        accommodation.set_facility(facilities)\n",
    "        accommodation.set_tag(tags)\n",
    "        accommodation.set_star(star)\n",
    "        accommodation.set_location(\n",
    "            address = location.get_address(),\n",
    "            province = location.get_province(),\n",
    "            district = location.get_district(),\n",
    "            sub_district = location.get_sub_district(),\n",
    "            province_code = location.get_province_code(),\n",
    "            district_code = location.get_district_code(),\n",
    "            sub_district_code = location.get_sub_district_code()\n",
    "        )\n",
    "        accommodation.set_rating(\n",
    "            score = rating,\n",
    "            rating_count = rating_count\n",
    "        )\n",
    "\n",
    "\n",
    "        accommodation_page_driver.quit()\n",
    "        break\n",
    "\n",
    "    return accommodation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_url_by_page(query_url: str, page: int) -> list[str]:\n",
    "\n",
    "    res_url_by_page = []\n",
    "\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        if(cnt_retry == 10):\n",
    "            print(\"max retry for scrape data by page ...\")\n",
    "            break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        # os.environ['proxy_port']\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "      \n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # just check for ip\n",
    "        # print(\"just check for ip :\")\n",
    "        # driver.get(\"https://httpbin.io/ip\")\n",
    "        # print(driver.page_source)\n",
    "\n",
    "        # find group of accommodation on the nth page\n",
    "        all_accommodations_card = []\n",
    "\n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            query_url_by_page = convert_url_by_page(\n",
    "                link_to_accommodation = query_url,\n",
    "                page = page\n",
    "            )\n",
    "            driver.get(query_url_by_page)\n",
    "            # scroll and wait for some msec\n",
    "            driver.execute_script('window.scrollBy(0, document.body.scrollHeight)')\n",
    "            \n",
    "            print(\"check current page url --> \", driver.current_url)\n",
    "\n",
    "            # wait for div (each accommodation section) to be present and visible\n",
    "            print(\"b1 part 1\")\n",
    "            print(\"debug get_all_url_by_page: accommodation by one page section\")\n",
    "            WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'jhsNf')))\n",
    "            all_accommodations_card = driver.find_elements(By.CLASS_NAME, 'jhsNf')\n",
    "\n",
    "            # if current page is 1, find button \"ดูทั้งหมด\"(if it exist) --> click to load more accommodation card elements\n",
    "            # assume that page 1 of target province (phuket for now) not less than 10\n",
    "            # if(page == 1 and len(all_accommodations_card) <= 10):\n",
    "            #     print(\"b 0.5\")\n",
    "            #     print(\"debug get_all_url_by_page: get click more btn for page 1\")\n",
    "            #     WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'sOtnj')))\n",
    "            #     click_more_btn = driver.find_element(By.CLASS_NAME, 'sOtnj')\n",
    "            #     click_more_btn.click()\n",
    "\n",
    "            #     # wait for div (each accommodation section) to be present and visible\n",
    "            #     print(\"b1 part 2\")\n",
    "            #     print(\"debug get_all_url_by_page: accommodation by one page section\")\n",
    "            #     WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'jhsNf')))\n",
    "            #     all_accommodations_card = driver.find_elements(By.CLASS_NAME, 'jhsNf')\n",
    "\n",
    "            # check if all accomodation card can get tag a and its attribute for url\n",
    "            print(\"b2\")\n",
    "            print(\"check in loop ...\")\n",
    "            for cur_accommodation_card in all_accommodations_card:\n",
    "\n",
    "                cur_accommodation_url = cur_accommodation_card.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                print(\"cur_accommodation_url : \", cur_accommodation_url)\n",
    "                res_url_by_page.append(cur_accommodation_url)\n",
    "            \n",
    "            driver.quit()\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"retry find get_all_url_by_page ...\")\n",
    "            cnt_retry += 1\n",
    "            driver.quit()\n",
    "            continue\n",
    "\n",
    "    return res_url_by_page.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_accommodation_by_province(province_url: str, province: str, page: int) -> pd.DataFrame:\n",
    "    # res_accommodation_df = pd.DataFrame()\n",
    "    res_accommodation_df = create_accommodation_df(Accommodation())\n",
    "    \n",
    "    cnt_for_debug = 0\n",
    "        \n",
    "    print(\"scraping accommodation | province --> %s | page --> %s\" % (province, page))\n",
    "\n",
    "    all_url_by_page = get_all_url_by_page(query_url = province_url, page = page)\n",
    "\n",
    "    # use data from 'res_get_data_by_page' to retrive data of specific accommodation\n",
    "    for cur_accommodation_url in all_url_by_page:\n",
    "        if(cnt_for_debug == 4):\n",
    "            break\n",
    "        # continue scraping data for a specific resgtaurant\n",
    "        cur_accommodation = scrape_single_accommodation(\n",
    "            link_to_accommodation = cur_accommodation_url,\n",
    "            province_th = province\n",
    "        )\n",
    "\n",
    "        cnt_for_debug += 1\n",
    "\n",
    "        # create data frame represent data scrape from current accommodation card\n",
    "        cur_accommodation_df = create_accommodation_df(accommodation=cur_accommodation)\n",
    "\n",
    "        # concat all data frame result\n",
    "        res_accommodation_df = pd.concat([res_accommodation_df, cur_accommodation_df])\n",
    "    \n",
    "    return res_accommodation_df.iloc[1:, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory res_accommodation_scraping created successfully\n",
      "scraping accommodation | province --> ภูเก็ต | page --> 1\n",
      "check current page url -->  https://th.tripadvisor.com/Hotels-g293920-Phuket-Hotels.html\n",
      "b1 part 1\n",
      "debug get_all_url_by_page: accommodation by one page section\n",
      "b2\n",
      "check in loop ...\n",
      "cur_accommodation_url :  https://th.tripadvisor.com/Hotel_Review-g297930-d315568-Reviews-Phuket_Marriott_Resort_Spa_Merlin_Beach-Patong_Kathu_Phuket.html\n",
      "cur_accommodation_url :  https://th.tripadvisor.com/Hotel_Review-g297930-d12725009-Reviews-The_Marina_Phuket_Hotel-Patong_Kathu_Phuket.html\n",
      "cur_accommodation_url :  https://th.tripadvisor.com/Hotel_Review-g1210687-d1379794-Reviews-Chanalai_Romantica_Resort-Kata_Beach_Karon_Phuket.html\n",
      "cur_accommodation_url :  https://th.tripadvisor.com/Hotel_Review-g297930-d13140255-Reviews-Hotel_Clover_Patong_Phuket-Patong_Kathu_Phuket.html\n",
      "cur_accommodation_url :  https://th.tripadvisor.com/Hotel_Review-g1210687-d308958-Reviews-Chanalai_Garden_Resort-Kata_Beach_Karon_Phuket.html\n",
      "cur_accommodation_url :  https://th.tripadvisor.com/Hotel_Review-g1223683-d519702-Reviews-JW_Marriott_Phuket_Resort_Spa-Mai_Khao_Thalang_District_Phuket.html\n",
      "cur_accommodation_url :  https://th.tripadvisor.com/Hotel_Review-g1379324-d305747-Reviews-Thavorn_Beach_Village_Resort_Spa-Kamala_Kathu_Phuket.html\n",
      "cur_accommodation_url :  https://th.tripadvisor.com/Hotel_Review-g297930-d557954-Reviews-Baan_Laimai_Beach_Resort_Spa-Patong_Kathu_Phuket.html\n",
      "retry find get_all_url_by_page ...\n",
      "check current page url -->  https://th.tripadvisor.com/Hotels-g293920-Phuket-Hotels.html\n",
      "b1 part 1\n",
      "debug get_all_url_by_page: accommodation by one page section\n",
      "b2\n",
      "check in loop ...\n",
      "cur_accommodation_url :  https://th.tripadvisor.com/Hotel_Review-g7847985-d1739625-Reviews-The_Shore_at_Katathani-Kata_Noi_Beach_Karon_Phuket.html?spAttributionToken=MjcwMTcwNTU\n",
      "cur_accommodation_url :  https://th.tripadvisor.com/Hotel_Review-g297930-d315568-Reviews-Phuket_Marriott_Resort_Spa_Merlin_Beach-Patong_Kathu_Phuket.html\n",
      "cur_accommodation_url :  https://th.tripadvisor.com/Hotel_Review-g297930-d12725009-Reviews-The_Marina_Phuket_Hotel-Patong_Kathu_Phuket.html\n",
      "cur_accommodation_url :  https://th.tripadvisor.com/Hotel_Review-g1210687-d1379794-Reviews-Chanalai_Romantica_Resort-Kata_Beach_Karon_Phuket.html\n",
      "cur_accommodation_url :  https://th.tripadvisor.com/Hotel_Review-g297930-d13140255-Reviews-Hotel_Clover_Patong_Phuket-Patong_Kathu_Phuket.html\n",
      "cur_accommodation_url :  https://th.tripadvisor.com/Hotel_Review-g1210687-d308958-Reviews-Chanalai_Garden_Resort-Kata_Beach_Karon_Phuket.html\n",
      "cur_accommodation_url :  https://th.tripadvisor.com/Hotel_Review-g10804710-d535970-Reviews-Beyond_Karon-Karon_Beach_Karon_Phuket.html?spAttributionToken=Mjc4MDU4MTU\n",
      "cur_accommodation_url :  https://th.tripadvisor.com/Hotel_Review-g1223683-d519702-Reviews-JW_Marriott_Phuket_Resort_Spa-Mai_Khao_Thalang_District_Phuket.html\n",
      "cur_accommodation_url :  https://th.tripadvisor.com/Hotel_Review-g1379324-d305747-Reviews-Thavorn_Beach_Village_Resort_Spa-Kamala_Kathu_Phuket.html\n",
      "cur_accommodation_url :  https://th.tripadvisor.com/Hotel_Review-g297930-d557954-Reviews-Baan_Laimai_Beach_Resort_Spa-Patong_Kathu_Phuket.html\n",
      "******************************************************\n",
      "scrape single accommodation...\n",
      "for accommodation :  https://th.tripadvisor.com/Hotel_Review-g297930-d315568-Reviews-Phuket_Marriott_Resort_Spa_Merlin_Beach-Patong_Kathu_Phuket.html\n",
      "debug scrape_single_accommodation: top info component section\n",
      "debug scrape_single_accommodation: bottom info component section\n",
      "debug scrape_single_attraction: common component section\n",
      "scrape data in adjust accommodation page...\n",
      "for link :  https://th.tripadvisor.com/ImproveListing-d315568.html\n",
      "debug option of adjust page: \n",
      "found target dropdown btn ...\n",
      "lat :  7.884707\n",
      "long :  98.272835\n",
      "name ->  ภูเก็ต แมริออท รีสอร์ท แอนด์ สปา, เมอร์ลิน บีช\n",
      "Phuket Marriott Resort & Spa, Merlin Beach\n",
      "phone -->  +66 76 335 300\n",
      "rating -->  4.5\n",
      "rating_count -->  5832\n",
      "cur_topic -->  สิ่งอำนวยความสะดวกของสถานที่ให้บริการ\n",
      "cur_topic -->  สิ่งอำนวยความสะดวกในห้องพัก\n",
      "cur_topic -->  ประเภทห้องพัก\n",
      "facilities -->  ['ที่จอดรถฟรี', 'อินเตอร์เน็ตความเร็วสูง (WiFi) ฟรี', 'ฟิตเนสเซนเตอร์พร้อมห้องออกกำลังกาย', 'สระว่ายน้ำ', 'บาร์ / เลานจ์', 'ชายหาด', 'การดำน้ำ', 'เด็กเข้าพักฟรี', 'ห้องพักปลอดสารก่อภูมิแพ้', 'ม่านกันแสง', 'เครื่องปรับอากาศ', 'ชายหาดส่วนตัว', 'ห้องน้ำเพิ่มเติม', 'เครื่องชงกาแฟ / ชา', 'เคเบิลทีวี / ทีวีดาวเทียม', 'โถชำระล้าง']\n",
      "tags -->  ['วิวมหาสมุทร', 'วิวสระว่ายน้ำ', 'ห้องพักปลอดบุหรี่', 'ห้องสวีท', 'ห้องสำหรับครอบครัว']\n",
      "star -->  5.0\n",
      "p2\n",
      "p3\n",
      "p4\n",
      "p5\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "max retry for scrape image...\n",
      "cur img path ->  []\n",
      "scrape location data for,  https://www.google.com/maps/search/?api=1&query=7.884707,98.272835\n",
      "found province : 83 ภูเก็ต\n",
      "found District : 8302 กะทู้\n",
      "province : 83 ภูเก็ต\n",
      "District : 8302 กะทู้\n",
      "Address :  99 ถนนหมื่นเงิน หาดไตรตรัง, ป่าตอง, กะทู้, จังหวัดภูเก็ต 83150 ไทย\n",
      "******************************************************\n",
      "scrape single accommodation...\n",
      "for accommodation :  https://th.tripadvisor.com/Hotel_Review-g297930-d12725009-Reviews-The_Marina_Phuket_Hotel-Patong_Kathu_Phuket.html\n",
      "debug scrape_single_accommodation: top info component section\n",
      "debug scrape_single_accommodation: bottom info component section\n",
      "debug scrape_single_attraction: common component section\n",
      "scrape data in adjust accommodation page...\n",
      "for link :  https://th.tripadvisor.com/ImproveListing-d12725009.html\n",
      "debug option of adjust page: \n",
      "found target dropdown btn ...\n",
      "lat :  7.899594\n",
      "long :  98.30387\n",
      "name ->  เดอะ มารีน่า ภูเก็ต\n",
      "The Marina Phuket Hotel\n",
      "can't find phone\n",
      "phone -->  \n",
      "rating -->  5.0\n",
      "rating_count -->  4199\n",
      "cur_topic -->  สิ่งอำนวยความสะดวกของสถานที่ให้บริการ\n",
      "cur_topic -->  สิ่งอำนวยความสะดวกในห้องพัก\n",
      "cur_topic -->  ประเภทห้องพัก\n",
      "facilities -->  ['ที่จอดรถฟรี', 'อินเตอร์เน็ตความเร็วสูง (WiFi) ฟรี', 'ฟิตเนสเซนเตอร์พร้อมห้องออกกำลังกาย', 'สระว่ายน้ำ', 'อาหารเช้าฟรี', 'เด็กเข้าพักฟรี', 'สระว่ายน้ำสำหรับเด็ก', 'บริการรถรับ-ส่งสนามบิน', 'ที่จอดรถพร้อมระบบรักษาความปลอดภัย', 'Wifi', 'ห้องล็อกเกอร์ของฟิตเนส / สปา', 'ผ้าขนหนูสระว่ายน้ำ / ชายหาด', 'สระว่ายน้ำไร้ขอบ', 'สระว่ายน้ำบนดาดฟ้า', 'สระว่ายน้ำเห็นวิว', 'สระว่ายน้ำสำหรับผู้ใหญ่', 'สระว่ายน้ำกลางแจ้ง', 'รั้วกั้นรอบสระน้ำ', 'บาร์ / เลานจ์', 'ร้านกาแฟ', 'ร้านอาหาร', 'มีบริการอาหารเช้า', 'อาหารเช้าแบบบุฟเฟ่ต์', 'บริการอาหารเช้าในห้องพัก', 'กาแฟสําเร็จรูปฟรี', 'ชาฟรี', 'ช่วงแฮปปี้อาวร์', 'มื้ออาหารสำหรับเด็ก', 'พื้นที่รับประทานอาหารกลางแจ้ง', 'บาร์อาหารว่าง', 'บาร์ริมสระว่ายน้ำ', 'บาร์ริมสระว่ายน้ำ', 'บาร์ชั้นดาดฟ้า', 'บริการรถรับ-ส่ง', 'บริการเช่ารถ', 'บริการแท็กซี่', 'ศูนย์บริการธุรกิจพร้อมบริการอินเทอร์เน็ต', 'สิ่งอำนวยความสะดวกสำหรับจัดประชุม', 'ห้องจัดเลี้ยง', 'ห้องประชุม', 'บริการนวดสำหรับคู่รัก', 'บริการนวดหน้า', 'บริการแช่เท้า', 'บริการนวดเท้า', 'บริการนวดทั่วตัว', 'บริการนวดมือ', 'บริการนวดศีรษะ', 'บริการนวด', 'เก้าอี้นวด', 'บริการนวดคอ', 'ห้องอบไอน้ำ', 'เฉลียงบนดาดฟ้า', 'การรักษาความปลอดภัยตลอด 24 ชั่วโมง', 'พื้นที่เก็บกระเป๋า', 'คอนเซียร์จ', 'บริการแลกเปลี่ยนเงินตราต่างประเทศ', 'โรงแรมปลอดบุหรี่', 'ดาดฟ้าสำหรับอาบแดด', 'เก้าอี้อาบแดด / เก้าอี้ชายหาด', 'เฉลียงรับแดด', 'พนักงานเปิดประตู', 'บริการเช็คอิน 24 ชั่วโมง', 'แผนกต้อนรับส่วนหน้าให้บริการตลอด 24 ชั่วโมง', 'บริการซักแห้ง', 'บริการซักรีด', 'บริการรีดผ้า', 'ม่านกันแสง', 'ห้องพักแบบเก็บเสียง', 'เครื่องปรับอากาศ', 'โต๊ะทำงาน', 'บริการทำความสะอาด', 'เครื่องชงกาแฟ / ชา', 'เคเบิลทีวี / ทีวีดาวเทียม', 'อ่างอาบน้ำ / ฝักบัวอาบน้ำ', 'ระเบียงส่วนตัว', 'บริการรูมเซอร์วิส', 'ปลอดภัย', 'พื้นที่นั่งเล่น', 'โซฟา', 'โทรศัพท์', 'น้ำบรรจุขวด', 'เตารีด', 'บริการปลุกตอนเช้า / นาฬิกาปลุก', 'มินิบาร์', 'ตู้เย็น', 'กาต้มน้ำไฟฟ้า', 'ทีวีจอแบน', 'เครื่องใช้สำหรับอาบน้ำฟรี', 'ไดร์เป่าผม']\n",
      "tags -->  ['วิวภูเขา', 'ห้องพักปลอดบุหรี่', 'ห้องสวีท', 'ห้องสำหรับครอบครัว']\n",
      "star -->  4.0\n",
      "p2\n",
      "p3\n",
      "p4\n",
      "p5\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "max retry for scrape image...\n",
      "cur img path ->  []\n",
      "scrape location data for,  https://www.google.com/maps/search/?api=1&query=7.899594,98.30387\n",
      "found province : 83 ภูเก็ต\n",
      "found District : 8302 กะทู้\n",
      "province : 83 ภูเก็ต\n",
      "District : 8302 กะทู้\n",
      "Address :  240/9 Phang Muang Sai Kor Road, ป่าตอง, กะทู้, จังหวัดภูเก็ต 83150 ไทย\n",
      "******************************************************\n",
      "scrape single accommodation...\n",
      "for accommodation :  https://th.tripadvisor.com/Hotel_Review-g1210687-d1379794-Reviews-Chanalai_Romantica_Resort-Kata_Beach_Karon_Phuket.html\n",
      "debug scrape_single_accommodation: top info component section\n",
      "debug scrape_single_accommodation: bottom info component section\n",
      "debug scrape_single_attraction: common component section\n",
      "scrape data in adjust accommodation page...\n",
      "for link :  https://th.tripadvisor.com/ImproveListing-d1379794.html\n",
      "debug option of adjust page: \n",
      "found target dropdown btn ...\n",
      "lat :  7.819152\n",
      "long :  98.3002\n",
      "name ->  Chanalai Romantica Resort\n",
      "phone -->  +66 76 333 105\n",
      "rating -->  5.0\n",
      "rating_count -->  1380\n",
      "cur_topic -->  สิ่งอำนวยความสะดวกของสถานที่ให้บริการ\n",
      "cur_topic -->  สิ่งอำนวยความสะดวกในห้องพัก\n",
      "cur_topic -->  ประเภทห้องพัก\n",
      "facilities -->  ['ที่จอดรถฟรี', 'อินเตอร์เน็ตความเร็วสูง (WiFi) ฟรี', 'สระว่ายน้ำ', 'อาหารเช้าฟรี', 'บริการรถรับ-ส่งสนามบิน', 'ศูนย์บริการธุรกิจพร้อมบริการอินเทอร์เน็ต', 'การรักษาความปลอดภัยตลอด 24 ชั่วโมง', 'พื้นที่เก็บกระเป๋า', 'เสื้อคลุมอาบน้ำ', 'เครื่องปรับอากาศ', 'บริการทำความสะอาด', 'ระเบียงส่วนตัว', 'บริการรูมเซอร์วิส', 'เครื่องชงกาแฟ / ชา', 'ทีวีจอแบน', 'อ่างอาบน้ำ / ฝักบัวอาบน้ำ']\n",
      "tags -->  ['วิวมหาสมุทร', 'ห้องพักปลอดบุหรี่']\n",
      "star -->  4.0\n",
      "p2\n",
      "p3\n",
      "p4\n",
      "p5\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m cur_province_url \u001b[38;5;241m=\u001b[39m cur_region_data[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# get dataframe result of all accommodation in current province\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m cur_res_allaccommodations_df \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_accommodation_by_province\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovince_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcur_province_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovince\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcur_province_th\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# don't forget to remove row with lat/long be zero\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# remove duplicate accommodation\u001b[39;00m\n\u001b[0;32m     23\u001b[0m cur_res_allaccommodations_df\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[29], line 16\u001b[0m, in \u001b[0;36mscrape_accommodation_by_province\u001b[1;34m(province_url, province, page)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# continue scraping data for a specific resgtaurant\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m cur_accommodation \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_single_accommodation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlink_to_accommodation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcur_accommodation_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovince_th\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprovince\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m cnt_for_debug \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# create data frame represent data scrape from current accommodation card\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[27], line 188\u001b[0m, in \u001b[0;36mscrape_single_accommodation\u001b[1;34m(link_to_accommodation, province_th)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstar --> \u001b[39m\u001b[38;5;124m\"\u001b[39m, star)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# find img_path\u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccommodation_page_driver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcur img path -> \u001b[39m\u001b[38;5;124m\"\u001b[39m, img_path)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# find location\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 46\u001b[0m, in \u001b[0;36mscrape_img\u001b[1;34m(accommodation_page_driver)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m     \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccommodation_page_driver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisibility_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcfCAA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     all_img_elements \u001b[38;5;241m=\u001b[39m accommodation_page_driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcfCAA\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfind image element -> \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(all_img_elements))\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:102\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    100\u001b[0m     screen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscreen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    101\u001b[0m     stacktrace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstacktrace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 102\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create directory 'res_accommodation_scraping'\n",
    "createDirectory(fh.STORE_ACCOMM_SCRAPING, 'res_accommodation_scraping')\n",
    "\n",
    "# *** select one province from 'ALL_PROVINCE_ACCOMM_DATA'\n",
    "# *** so, change \"Idx_of_region\" everytime when scrape another province\n",
    "Idx_of_region = 0\n",
    "cur_region_data = ALL_PROVINCE_ACCOMM_DATA[Idx_of_region]\n",
    "\n",
    "cur_province_en = cur_region_data[0]\n",
    "cur_province_th = cur_region_data[1]\n",
    "cur_province_url = cur_region_data[2]\n",
    "\n",
    "# get dataframe result of all accommodation in current province\n",
    "cur_res_allaccommodations_df = scrape_accommodation_by_province(\n",
    "    province_url = cur_province_url,\n",
    "    province = cur_province_th,\n",
    "    page = 1\n",
    ")\n",
    "\n",
    "# don't forget to remove row with lat/long be zero\n",
    "\n",
    "# remove duplicate accommodation\n",
    "cur_res_allaccommodations_df.drop_duplicates(subset=['name'], inplace=True)\n",
    "# set new index\n",
    "cur_res_allaccommodations_df.set_index(['name'], inplace=True)\n",
    "\n",
    "# save result dataframe to .csv\n",
    "res_file_name = 'res_accommodation_%s.csv' % (cur_province_en)\n",
    "res_path = os.path.join(fh.STORE_ACCOMM_SCRAPING, 'res_accommodation_scraping', res_file_name) \n",
    "cur_res_allaccommodations_df.to_csv(res_path, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
