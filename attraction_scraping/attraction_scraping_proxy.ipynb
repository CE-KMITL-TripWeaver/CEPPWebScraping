{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_37996\\2072499850.py\", line 10, in <module>\n",
      "    from packages.file_handler_package.file_handler import *\n",
      "  File \"c:\\Users\\user\\git\\CEPPWebScraping\\attraction_scraping\\..\\packages\\file_handler_package\\file_handler.py\", line 5, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 39, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\compat\\__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\compat\\pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_37996\\2072499850.py\", line 10, in <module>\n",
      "    from packages.file_handler_package.file_handler import *\n",
      "  File \"c:\\Users\\user\\git\\CEPPWebScraping\\attraction_scraping\\..\\packages\\file_handler_package\\file_handler.py\", line 5, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.dtypes import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\dtypes.py\", line 24, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_37996\\2072499850.py\", line 10, in <module>\n",
      "    from packages.file_handler_package.file_handler import *\n",
      "  File \"c:\\Users\\user\\git\\CEPPWebScraping\\attraction_scraping\\..\\packages\\file_handler_package\\file_handler.py\", line 5, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 50, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\numexpr\\__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_37996\\2072499850.py\", line 10, in <module>\n",
      "    from packages.file_handler_package.file_handler import *\n",
      "  File \"c:\\Users\\user\\git\\CEPPWebScraping\\attraction_scraping\\..\\packages\\file_handler_package\\file_handler.py\", line 5, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 64, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py\", line 60, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\bottleneck\\__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import constants.constants as const\n",
    "import constants.file_handler_constants as fh\n",
    "from constants.attraction_constants import *\n",
    "\n",
    "from packages.attraction.Attraction import *\n",
    "from packages.file_handler_package.file_handler import *\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv, dotenv_values \n",
    "\n",
    "from selenium.webdriver import Remote, ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.actions.wheel_input import ScrollOrigin\n",
    "from selenium.webdriver import ActionChains\n",
    "\n",
    "from seleniumwire import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.edge.options import Options\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import ContentType\n",
    "from PIL import Image\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attraction_df(attraction: Attraction) -> pd.DataFrame:\n",
    "    attraction_dict = {\n",
    "        'name' : [attraction.get_name()],\n",
    "        'type' : [attraction.get_type()],\n",
    "        'description' : [attraction.get_description()],\n",
    "        'latitude' : [attraction.get_latitude()],\n",
    "        'longitude' : [attraction.get_longitude()],\n",
    "        'imgPath' : [attraction.get_imgPath()],\n",
    "        'phone': [attraction.get_phone()],\n",
    "        'website': [attraction.get_website()],\n",
    "        'openingHour': [attraction.get_openingHour()],\n",
    "\n",
    "        # location\n",
    "        'address' : [attraction.get_location().get_address()],\n",
    "        'province' : [attraction.get_location().get_province()],\n",
    "        'district' : [attraction.get_location().get_district()],\n",
    "        'subDistrict' : [attraction.get_location().get_sub_district()],\n",
    "        'province_code' : [attraction.get_location().get_province_code()],\n",
    "        'district_code' : [attraction.get_location().get_district_code()],\n",
    "        'sub_district_code' : [attraction.get_location().get_sub_district_code()],\n",
    "\n",
    "        # rating\n",
    "        'score' : [attraction.get_rating().get_score()],\n",
    "        'ratingCount' : [attraction.get_rating().get_ratingCount()],\n",
    "    }\n",
    "\n",
    "    for cur_tag in ATTRACTION_TAG_SCORE:\n",
    "        attraction_dict[cur_tag] = attraction.get_attractionTag().get_tag_score(cur_tag)\n",
    "\n",
    "    attraction_df = pd.DataFrame(attraction_dict)\n",
    "    \n",
    "    return attraction_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_url_by_page(link_to_attraction: str, page: int) -> str:\n",
    "\n",
    "    if(page == 1):\n",
    "        return link_to_attraction\n",
    "    \n",
    "    first_page_url_split = link_to_attraction.split('-')\n",
    "    nth_count_page = 'oa%s' % ((page - 1) * 30)\n",
    "    first_page_url_split[-2] = nth_count_page\n",
    "    res_page_url =  \"-\".join(first_page_url_split)\n",
    "\n",
    "    return res_page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_type_from_url(url: str) -> str:\n",
    "    all_params_containers_str = url.split('/')[-1].split('&')\n",
    "    \n",
    "    if(len(all_params_containers_str) == 1):\n",
    "        return ''\n",
    "    \n",
    "    for cur_container_str in all_params_containers_str:\n",
    "        params, val = cur_container_str.split('=')\n",
    "        if(params == 'type'):\n",
    "            return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScorefromGeminiAPI(name:str, latitude:float, longitude:float, all_img_url:list[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Gets tag scores for a given attraction using the Gemini API.\n",
    "\n",
    "    Args:\n",
    "        name: Name of the attraction.\n",
    "        latitude: Latitude of the attraction.\n",
    "        longitude: Longitude of the attraction.\n",
    "        all_img_url: List of image URLs of the attraction.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary representing scores for all tags.\n",
    "    \"\"\"\n",
    "\n",
    "    # create a 'temp' directory to store temporarily downloaded images, which will be used in requests to the Gemini API\n",
    "    createDirectory(fh.STORE_ATTRACTION_SCRAPING, 'temp')\n",
    "\n",
    "    for Idx, cur_url in enumerate(all_img_url):\n",
    "        if(cur_url == ''):\n",
    "            break\n",
    "        response = requests.get(cur_url)\n",
    "        if response.status_code == 200:\n",
    "            filename = 'temp/temp_img_{0}.jpeg'.format(Idx)\n",
    "            with open(filename, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "\n",
    "    # send API request to retrieve the score for the current attraction (including a query and the main image).\n",
    "    genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "    model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "\n",
    "    \n",
    "    text_prompt = \"Provide place name, latitude, and longitude. I will return a JSON string containing scores (0-1) for following attributes(nothing else no other sentences)\" + \\\n",
    "    \"\\nfor example: \\'{\\\"Tourism\\\":0,\\\"Adventure\\\":0,\\\"Meditation\\\":0,\\\"Art\\\":0,\\\"Cultural\\\":0,\\\"Landscape\\\":0,\\\"Nature\\\":0,\\\"Historical\\\":0,\\\"Cityscape\\\":0,\\\"Beach\\\":0,\\\"Mountain\\\":0,\\\"Architecture\\\":0,\\\"Temple\\\":0,\\\"WalkingStreet\\\":0,\\\"Market\\\":0,\\\"Village\\\":0,\\\"NationalPark\\\":0,\\\"Diving\\\":0,\\\"Snuggle\\\":0,\\\"Waterfall\\\":0,\\\"Island\\\":0,\\\"Shopping\\\":0,\\\"Camping\\\":0,\\\"Fog\\\":0,\\\"Cycling\\\":0,\\\"Monument\\\":0,\\\"Zoo\\\":0,\\\"Waterpark\\\":0,\\\"Hiking\\\":0,\\\"Museum\\\":0,\\\"Riverside\\\":0,\\\"NightLife\\\":0,\\\"Family\\\":0,\\\"Kid\\\":0,\\\"Landmark\\\":0,\\\"Forest\\\":0}\" + \\\n",
    "    \"\\n{0}, {1}, {2} give me score for this\".format(name, latitude, longitude)\n",
    "\n",
    "    # send a prompt to the model\n",
    "    prompt = [text_prompt]\n",
    "    for Idx, cur_path_img in enumerate(glob.glob(os.path.join(fh.STORE_ATTRACTION_SCRAPING, 'temp', '*.jpeg'))):\n",
    "        # use a maximum of 3 images in the prompt to reduce token usage.\n",
    "        if(Idx == 3):\n",
    "            break\n",
    "        cur_img_prompt = Image.open(cur_path_img)\n",
    "        prompt.append(cur_img_prompt)\n",
    "        \n",
    "    print(\"total_tokens: \", model.count_tokens(prompt))\n",
    "    \n",
    "    res_score_dict = {}\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        # remove directory 'temp'\n",
    "        removeNoneEmptyDir(os.path.join(fh.STORE_ATTRACTION_SCRAPING, 'temp'))\n",
    "        res_start_Idx = response.text.find('{')\n",
    "        res_end_Idx = response.text.find('}')\n",
    "        res_score_dict =  json.loads(response.text[res_start_Idx:res_end_Idx+1])\n",
    "\n",
    "    except Exception as e:\n",
    "        # remove directory 'temp'\n",
    "        removeNoneEmptyDir(os.path.join(fh.STORE_ATTRACTION_SCRAPING, 'temp'))\n",
    "        print(\"failed to use gemini api\")\n",
    "    \n",
    "    return res_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_img(attraction_page_driver: webdriver) -> list[str]:\n",
    "\n",
    "    res_imgPath = []\n",
    "\n",
    "    possible_click_img_xpath = [\n",
    "        '//*[@id=\"AR_ABOUT\"]/div[2]/div/div/div/div/div[1]/div/div[3]/button',\n",
    "        '//*[@id=\"AR_ABOUT\"]/div[2]/div/div/div/div/div[1]/div/div/div/div[1]/div/div[7]/button',\n",
    "        '//*[@id=\"AR_ABOUT\"]/div/div/div/div/div/div[1]/div/div/div/div[1]/div/div[7]/button',\n",
    "    ]\n",
    "    \n",
    "    btn_img_xpath = \"\"\n",
    "    print('y2')\n",
    "    for cur_xpath in possible_click_img_xpath:\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, cur_xpath)))\n",
    "            btn_img_xpath = cur_xpath\n",
    "            break\n",
    "        \n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    if(not len(btn_img_xpath)):\n",
    "        print(\"can't scrape img (no img ?)\")\n",
    "        return ['']\n",
    "\n",
    "    # find button and click\n",
    "    # to see modal then scrape image address\n",
    "    try:\n",
    "        # WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, btn_img_xpath)))\n",
    "        print('y3')\n",
    "        click_img_btn = attraction_page_driver.find_element(By.XPATH, btn_img_xpath)\n",
    "\n",
    "        # Move to the element and click\n",
    "        print(\"y4\")\n",
    "        actions = ActionChains(attraction_page_driver)\n",
    "        actions.move_to_element(click_img_btn).click().perform()\n",
    "        print(\"y5\")\n",
    "        \n",
    "        print(\"cur img url --> \", attraction_page_driver.current_url)\n",
    "\n",
    "        type_value = extract_type_from_url(url=attraction_page_driver.current_url)\n",
    "        print(\"cur img section type --> \", type_value)\n",
    "\n",
    "        \n",
    "        # case 1 img section UI:  https://th.tripadvisor.com/Attraction_Review-g297930-d1866109-Reviews-Bangla_Road-Patong_Kathu_Phuket.html#/media/1866109/?albumid=-160&type=ALL_INCLUDING_RESTRICTED&category=-160\n",
    "        if(type_value == \"ALL_INCLUDING_RESTRICTED\"):\n",
    "            print(\"enter case 1 img ...\")\n",
    "            is_end_scrape_img = False\n",
    "            cnt_retry = 0\n",
    "            while(not is_end_scrape_img):\n",
    "                if(cnt_retry == 10):\n",
    "                    print(\"max retry for scrape image...\")\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    WebDriverWait(attraction_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'BtGfv')))\n",
    "                    all_img_containers = attraction_page_driver.find_elements(By.CLASS_NAME, 'BtGfv')\n",
    "                    print(\"find image element -> \", len(all_img_containers))\n",
    "                    for cur_container in all_img_containers:\n",
    "                        cur_img_element = cur_container.find_element(By.TAG_NAME, 'img')\n",
    "                        cur_bgImg_val = cur_img_element.get_attribute('src')\n",
    "                        res_imgPath.append(cur_bgImg_val)\n",
    "                        \n",
    "                    is_end_scrape_img = True\n",
    "\n",
    "                except Exception as e:\n",
    "                    cnt_retry += 1\n",
    "                    print(\"retry scrape img case 1 ...\")\n",
    "\n",
    "\n",
    "        # case 2 img section UI: https://th.tripadvisor.com/Attraction_Review-g297930-d1866109-Reviews-Bangla_Road-Patong_Kathu_Phuket.html#/media-atf/1866109/?albumid=-160&type=0&category=-160\n",
    "        else:\n",
    "            print(\"enter case 2 img ...\")\n",
    "            is_end_scrape_img = False\n",
    "            cnt_retry = 0\n",
    "            while(not is_end_scrape_img):\n",
    "                if(cnt_retry == 10):\n",
    "                    print(\"max retry for scrape image...\")\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    WebDriverWait(attraction_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'cfCAA')))\n",
    "                    all_img_elements = attraction_page_driver.find_elements(By.CLASS_NAME, 'cfCAA')\n",
    "                    print(\"find image element -> \", len(all_img_elements))\n",
    "                    for cur_img_element in all_img_elements:\n",
    "                        cur_bgImg_val = cur_img_element.value_of_css_property('background-image')\n",
    "                        match = re.search(r'url\\(\"(.*?)\"\\)', cur_bgImg_val)\n",
    "                        if match:\n",
    "                            res_imgPath.append(match.group(1))\n",
    "\n",
    "                    is_end_scrape_img = True\n",
    "\n",
    "                except Exception as e:\n",
    "                    cnt_retry += 1\n",
    "                    print(\"retry scrape img case 2 ...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"can't scrape img \")\n",
    "    \n",
    "\n",
    "    return res_imgPath.copy()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_location(attraction_page_driver: webdriver, latitude: float, longitude: float, province_th: str) -> Location:\n",
    "\n",
    "    # find better address description on wongnai\n",
    "    # for example: \"991 ถนนพระราม 1 Pathum Wan, กรุงเทพมหานคร (กทม.) 10330 ไทย\"\n",
    "    address_tripAdvisor = \"\"\n",
    "    possible_address_xpath = [\n",
    "        '//*[@id=\"tab-data-WebPresentation_PoiLocationSectionGroup\"]/div/div/div[2]/div[1]/div/div/div/div[1]/button/span',\n",
    "        '//*[@id=\"tab-data-WebPresentation_PoiLocationSectionGroup\"]/div/div/div[2]/div[1]/div/div/div/div/button/span',\n",
    "        '//*[@id=\"tab-data-WebPresentation_PoiLocationSectionGroup\"]/div/div/div[2]/div[1]/div/div/div/div[1]/button/span'\n",
    "    ]\n",
    "\n",
    "\n",
    "    for cur_address_xpath in possible_address_xpath:\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, cur_address_xpath)))\n",
    "            address_element = attraction_page_driver.find_element(By.XPATH, cur_address_xpath)\n",
    "            address_tripAdvisor = address_element.text\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "\n",
    "    # start scrape location\n",
    "    res_location = Location()\n",
    "    cnt_retry = 0\n",
    "    try:\n",
    "        while(True):\n",
    "            if(cnt_retry == 10):\n",
    "                print(\"max retry for scrape Google Map ...\")\n",
    "                break\n",
    "            \n",
    "            # set up new webdriver to work googlemap url(query for specific lat/long)\n",
    "            possible_addressGoogleMap_elements = []\n",
    "            try:\n",
    "                # set Chrome options to run in headless mode\n",
    "                # options = Options()\n",
    "                options = webdriver.ChromeOptions()\n",
    "                options.add_argument(\"start-maximized\")\n",
    "                # options.add_argument(\"--headless=new\")\n",
    "                options.add_experimental_option(\n",
    "                    \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "                )\n",
    "\n",
    "                google_map_driver = webdriver.Chrome(options=options)\n",
    "                \n",
    "                google_map_query = \"https://www.google.com/maps/search/?api=1&query=%s,%s\" % (latitude, longitude)\n",
    "                google_map_driver.get(google_map_query)\n",
    "                print(\"scrape location data for, \", google_map_query)\n",
    "                \n",
    "                WebDriverWait(google_map_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'DkEaL')))\n",
    "                possible_addressGoogleMap_elements = google_map_driver.find_elements(By.CLASS_NAME, 'DkEaL')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"retry  scrape Google Map..\")\n",
    "                cnt_retry += 1\n",
    "                google_map_driver.close()\n",
    "                continue\n",
    "\n",
    "\n",
    "            # after init new webdriver -> continure scrape location data\n",
    "\n",
    "            # if found some wiered place that doesn't even have its address\n",
    "            # skip this case for now...\n",
    "            if(not len(possible_addressGoogleMap_elements)):\n",
    "                return res_location\n",
    "\n",
    "            subStrDistrict = \"อำเภอ\"\n",
    "            subStrSubDistrict = \"ตำบล\"\n",
    "\n",
    "            if province_th == \"กรุงเทพมหานคร\":\n",
    "                subStrDistrict = \"เขต\"\n",
    "                subStrSubDistrict = \"แขวง\"\n",
    "\n",
    "            district = 0\n",
    "            subDirstrict = 0\n",
    "\n",
    "            # find location\n",
    "            useData = None\n",
    "            for cur_element in possible_addressGoogleMap_elements:\n",
    "                if province_th in cur_element.text and cur_element.text.find(subStrDistrict) != -1:\n",
    "                    useData = cur_element.text.replace(\",\",\"\").replace(\"เเ\",\"แ\")\n",
    "                    break\n",
    "           \n",
    "            if(useData != None):\n",
    "                # print(\"Full Address :\",useData)\n",
    "                # another brute force way in case of province 'กรุงเทพหมานคร' not have word 'แขวง' in address\n",
    "                if(province_th == 'กรุงเทพมหานคร' and useData.find(subStrSubDistrict) == -1):\n",
    "                    subAddress_split = useData.split(' ')\n",
    "                    cur_province_Idx = subAddress_split.index(province_th)\n",
    "                    district = subAddress_split[cur_province_Idx - 1].replace(\"เขต\",\"\")\n",
    "\n",
    "                else:\n",
    "                    start_address_index = useData.find(subStrDistrict)\n",
    "                    subAddress = useData[start_address_index:]\n",
    "                    district = subAddress[subAddress.find(subStrDistrict)+len(subStrDistrict):subAddress.find(province_th)].replace(\" \",\"\")               \n",
    "\n",
    "                if district == \"เมือง\":\n",
    "                    district = district+province_th\n",
    "\n",
    "                # filter row to find 'ISO_3166_code', 'zip_code', 'geo_code'\n",
    "                geo_code_df = pd.read_csv(fh.PATH_TO_GEOCODE)\n",
    "                filtered_rows = geo_code_df[\n",
    "                    (geo_code_df['province_th'] == province_th) & (geo_code_df['district_th'] == district)\n",
    "                ]\n",
    "                filtered_rows.reset_index(inplace=True, drop=True)\n",
    "                \n",
    "                if not filtered_rows.empty:\n",
    "                    print(\"found province :\",filtered_rows.loc[0, 'ISO_3166_code'], province_th)\n",
    "                    print(\"found District :\",filtered_rows.loc[0, 'zip_code'], district)\n",
    "\n",
    "                    res_location.set_address(address_tripAdvisor if len(address_tripAdvisor) else useData)\n",
    "                    res_location.set_province(province_th)\n",
    "                    res_location.set_district(district)\n",
    "                    res_location.set_sub_district(\"\")\n",
    "                    res_location.set_province_code(filtered_rows.loc[0, 'ISO_3166_code'])\n",
    "                    res_location.set_district_code(filtered_rows.loc[0, 'zip_code'])\n",
    "                    res_location.set_sub_district_code(0)\n",
    "\n",
    "                else:\n",
    "                    print(\"not found province :\", province_th)\n",
    "                    print(\"not found District :\", district)\n",
    "\n",
    "                    res_location.set_address(address_tripAdvisor if len(address_tripAdvisor) else useData)\n",
    "                    res_location.set_province(province_th)\n",
    "                    res_location.set_district(district)\n",
    "                    res_location.set_sub_district(\"\")\n",
    "                    res_location.set_province_code(0)\n",
    "                    res_location.set_district_code(0)\n",
    "                    res_location.set_sub_district_code(0)\n",
    "\n",
    "            google_map_driver.close()\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"can't scrape location data\")\n",
    "\n",
    "    return res_location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape lat/long, openingHours, types (there are in another page of current attraction)\n",
    "def scrape_adjust_page(link_to_adjust_page: str) -> tuple[float, float, dict, list[str]]:\n",
    "    lat = 0\n",
    "    long = 0\n",
    "    openingHours = {}\n",
    "    types = []\n",
    "\n",
    "    # create new webdriver to continue scrape lat/long, openingHours in adjust attraction page\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape single attraction ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "\n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        adjust_page_driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # retry in case of web restrictions, some elements not loaded\n",
    "        try:\n",
    "            print(\"scrape data in adjust attraction page...\")\n",
    "            print(\"for link : \", link_to_adjust_page)\n",
    "            adjust_page_driver.get(link_to_adjust_page)\n",
    "\n",
    "            print(\"debug option of adjust page: \")\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'DiHOR')))\n",
    "\n",
    "            # find dropdown --> click display data below --> cick display lat/long input form\n",
    "            possible_target_btn = adjust_page_driver.find_elements(By.CLASS_NAME, 'DiHOR')\n",
    "            for cur_dropdown_btn in possible_target_btn:\n",
    "                cur_dropdown_text = cur_dropdown_btn.text\n",
    "                if(\"แนะนำการแก้ไขข้อมูลของสถานที่นี้\" in cur_dropdown_text):\n",
    "                    print(\"found target dropdown btn ...\")\n",
    "                    cur_dropdown_btn.click()\n",
    "                    WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/button')))\n",
    "                    # find button click to display lat/long input form\n",
    "                    display_lat_long_btn = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/button')\n",
    "                    display_lat_long_btn.click()\n",
    "\n",
    "        except Exception as e:\n",
    "            cnt_retry += 1\n",
    "            adjust_page_driver.quit()\n",
    "            print(\"retry adjust page...\")\n",
    "            continue\n",
    "\n",
    "      \n",
    "        # find lat/long\n",
    "        try:\n",
    "            WebDriverWait(adjust_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[1]/div/div[2]/div/div/div/span')))\n",
    "            WebDriverWait(adjust_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[2]/div/div[2]/div/div/div/span')))\n",
    "    \n",
    "            lat_input_container = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[1]/div/div[2]/div/div/div/span')\n",
    "            lat_input_element = lat_input_container.find_element(By.TAG_NAME, 'input')\n",
    "            lat = float(lat_input_element.get_attribute('value'))\n",
    "\n",
    "            long_input_container = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[2]/div/div[2]/div/div/div/span')\n",
    "            long_input_element = long_input_container.find_element(By.TAG_NAME, 'input')\n",
    "            long = float(long_input_element.get_attribute('value'))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find lat/long\")\n",
    "        \n",
    "        print(\"lat : \", lat)\n",
    "        print(\"long : \", long)\n",
    "\n",
    "        # **if can't find lat/long --> don't scrape this attaction\n",
    "        if(lat == 0 and long == 0):\n",
    "            print(\"in scrape_adjust_page --> can't find lat/long --> 0, 0\")\n",
    "            return lat, long, openingHours.copy(), types.copy()\n",
    "\n",
    "\n",
    "        # find openingHours\n",
    "        try:\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'dNAjp')))\n",
    "            all_openingHours_container = adjust_page_driver.find_elements(By.CLASS_NAME, 'dNAjp')\n",
    "            for cur_openingHours_container in all_openingHours_container:\n",
    "                cur_day_element = cur_openingHours_container.find_element(By.CLASS_NAME, 'ngXxk')\n",
    "                cur_day_text = cur_day_element.text.replace(\":\", \"\")\n",
    "\n",
    "                cur_time_element = cur_openingHours_container.find_element(By.CLASS_NAME, 'KxBGd')\n",
    "                cur_time_text = cur_time_element.text\n",
    "\n",
    "                openingHours[cur_day_text] = cur_time_text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find openingHours ...\")\n",
    "\n",
    "        print(\"openingHours : \", openingHours.copy())\n",
    "\n",
    "\n",
    "        # find types\n",
    "        try:\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'ZCWaz')))\n",
    "            all_type_elements = adjust_page_driver.find_elements(By.CLASS_NAME, 'ZCWaz')\n",
    "            for cur_element in all_type_elements:\n",
    "                cur_type_text = cur_element.text\n",
    "                types.append(cur_type_text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find types ...\")\n",
    "\n",
    "        print(\"types : \", types.copy())\n",
    "        \n",
    "\n",
    "        adjust_page_driver.quit()\n",
    "        break\n",
    "\n",
    "    return lat, long, openingHours.copy(), types.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_attraction(link_to_attraction: str, province_th: str) -> Attraction:\n",
    "    \n",
    "    attraction = Attraction()\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        if(cnt_retry == 10):\n",
    "            print(\"max retry for scrape single attraction ...\")\n",
    "            break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set web browser options to run\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", \n",
    "            {\n",
    "                \"profile.managed_default_content_settings.images\": 2, # Disable image\n",
    "                \"profile.default_content_setting_values.cookies\": 2,  # Block all cookies\n",
    "                \"profile.default_content_settings.popups\": 0,         # Disable popups\n",
    "                \"profile.managed_default_content_settings.cookies\": 2  # Disable third-party cookies\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # initialize the web driver with service, selenium-wire options, and web browser options\n",
    "        attraction_page_driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            print(\"******************************************************\")\n",
    "            print(\"scrape single attraction...\")\n",
    "            print(\"for attraction : \", link_to_attraction)\n",
    "            attraction_page_driver.get(link_to_attraction)\n",
    "            # attraction_page_driver.get('https://th.tripadvisor.com/Attraction_Review-g1389361-d2433844-Reviews-Big_Buddha_Phuket-Chalong_Phuket_Town_Phuket.html')\n",
    "\n",
    "            print(\"debug scrape_single_attraction: common component section\")\n",
    "            WebDriverWait(attraction_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'IDaDx')))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"retry single attraction case 1...\")\n",
    "            cnt_retry += 1\n",
    "            attraction_page_driver.quit()\n",
    "            continue\n",
    "        \n",
    "        # convert attraction url to adjust page url\n",
    "        # for example: from 'https://th.tripadvisor.com/Attraction_Review-g297930-d1866109-Reviews-Bangla_Road-Patong_Kathu_Phuket.html' to 'https://th.tripadvisor.com/ImproveListing-d1866109.html'\n",
    "        link_to_adjust_page = 'https://th.tripadvisor.com/ImproveListing-%s.html' % (link_to_attraction.split('-')[2])\n",
    "\n",
    "        # ** find lat/long, openingHours, types (there are in another page of current attraction)\n",
    "        # ** if this attraction not have lat/long\n",
    "        # ** don't continue to scrape\n",
    "        lat, long, openingHours, types = scrape_adjust_page(\n",
    "            link_to_adjust_page = link_to_adjust_page\n",
    "        )\n",
    "        \n",
    "        # **if can't find lat/long --> don't scrape this attaction\n",
    "        if(lat == 0 and long == 0):\n",
    "            print(\"in scrape_single_attraction --> can't find lat/long --> don't scrape this attraction ...\")\n",
    "            attraction_page_driver.quit()\n",
    "            return attraction\n",
    "\n",
    "        # find name\n",
    "        name = \"\"\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[1]/header/div[3]/div[1]/div/h1')))\n",
    "            name_element = attraction_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[1]/header/div[3]/div[1]/div/h1')\n",
    "            name = name_element.text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find name\")\n",
    "\n",
    "        print(\"name -> \", name)\n",
    "\n",
    "        # find description\n",
    "        description = \"\"\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"AR_ABOUT\"]/div[1]')))\n",
    "            \n",
    "            description_container = attraction_page_driver.find_element(By.XPATH, '//*[@id=\"AR_ABOUT\"]/div[1]')\n",
    "            header_element = description_container.find_element(By.CLASS_NAME, 'biGQs')\n",
    "            header_text = header_element.text\n",
    "            if(header_text == 'ข้อมูล'):\n",
    "                description_element = attraction_page_driver.find_element(By.CLASS_NAME, 'JguWG')\n",
    "                description = description_element.text\n",
    "                \n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find description\")\n",
    "\n",
    "        print(\"description -> \", description)\n",
    "        \n",
    "        # find rating\n",
    "        rating = 0\n",
    "        rating_count = 0\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[2]/div[2]/div/div[1]/section[1]/div/div/div/div/div[1]/div[1]/a/div')))\n",
    "            score_element = attraction_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[2]/div[2]/div/div[1]/section[1]/div/div/div/div/div[1]/div[1]/a/div')\n",
    "            score_text_list = score_element.get_attribute('aria-label').split(' ')\n",
    "            for Idx in range(1, len(score_text_list)):\n",
    "                # set rating\n",
    "                if(score_text_list[Idx - 1] == \"คะแนน\"):\n",
    "                    rating = float(score_text_list[Idx])\n",
    "\n",
    "                elif(score_text_list[Idx - 1] == \"รีวิว\"):\n",
    "                    rating_count = int(score_text_list[Idx].replace(',', ''))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find rating and rating_count\")\n",
    "\n",
    "        print(\"rating --> \", rating)\n",
    "        print(\"rating_count --> \", rating_count)\n",
    "\n",
    "        # find img_path\n",
    "        img_path = scrape_img(attraction_page_driver)\n",
    "        print(\"cur img path -> \", img_path)\n",
    "\n",
    "        # find location\n",
    "        location = scrape_location(\n",
    "            attraction_page_driver = attraction_page_driver,\n",
    "            latitude = lat,\n",
    "            longitude = long,\n",
    "            province_th = province_th\n",
    "        )\n",
    "        print(\"province :\", location.get_province_code(), location.get_province())\n",
    "        print(\"District :\", location.get_district_code(), location.get_district())\n",
    "        print(\"Address : \", location.get_address())\n",
    "\n",
    "        # find attractionTag score\n",
    "        # attractionTag_score = getScorefromGeminiAPI(\n",
    "        #     name = name,\n",
    "        #     latitude = lat,\n",
    "        #     longitude = long,\n",
    "        #     all_img_url = img_path\n",
    "        # )\n",
    "        # print(\"attractionTag score : \")\n",
    "        # print(attractionTag_score)\n",
    "\n",
    "        # set some of \"Attraction\" object properties\n",
    "        attraction.set_name(name)\n",
    "        attraction.set_type(types)\n",
    "        attraction.set_description(description)\n",
    "        attraction.set_latitude(lat)\n",
    "        attraction.set_longitude(long)\n",
    "        attraction.set_imgPath(img_path)\n",
    "        attraction.set_website(link_to_attraction)\n",
    "        attraction.set_openingHour(openingHours)\n",
    "        attraction.set_location(\n",
    "            address = location.get_address(),\n",
    "            province = location.get_province(),\n",
    "            district = location.get_district(),\n",
    "            sub_district = location.get_sub_district(),\n",
    "            province_code = location.get_province_code(),\n",
    "            district_code = location.get_district_code(),\n",
    "            sub_district_code = location.get_sub_district_code()\n",
    "        )\n",
    "        attraction.set_rating(\n",
    "            score = rating,\n",
    "            rating_count = rating_count\n",
    "        )\n",
    "        # attraction.set_attractionTag(attractionTag_score)\n",
    "\n",
    "        attraction_page_driver.quit()\n",
    "        break\n",
    "\n",
    "    return attraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_url_by_page(query_url: str, page: int) -> list[str]:\n",
    "\n",
    "    res_url_by_page = []\n",
    "\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        if(cnt_retry == 10):\n",
    "            print(\"max retry for scrape data by page ...\")\n",
    "            break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        # os.environ['proxy_port']\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "      \n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # just check for ip\n",
    "        # print(\"just check for ip :\")\n",
    "        # driver.get(\"https://httpbin.io/ip\")\n",
    "        # print(driver.page_source)\n",
    "\n",
    "        # find group of attraction on the nth page\n",
    "        all_attractions_card = []\n",
    "\n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            query_url_by_page = convert_url_by_page(\n",
    "                link_to_attraction = query_url,\n",
    "                page = page\n",
    "            )\n",
    "            driver.get(query_url_by_page)\n",
    "            # scroll and wait for some msec\n",
    "            driver.execute_script('window.scrollBy(0, document.body.scrollHeight)')\n",
    "            \n",
    "            print(\"check current page url --> \", driver.current_url)\n",
    "\n",
    "            # wait for div (each attraction section) to be present and visible\n",
    "            print(\"b1\")\n",
    "            print(\"debug get_all_url_by_page: attraction by one page section\")\n",
    "            # WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'XJlaI')))\n",
    "            WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'KRObB')))\n",
    "\n",
    "            print(\"b2\")\n",
    "            print(\"debug get_all_url_by_page: text\")\n",
    "            # WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'BKifx')))\n",
    "            WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'sVTRu')))\n",
    "\n",
    "            # print(\"b3\")\n",
    "            # print(\"debug get_all_url_by_page: link to single attraction\")\n",
    "            # WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.TAG_NAME, 'a')))\n",
    "\n",
    "            print(\"b3\")\n",
    "            print(\"check in loop ...\")\n",
    "            # all_attractions_card = driver.find_elements(By.CLASS_NAME, 'XJlaI')\n",
    "            all_attractions_card = driver.find_elements(By.CLASS_NAME, 'KRObB')\n",
    "            for cur_attraction_card in all_attractions_card:\n",
    "    \n",
    "                cur_attraction_url = cur_attraction_card.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "\n",
    "                # check_text = cur_attraction_card.find_element(By.CLASS_NAME, 'BKifx').text  \n",
    "                check_text = cur_attraction_card.find_element(By.CLASS_NAME, 'sVTRu').text  \n",
    "               \n",
    "                # check if cuurent card is for attraction ?\n",
    "                is_attraction_keyword = True\n",
    "                not_attraction_keyword = ['ทัวร์', 'ทริป', \"สปา\", \"กิจกรรมทางวัฒนธรรม\", 'ชั้นเรียน', 'รถรับส่ง', 'อุปกรณ์ให้เช่า', 'ร้านขายของ', 'นั่งเรือเที่ยว']\n",
    "                for cur_check_word in not_attraction_keyword:\n",
    "                    if(cur_check_word in check_text):\n",
    "                        is_attraction_keyword = False\n",
    "                        break\n",
    "                \n",
    "                if(is_attraction_keyword):\n",
    "                    print(\"cur_attraction_url : \", cur_attraction_url)\n",
    "                    print(\"check cur text : \", check_text)\n",
    "                    res_url_by_page.append(cur_attraction_url)\n",
    "            \n",
    "            driver.quit()\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"retry find get_all_url_by_page ...\")\n",
    "            cnt_retry += 1\n",
    "            driver.quit()\n",
    "            continue\n",
    "\n",
    "    return res_url_by_page.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_attractions_by_province(page: int, province_url: str, province: str) -> pd.DataFrame:\n",
    "    # res_attraction_df = pd.DataFrame()\n",
    "    res_attraction_df = create_attraction_df(Attraction())\n",
    "    \n",
    "    cnt_for_debug = 0\n",
    "        \n",
    "    print(\"scraping attraction | province --> %s | page --> %s\" % (province, page))\n",
    "\n",
    "    all_url_by_page = get_all_url_by_page(query_url = province_url, page = page)\n",
    "\n",
    "    # use data from 'res_get_data_by_page' to retrive data of specific attraction\n",
    "    for cur_attraction_url in all_url_by_page:\n",
    "        if(cnt_for_debug == 3):\n",
    "            break\n",
    "\n",
    "        # continue scraping data for a specific resgtaurant\n",
    "        cur_attraction = scrape_single_attraction(\n",
    "            link_to_attraction = cur_attraction_url,\n",
    "            province_th = province\n",
    "        )\n",
    "        cnt_for_debug += 1\n",
    "\n",
    "\n",
    "        # create data frame represent data scrape from current attraction card\n",
    "        cur_attraction_df = create_attraction_df(attraction=cur_attraction)\n",
    "\n",
    "        # concat all data frame result\n",
    "        res_attraction_df = pd.concat([res_attraction_df, cur_attraction_df])\n",
    "    \n",
    "    return res_attraction_df.iloc[1:, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping attraction | province --> ภูเก็ต | page --> 1\n",
      "check current page url -->  https://th.tripadvisor.com/Attractions-g293920-Activities-a_allAttractions.true-Phuket.html\n",
      "b1\n",
      "debug get_all_url_by_page: attraction by one page section\n",
      "retry find get_all_url_by_page ...\n",
      "check current page url -->  https://th.tripadvisor.com/Attractions-g293920-Activities-a_allAttractions.true-Phuket.html\n",
      "b1\n",
      "debug get_all_url_by_page: attraction by one page section\n",
      "b2\n",
      "debug get_all_url_by_page: text\n",
      "b3\n",
      "check in loop ...\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297930-d1866109-Reviews-Bangla_Road-Patong_Kathu_Phuket.html\n",
      "check cur text :  จุดที่น่าสนใจและสถานที่สำคัญ\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1389361-d2433844-Reviews-Big_Buddha_Phuket-Chalong_Phuket_Town_Phuket.html\n",
      "check cur text :  สถานที่ทางศาสนา\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1210687-d450974-Reviews-Kata_Beach-Kata_Beach_Karon_Phuket.html\n",
      "check cur text :  ชายหาด\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1224343-d8550982-Reviews-Banana_Beach-Ko_He_Phuket.html\n",
      "check cur text :  ชายหาด\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1224250-d13237105-Reviews-Green_Elephant_Sanctuary_Park-Choeng_Thale_Thalang_District_Phuket.html\n",
      "check cur text :  สถานที่ชมธรรมชาติ/ชีวิตสัตว์ป่า\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297930-d2454044-Reviews-Patong_Beach-Patong_Kathu_Phuket.html\n",
      "check cur text :  ชายหาด\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g10804710-d450973-Reviews-Karon_Beach-Karon_Beach_Karon_Phuket.html\n",
      "check cur text :  ชายหาด\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1231757-d553203-Reviews-Nai_Harn_Beach-Nai_Harn_Rawai_Phuket.html\n",
      "check cur text :  จุดชมวิว\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297930-d3387563-Reviews-Jungceylon-Patong_Kathu_Phuket.html\n",
      "check cur text :  ศูนย์การค้า\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297930-d778266-Reviews-Freedom_Beach-Patong_Kathu_Phuket.html\n",
      "check cur text :  ชายหาด\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297931-d23965533-Reviews-Andamanda_Phuket-Kathu_Phuket.html\n",
      "check cur text :  สวนน้ำ\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1379324-d553530-Reviews-Kamala_Beach-Kamala_Kathu_Phuket.html\n",
      "check cur text :  ชายหาด\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2315818-d5995325-Reviews-Old_Phuket_Town-Talat_Yai_Phuket_Town_Phuket.html\n",
      "check cur text :  เส้นทางเดินชมวิว\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1231756-d24186033-Reviews-Phuket_Elephant_Care-Nai_Thon_Thalang_District_Phuket.html\n",
      "check cur text :  สถานที่ชมธรรมชาติ/ชีวิตสัตว์ป่า\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1389361-d553516-Reviews-Chaithararam_Temple_Wat_Chalong-Chalong_Phuket_Town_Phuket.html\n",
      "check cur text :  จุดชมวิว\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1215781-d2032284-Reviews-Naka_Market-Phuket_Town_Phuket.html\n",
      "check cur text :  ตลาดนัด/ถนนคนเดิน\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2315814-d1965786-Reviews-Central_Phuket-Wichit_Phuket_Town_Phuket.html\n",
      "check cur text :  ศูนย์การค้า\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1215780-d1161264-Reviews-Kata_Noi_Beach-Karon_Phuket.html\n",
      "check cur text :  ชายหาด\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g293920-d6648473-Reviews-Soi_Dog_Foundation-Phuket.html\n",
      "check cur text :  สถานที่ชมธรรมชาติ/ชีวิตสัตว์ป่า\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g293920-d592496-Reviews-Bang_Tao_Beach-Phuket.html\n",
      "check cur text :  ชายหาด\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1224250-d1161253-Reviews-Surin_Beach-Choeng_Thale_Thalang_District_Phuket.html\n",
      "check cur text :  ชายหาด\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1215781-d3830436-Reviews-Monkey_Hill-Phuket_Town_Phuket.html\n",
      "check cur text :  จุดชมวิว\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297937-d553534-Reviews-Gibbon_Rehabilitation_Project-Thalang_District_Phuket.html\n",
      "check cur text :  สถานที่ชมธรรมชาติ/ชีวิตสัตว์ป่า\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g293920-d1149585-Reviews-Mai_Khao_Beach-Phuket.html\n",
      "check cur text :  ชายหาด\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1215773-d1036078-Reviews-Naiyang_Beach-Nai_Yang_Sakhu_Thalang_District_Phuket.html\n",
      "check cur text :  ชายหาด\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297934-d553523-Reviews-Promthep_Cape-Rawai_Phuket.html\n",
      "check cur text :  จุดชมวิว\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1215781-d8776186-Reviews-Sunday_Walking_Street_Market_Lard_Yai-Phuket_Town_Phuket.html\n",
      "check cur text :  ตลาดนัด/ถนนคนเดิน\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g293920-d631985-Reviews-Koh_Yao_Yai-Phuket.html\n",
      "check cur text :  เกาะ\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1215780-d3448295-Reviews-Karon_Viewpoint-Karon_Phuket.html\n",
      "check cur text :  จุดชมวิว\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g293920-d1197179-Reviews-Banana_Beach-Phuket.html\n",
      "check cur text :  ชายหาด\n",
      "******************************************************\n",
      "scrape single attraction...\n",
      "for attraction :  https://th.tripadvisor.com/Attraction_Review-g297930-d1866109-Reviews-Bangla_Road-Patong_Kathu_Phuket.html\n",
      "debug scrape_single_attraction: common component section\n",
      "scrape data in adjust attraction page...\n",
      "for link :  https://th.tripadvisor.com/ImproveListing-d1866109.html\n",
      "debug option of adjust page: \n",
      "found target dropdown btn ...\n",
      "lat :  7.893388\n",
      "long :  98.29736\n",
      "can't find openingHours ...\n",
      "openingHours :  {}\n",
      "types :  ['สถานที่สำคัญ/จุดที่น่าสนใจ']\n",
      "can't find name\n",
      "name ->  \n",
      "description ->  \n",
      "rating -->  4.0\n",
      "rating_count -->  21028\n",
      "y2\n",
      "y3\n",
      "y4\n",
      "y5\n",
      "cur img url -->  https://th.tripadvisor.com/Attraction_Review-g297930-d1866109-Reviews-Bangla_Road-Patong_Kathu_Phuket.html#/media/1866109/?albumid=-160&type=ALL_INCLUDING_RESTRICTED&category=-160\n",
      "cur img section type -->  ALL_INCLUDING_RESTRICTED\n",
      "enter case 1 img ...\n",
      "find image element ->  15\n",
      "cur img path ->  ['https://dynamic-media-cdn.tripadvisor.com/media/photo-o/05/1b/97/47/bangla-road.jpg?w=500&h=-1&s=1', 'https://dynamic-media-cdn.tripadvisor.com/media/photo-o/2e/1d/0e/d6/caption.jpg?w=500&h=-1&s=1', 'https://dynamic-media-cdn.tripadvisor.com/media/photo-o/2e/1d/0e/d5/caption.jpg?w=500&h=-1&s=1', 'https://dynamic-media-cdn.tripadvisor.com/media/photo-o/2e/1d/0e/d4/caption.jpg?w=500&h=-1&s=1', 'https://dynamic-media-cdn.tripadvisor.com/media/photo-o/2e/1d/0e/d3/caption.jpg?w=500&h=-1&s=1', 'https://dynamic-media-cdn.tripadvisor.com/media/photo-o/2e/03/4c/18/caption.jpg?w=500&h=-1&s=1', 'https://dynamic-media-cdn.tripadvisor.com/media/photo-o/2e/03/4c/17/caption.jpg?w=500&h=-1&s=1', 'https://dynamic-media-cdn.tripadvisor.com/media/photo-s/02/b9/e5/f4/bangla-road.jpg?w=500&h=-1&s=1', 'https://dynamic-media-cdn.tripadvisor.com/media/photo-o/02/10/38/e8/bangla-road-in-the-day.jpg?w=500&h=-1&s=1', 'https://dynamic-media-cdn.tripadvisor.com/media/photo-o/2b/4a/26/91/creative-drinks-and-superior.jpg?w=500&h=-1&s=1', 'https://dynamic-media-cdn.tripadvisor.com/media/photo-o/1b/87/5e/f0/photo2jpg.jpg?w=500&h=-1&s=1', 'https://dynamic-media-cdn.tripadvisor.com/media/photo-o/1a/e7/4a/92/bangla-road.jpg?w=500&h=-1&s=1', 'https://dynamic-media-cdn.tripadvisor.com/media/photo-o/1a/a0/8a/76/caption.jpg?w=500&h=-1&s=1', 'https://dynamic-media-cdn.tripadvisor.com/media/photo-o/1a/8a/e4/1d/photo0jpg.jpg?w=500&h=-1&s=1', 'https://dynamic-media-cdn.tripadvisor.com/media/photo-o/1a/58/9a/4b/photo0jpg.jpg?w=500&h=-1&s=1']\n",
      "scrape location data for,  https://www.google.com/maps/search/?api=1&query=7.893388,98.29736\n",
      "found province : 83 ภูเก็ต\n",
      "found District : 8302 กะทู้\n",
      "province : 83 ภูเก็ต\n",
      "District : 8302 กะทู้\n",
      "Address :  V7VW+9W5 ตำบลป่าตอง อำเภอกะทู้ ภูเก็ต\n",
      "******************************************************\n",
      "scrape single attraction...\n",
      "for attraction :  https://th.tripadvisor.com/Attraction_Review-g1389361-d2433844-Reviews-Big_Buddha_Phuket-Chalong_Phuket_Town_Phuket.html\n",
      "debug scrape_single_attraction: common component section\n",
      "scrape data in adjust attraction page...\n",
      "for link :  https://th.tripadvisor.com/ImproveListing-d2433844.html\n",
      "debug option of adjust page: \n",
      "retry adjust page...\n",
      "scrape data in adjust attraction page...\n",
      "for link :  https://th.tripadvisor.com/ImproveListing-d2433844.html\n",
      "debug option of adjust page: \n",
      "found target dropdown btn ...\n",
      "lat :  7.827575\n",
      "long :  98.31284\n",
      "openingHours :  {'จันทร์': 'ปิด', 'อังคาร': 'ปิด', 'พุธ': 'ปิด', 'พฤหัสบดี': 'ปิด', 'ศุกร์': 'ปิด', 'เสาร์': '08:00-18:30', 'อาทิตย์': 'ปิด'}\n",
      "types :  ['สถานที่ทางศาสนาและสถานที่ศักดิ์สิทธิ์', 'สถานที่สำคัญ/จุดที่น่าสนใจ']\n",
      "can't find name\n",
      "name ->  \n",
      "description ->  \n",
      "rating -->  4.5\n",
      "rating_count -->  16886\n",
      "y2\n",
      "y3\n",
      "y4\n",
      "y5\n",
      "cur img url -->  https://th.tripadvisor.com/Attraction_Review-g1389361-d2433844-Reviews-Big_Buddha_Phuket-Chalong_Phuket_Town_Phuket.html#/media/2433844/?albumid=-160&type=ALL_INCLUDING_RESTRICTED&category=-160\n",
      "cur img section type -->  ALL_INCLUDING_RESTRICTED\n",
      "enter case 1 img ...\n"
     ]
    }
   ],
   "source": [
    "# *** select one province from 'ALL_PROVINCE_TRIPADVISOR_DATA'\n",
    "# *** so, change \"Idx_of_region\" everytime when scrape another province\n",
    "Idx_of_region = 0\n",
    "cur_region_data = ALL_PROVINCE_TRIPADVISOR_DATA[Idx_of_region]\n",
    "\n",
    "# select first and last page to scrape\n",
    "# but in this module will not have any effect (just some dummy number to use with file name)\n",
    "# will have effect on module \"mulProcess_attraction_scraping_proxy\"\n",
    "first_page = 1\n",
    "last_page = 2\n",
    "\n",
    "cur_province_en = cur_region_data[0]\n",
    "cur_province_th = cur_region_data[1]\n",
    "cur_province_url = cur_region_data[2]\n",
    "\n",
    "# cur_res_allAttractions_df = create_attraction_df(Attraction())\n",
    "\n",
    "# get dataframe result of all attraction in current province\n",
    "cur_res_allAttractions_df = scrape_attractions_by_province(\n",
    "    page = 1,\n",
    "    province_url = cur_province_url,\n",
    "    province = cur_province_th\n",
    ")\n",
    "\n",
    "# don't forget to remove row with lat/long be zero\n",
    "\n",
    "# remove duplicate attraction\n",
    "cur_res_allAttractions_df.drop_duplicates(subset=['name'], inplace=True)\n",
    "# set new index\n",
    "cur_res_allAttractions_df.set_index(['name'], inplace=True)\n",
    "\n",
    "# create directory to store result of scraping attraction\n",
    "# for example: 'attraction_scraping\\res_attraction_scraping\\res_attraction_Phuket'\n",
    "createDirectory(fh.STORE_ATTRACTION_SCRAPING, os.path.join('res_attraction_scraping', 'res_attraction_%s' % (cur_province_en)))\n",
    "\n",
    "# save result dataframe to .csv\n",
    "# for example: 'res_attraction_Phuket_page_1_44.csv'\n",
    "res_file_name = 'res_attraction_%s_page_%s_%s.csv' % (cur_province_en, first_page, last_page)\n",
    "res_path = os.path.join(fh.STORE_ATTRACTION_SCRAPING, 'res_attraction_scraping', 'res_attraction_%s' % (cur_province_en), res_file_name)\n",
    "cur_res_allAttractions_df.to_csv(res_path, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
