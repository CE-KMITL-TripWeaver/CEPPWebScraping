{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pyautogui\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import constants.constants as const\n",
    "import constants.file_handler_constants as fh\n",
    "from constants.attraction_constants import *\n",
    "\n",
    "from packages.attraction.Attraction import *\n",
    "from packages.file_handler_package.file_handler import *\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv, dotenv_values \n",
    "\n",
    "from selenium.webdriver import Remote, ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.actions.wheel_input import ScrollOrigin\n",
    "from selenium.webdriver import ActionChains\n",
    "\n",
    "from seleniumwire import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from selenium.webdriver.edge.options import Options\n",
    "\n",
    "\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attraction_df(attraction: Attraction) -> pd.DataFrame:\n",
    "    attraction_dict = {\n",
    "        'name' : [attraction.get_name()],\n",
    "        'description' : [attraction.get_description()],\n",
    "        'latitude' : [attraction.get_latitude()],\n",
    "        'longitude' : [attraction.get_longitude()],\n",
    "        'imgPath' : [attraction.get_imgPath()],\n",
    "        'phone': [attraction.get_phone()],\n",
    "        'website': [attraction.get_website()],\n",
    "        'openingHour': [attraction.get_openingHour()],\n",
    "\n",
    "        # location\n",
    "        'address' : [attraction.get_location().get_address()],\n",
    "        'province' : [attraction.get_location().get_province()],\n",
    "        'district' : [attraction.get_location().get_district()],\n",
    "        'subDistrict' : [attraction.get_location().get_sub_district()],\n",
    "        'province_code' : [attraction.get_location().get_province_code()],\n",
    "        'district_code' : [attraction.get_location().get_district_code()],\n",
    "        'sub_district_code' : [attraction.get_location().get_sub_district_code()],\n",
    "\n",
    "        # rating\n",
    "        'score' : [attraction.get_rating().get_score()],\n",
    "        'ratingCount' : [attraction.get_rating().get_ratingCount()],\n",
    "    }\n",
    "\n",
    "    for cur_tag in ATTRACTION_TAG_SCORE:\n",
    "        attraction_dict[cur_tag] = attraction.get_attractionTag().get_tag_score(cur_tag)\n",
    "\n",
    "    attraction_df = pd.DataFrame(attraction_dict)\n",
    "    \n",
    "    return attraction_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_img(attraction_page_driver: webdriver) -> list[str]:\n",
    "    \n",
    "    res_imgPath = []\n",
    "\n",
    "    possible_click_img_xpath = [\n",
    "        '//*[@id=\"AR_ABOUT\"]/div[2]/div/div/div/div/div[1]/div/div/div/div[1]/div/div[7]/button',\n",
    "        '//*[@id=\"AR_ABOUT\"]/div/div/div/div/div/div[1]/div/div/div/div[1]/div/div[7]/button'\n",
    "    ]\n",
    "    \n",
    "    btn_img_xpath = \"\"\n",
    "    for cur_xpath in possible_click_img_xpath:\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, cur_xpath)))\n",
    "            btn_img_xpath = cur_xpath\n",
    "            break\n",
    "        \n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    if(not len(btn_img_xpath)):\n",
    "        print(\"can't scrape img (no img ?)\")\n",
    "        return ['']\n",
    "\n",
    "    # find button and click\n",
    "    # to see modal then scrape image address\n",
    "    try:\n",
    "        WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, btn_img_xpath)))\n",
    "        click_img_btn = attraction_page_driver.find_element(By.XPATH, btn_img_xpath)\n",
    "        click_img_btn.click()\n",
    "        is_end_scrape_img = False\n",
    "        cnt_retry = 0\n",
    "        while(not is_end_scrape_img):\n",
    "            if(cnt_retry == 10):\n",
    "                print(\"max retry for scrape image...\")\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                WebDriverWait(attraction_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'cfCAA')))\n",
    "                all_img_elements = attraction_page_driver.find_elements(By.CLASS_NAME, 'cfCAA')\n",
    "                print(\"find image element -> \", len(all_img_elements))\n",
    "                for cur_img_element in all_img_elements:\n",
    "                    cur_bgImg_val = cur_img_element.value_of_css_property('background-image')\n",
    "                    match = re.search(r'url\\(\"(.*?)\"\\)', cur_bgImg_val)\n",
    "                    if match:\n",
    "                        res_imgPath.append(match.group(1))\n",
    "\n",
    "                is_end_scrape_img = True\n",
    "\n",
    "            except Exception as e:\n",
    "                cnt_retry += 1\n",
    "                print(\"retry scrape img...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "            pass\n",
    "    \n",
    "\n",
    "    return res_imgPath.copy()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_location(attraction_page_driver: webdriver, latitude: float, longitude: float, province_th: str) -> Location:\n",
    "\n",
    "    # find better address description on wongnai\n",
    "    # for example: \"991 ถนนพระราม 1 Pathum Wan, กรุงเทพมหานคร (กทม.) 10330 ไทย\"\n",
    "    address_tripAdvisor = \"\"\n",
    "    possible_address_xpath = [\n",
    "        '//*[@id=\"tab-data-WebPresentation_PoiLocationSectionGroup\"]/div/div/div[2]/div[1]/div/div/div/div[1]/button/span',\n",
    "        '//*[@id=\"tab-data-WebPresentation_PoiLocationSectionGroup\"]/div/div/div[2]/div[1]/div/div/div/div/button/span'\n",
    "    ]\n",
    "\n",
    "\n",
    "    for cur_address_xpath in possible_address_xpath:\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, cur_address_xpath)))\n",
    "            address_element = attraction_page_driver.find_element(By.XPATH, cur_address_xpath)\n",
    "            address_tripAdvisor = address_element.text\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "\n",
    "    # start scrape location\n",
    "    res_location = Location()\n",
    "    cnt_retry = 0\n",
    "    try:\n",
    "        while(True):\n",
    "            if(cnt_retry == 10):\n",
    "                print(\"max retry for scrape Google Map ...\")\n",
    "                break\n",
    "            \n",
    "            # set up new webdriver to work googlemap url(query for specific lat/long)\n",
    "            possible_addressGoogleMap_elements = []\n",
    "            try:\n",
    "                # set Chrome options to run in headless mode\n",
    "                # options = Options()\n",
    "                options = webdriver.ChromeOptions()\n",
    "                options.add_argument(\"start-maximized\")\n",
    "                # options.add_argument(\"--headless=new\")\n",
    "                options.add_experimental_option(\n",
    "                    \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "                )\n",
    "\n",
    "                google_map_driver = webdriver.Chrome(options=options)\n",
    "                \n",
    "                google_map_query = \"https://www.google.com/maps/search/?api=1&query=%s,%s\" % (latitude, longitude)\n",
    "                google_map_driver.get(google_map_query)\n",
    "                print(\"scrape location data for, \", google_map_query)\n",
    "                \n",
    "                WebDriverWait(google_map_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'DkEaL')))\n",
    "                possible_addressGoogleMap_elements = google_map_driver.find_elements(By.CLASS_NAME, 'DkEaL')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"retry  scrape Google Map..\")\n",
    "                cnt_retry += 1\n",
    "                google_map_driver.close()\n",
    "                continue\n",
    "\n",
    "\n",
    "            # after init new webdriver -> continure scrape location data\n",
    "\n",
    "            # if found some wiered place that doesn't even have its address\n",
    "            # skip this case for now...\n",
    "            if(not len(possible_addressGoogleMap_elements)):\n",
    "                return res_location\n",
    "\n",
    "            subStrDistrict = \"อำเภอ\"\n",
    "            subStrSubDistrict = \"ตำบล\"\n",
    "\n",
    "            if province_th == \"กรุงเทพมหานคร\":\n",
    "                subStrDistrict = \"เขต\"\n",
    "                subStrSubDistrict = \"แขวง\"\n",
    "\n",
    "            district = 0\n",
    "            subDirstrict = 0\n",
    "\n",
    "            # find location\n",
    "            useData = None\n",
    "            for cur_element in possible_addressGoogleMap_elements:\n",
    "                if province_th in cur_element.text and cur_element.text.find(subStrDistrict) != -1:\n",
    "                    useData = cur_element.text.replace(\",\",\"\").replace(\"เเ\",\"แ\")\n",
    "                    break\n",
    "           \n",
    "            if(useData != None):\n",
    "                # print(\"Full Address :\",useData)\n",
    "                # another brute force way in case of province 'กรุงเทพหมานคร' not have word 'แขวง' in address\n",
    "                if(province_th == 'กรุงเทพมหานคร' and useData.find(subStrSubDistrict) == -1):\n",
    "                    subAddress_split = useData.split(' ')\n",
    "                    cur_province_Idx = subAddress_split.index(province_th)\n",
    "                    district = subAddress_split[cur_province_Idx - 1].replace(\"เขต\",\"\")\n",
    "\n",
    "                else:\n",
    "                    start_address_index = useData.find(subStrDistrict)\n",
    "                    subAddress = useData[start_address_index:]\n",
    "                    district = subAddress[subAddress.find(subStrDistrict)+len(subStrDistrict):subAddress.find(province_th)].replace(\" \",\"\")               \n",
    "\n",
    "                if district == \"เมือง\":\n",
    "                    district = district+province_th\n",
    "\n",
    "                # filter row to find 'ISO_3166_code', 'zip_code', 'geo_code'\n",
    "                geo_code_df = pd.read_csv(fh.PATH_TO_GEOCODE)\n",
    "                filtered_rows = geo_code_df[\n",
    "                    (geo_code_df['province_th'] == province_th) & (geo_code_df['district_th'] == district)\n",
    "                ]\n",
    "                filtered_rows.reset_index(inplace=True, drop=True)\n",
    "                \n",
    "                if not filtered_rows.empty:\n",
    "                    print(\"found province :\",filtered_rows.loc[0, 'ISO_3166_code'], province_th)\n",
    "                    print(\"found District :\",filtered_rows.loc[0, 'zip_code'], district)\n",
    "\n",
    "                    res_location.set_address(address_tripAdvisor if len(address_tripAdvisor) else useData)\n",
    "                    res_location.set_province(province_th)\n",
    "                    res_location.set_district(district)\n",
    "                    res_location.set_sub_district(\"\")\n",
    "                    res_location.set_province_code(filtered_rows.loc[0, 'ISO_3166_code'])\n",
    "                    res_location.set_district_code(filtered_rows.loc[0, 'zip_code'])\n",
    "                    res_location.set_sub_district_code(0)\n",
    "\n",
    "                else:\n",
    "                    print(\"not found province :\", province_th)\n",
    "                    print(\"not found District :\", district)\n",
    "\n",
    "                    res_location.set_address(address_tripAdvisor if len(address_tripAdvisor) else useData)\n",
    "                    res_location.set_province(province_th)\n",
    "                    res_location.set_district(district)\n",
    "                    res_location.set_sub_district(\"\")\n",
    "                    res_location.set_province_code(0)\n",
    "                    res_location.set_district_code(0)\n",
    "                    res_location.set_sub_district_code(0)\n",
    "\n",
    "            google_map_driver.close()\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"can't scrape location data\")\n",
    "\n",
    "    return res_location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape lat/long, and openingHours (there are in another page of current attraction)\n",
    "def scrape_location_latlong_openingHours(attraction_page_driver: webdriver, link_to_adjust_page: str) -> tuple[float, float, dict]:\n",
    "    lat = 0\n",
    "    long = 0\n",
    "    openingHours = {}\n",
    "\n",
    "    # create new webdriver to continue scrape lat/long, openingHours in adjust attraction page\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape single attraction ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "\n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        adjust_page_driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # retry in case of web restrictions, some elements not loaded\n",
    "        try:\n",
    "            print(\"scrape data in adjust attraction page...\")\n",
    "            print(\"for link : \", link_to_adjust_page)\n",
    "            adjust_page_driver.get(link_to_adjust_page)\n",
    "\n",
    "            print(\"debug option of adjust page: \")\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'DiHOR')))\n",
    "\n",
    "            # find dropdown --> click display data below --> cick display lat/long input form\n",
    "            possible_target_btn = adjust_page_driver.find_elements(By.CLASS_NAME, 'DiHOR')\n",
    "            for cur_dropdown_btn in possible_target_btn:\n",
    "                cur_dropdown_text = cur_dropdown_btn.text\n",
    "                if(\"แนะนำการแก้ไขข้อมูลของสถานที่นี้\" in cur_dropdown_text):\n",
    "                    print(\"found target dropdown btn ...\")\n",
    "                    cur_dropdown_btn.click()\n",
    "                    WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/button')))\n",
    "                    # find button click to display lat/long input form\n",
    "                    display_lat_long_btn = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/button')\n",
    "                    display_lat_long_btn.click()\n",
    "\n",
    "        except Exception as e:\n",
    "            cnt_retry += 1\n",
    "            adjust_page_driver.quit()\n",
    "            print(\"retry adjust page...\")\n",
    "            continue\n",
    "\n",
    "      \n",
    "        # find lat/long\n",
    "        try:\n",
    "            WebDriverWait(adjust_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[1]/div/div[2]/div/div/div/span')))\n",
    "            WebDriverWait(adjust_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[2]/div/div[2]/div/div/div/span')))\n",
    "    \n",
    "            lat_input_container = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[1]/div/div[2]/div/div/div/span')\n",
    "            lat_input_element = lat_input_container.find_element(By.TAG_NAME, 'input')\n",
    "            lat = float(lat_input_element.get_attribute('value'))\n",
    "\n",
    "            long_input_container = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[2]/div/div[2]/div/div/div/span')\n",
    "            long_input_element = long_input_container.find_element(By.TAG_NAME, 'input')\n",
    "            long = float(long_input_element.get_attribute('value'))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find lat/long\")\n",
    "        \n",
    "        print(\"lat : \", lat)\n",
    "        print(\"long : \", long)\n",
    "\n",
    "        # **if can't find lat/long --> don't scrape this attaction\n",
    "        if(lat == 0 and long == 0):\n",
    "            print(\"in scrape_location_latlong_openingHours --> can't find lat/long --> 0, 0\")\n",
    "            return lat, long, openingHours.copy()\n",
    "\n",
    "        # find openingHours\n",
    "        try:\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'dNAjp')))\n",
    "            all_openingHours_container = adjust_page_driver.find_elements(By.CLASS_NAME, 'dNAjp')\n",
    "            for cur_openingHours_container in all_openingHours_container:\n",
    "                cur_day_element = cur_openingHours_container.find_element(By.CLASS_NAME, 'ngXxk')\n",
    "                cur_day_text = cur_day_element.text.replace(\":\", \"\")\n",
    "\n",
    "                cur_time_element = cur_openingHours_container.find_element(By.CLASS_NAME, 'KxBGd')\n",
    "                cur_time_text = cur_time_element.text\n",
    "\n",
    "                openingHours[cur_day_text] = cur_time_text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find openingHours ...\")\n",
    "\n",
    "        print(\"openingHours : \", openingHours.copy())\n",
    "\n",
    "        adjust_page_driver.quit()\n",
    "        break\n",
    "\n",
    "    return lat, long, openingHours.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_attraction(link_to_attraction: str, province_th: str) -> Attraction:\n",
    "    \n",
    "    attraction = Attraction()\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape single attraction ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", \n",
    "            {\n",
    "                \"profile.managed_default_content_settings.images\": 2, # Disable image\n",
    "                \"profile.default_content_setting_values.cookies\": 2,  # Block all cookies\n",
    "                \"profile.default_content_settings.popups\": 0,         # Disable popups\n",
    "                \"profile.managed_default_content_settings.cookies\": 2  # Disable third-party cookies\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        attraction_page_driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            print(\"******************************************************\")\n",
    "            print(\"scrape single attraction...\")\n",
    "            print(\"for attraction : \", link_to_attraction)\n",
    "            attraction_page_driver.get(link_to_attraction)\n",
    "\n",
    "            print(\"debug scrape_single_attraction: common component section\")\n",
    "            WebDriverWait(attraction_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'IDaDx')))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"retry single attraction case 1...\")\n",
    "            cnt_retry += 1\n",
    "            attraction_page_driver.quit()\n",
    "            continue\n",
    "        \n",
    "        # convert attraction url to adjust page url\n",
    "        # for example: from 'https://th.tripadvisor.com/Attraction_Review-g297930-d1866109-Reviews-Bangla_Road-Patong_Kathu_Phuket.html' to 'https://th.tripadvisor.com/ImproveListing-d1866109.html'\n",
    "        link_to_adjust_page = 'https://th.tripadvisor.com/ImproveListing-%s.html' % (link_to_attraction.split('-')[2])\n",
    "\n",
    "        # ** find lat/long, location data and openingHours (there are in another page of current attraction)\n",
    "        # ** if this attraction not have lat/long\n",
    "        # ** don't continue to scrape\n",
    "        lat, long, openingHours = scrape_location_latlong_openingHours(\n",
    "            attraction_page_driver = attraction_page_driver,\n",
    "            link_to_adjust_page = link_to_adjust_page\n",
    "        )\n",
    "        \n",
    "        # **if can't find lat/long --> don't scrape this attaction\n",
    "        if(lat == 0 and long == 0):\n",
    "            print(\"in scrape_single_attraction --> can't find lat/long --> don't scrape this attraction ...\")\n",
    "            attraction_page_driver.quit()\n",
    "            return attraction\n",
    "\n",
    "        # find name\n",
    "        name = \"\"\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[1]/header/div[3]/div[1]/div/h1')))\n",
    "            name_element = attraction_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[1]/header/div[3]/div[1]/div/h1')\n",
    "            name = name_element.text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find name\")\n",
    "\n",
    "        print(\"name -> \", name)\n",
    "\n",
    "        # find description\n",
    "        description = \"\"\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"AR_ABOUT\"]/div[1]')))\n",
    "            \n",
    "            description_container = attraction_page_driver.find_element(By.XPATH, '//*[@id=\"AR_ABOUT\"]/div[1]')\n",
    "            header_element = description_container.find_element(By.CLASS_NAME, 'biGQs')\n",
    "            header_text = header_element.text\n",
    "            if(header_text == 'ข้อมูล'):\n",
    "                description_element = attraction_page_driver.find_element(By.CLASS_NAME, 'JguWG')\n",
    "                description = description_element.text\n",
    "                \n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find description\")\n",
    "\n",
    "        print(\"description -> \", description)\n",
    "        \n",
    "        # find rating\n",
    "        rating = 0\n",
    "        rating_count = 0\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[2]/div[2]/div/div[1]/section[1]/div/div/div/div/div[1]/div[1]/a/div')))\n",
    "            score_element = attraction_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[2]/div[2]/div/div[1]/section[1]/div/div/div/div/div[1]/div[1]/a/div')\n",
    "            score_text_list = score_element.get_attribute('aria-label').split(' ')\n",
    "            for Idx in range(1, len(score_text_list)):\n",
    "                # set rating\n",
    "                if(score_text_list[Idx - 1] == \"คะแนน\"):\n",
    "                    rating = float(score_text_list[Idx])\n",
    "\n",
    "                elif(score_text_list[Idx - 1] == \"รีวิว\"):\n",
    "                    rating_count = int(score_text_list[Idx].replace(',', ''))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find rating and rating_count\")\n",
    "\n",
    "        print(\"rating --> \", rating)\n",
    "        print(\"rating_count --> \", rating_count)\n",
    "\n",
    "        # find img_path\n",
    "        img_path = scrape_img(attraction_page_driver)\n",
    "        print(\"cur img path -> \", img_path)\n",
    "\n",
    "        # find location\n",
    "        location = scrape_location(\n",
    "            attraction_page_driver = attraction_page_driver,\n",
    "            latitude = lat,\n",
    "            longitude = long,\n",
    "            province_th = province_th\n",
    "        )\n",
    "        print(\"province :\", location.get_province_code(), location.get_province())\n",
    "        print(\"District :\", location.get_district_code(), location.get_district())\n",
    "\n",
    "        # set some of \"Attraction\" object properties\n",
    "        attraction.set_name(name)\n",
    "        attraction.set_description(description)\n",
    "        attraction.set_latitude(lat)\n",
    "        attraction.set_longitude(long)\n",
    "        attraction.set_imgPath(img_path)\n",
    "        attraction.set_website(link_to_attraction)\n",
    "        attraction.set_openingHour(openingHours)\n",
    "        attraction.set_location(\n",
    "            address = location.get_address(),\n",
    "            province = location.get_province(),\n",
    "            district = location.get_district(),\n",
    "            sub_district = location.get_sub_district(),\n",
    "            province_code = location.get_province_code(),\n",
    "            district_code = location.get_district_code(),\n",
    "            sub_district_code = location.get_sub_district_code()\n",
    "        )\n",
    "        attraction.set_rating(\n",
    "            score = rating,\n",
    "            rating_count = rating_count\n",
    "        )\n",
    "\n",
    "\n",
    "        attraction_page_driver.quit()\n",
    "        break\n",
    "\n",
    "    return attraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_url_by_page(query_url: str) -> list[str]:\n",
    "\n",
    "    res_url_by_page = []\n",
    "\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        if(cnt_retry == 10):\n",
    "            print(\"max retry for scrape data by page ...\")\n",
    "            break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        # os.environ['proxy_port']\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "      \n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # just check for ip\n",
    "        # print(\"just check for ip :\")\n",
    "        # driver.get(\"https://httpbin.io/ip\")\n",
    "        # print(driver.page_source)\n",
    "\n",
    "        # find group of attraction on the nth page\n",
    "        all_attractions_card = []\n",
    "\n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            driver.get(query_url)\n",
    "            # scroll and wait for some msec\n",
    "            driver.execute_script('window.scrollBy(0, document.body.scrollHeight)')\n",
    "            \n",
    "            print(\"check current page url --> \", driver.current_url)\n",
    "\n",
    "            # wait for div (each attraction section) to be present and visible\n",
    "            print(\"b1\")\n",
    "            print(\"debug get_all_url_by_page: attraction by one page section\")\n",
    "            WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'XJlaI')))\n",
    "\n",
    "            print(\"b2\")\n",
    "            print(\"debug get_all_url_by_page: text\")\n",
    "            WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'BKifx')))\n",
    "            \n",
    "            # print(\"b3\")\n",
    "            # print(\"debug get_all_url_by_page: link to single attraction\")\n",
    "            # WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.TAG_NAME, 'a')))\n",
    "\n",
    "            print(\"b3\")\n",
    "            print(\"check in loop ...\")\n",
    "            all_attractions_card = driver.find_elements(By.CLASS_NAME, 'XJlaI')\n",
    "            for cur_attraction_card in all_attractions_card:\n",
    "\n",
    "                cur_attraction_url = cur_attraction_card.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "\n",
    "                check_text = cur_attraction_card.find_element(By.CLASS_NAME, 'BKifx').text  \n",
    "               \n",
    "                # check if cuurent card is for attraction ?\n",
    "                not_attraction_keyword = ['ทัวร์', \"สปา\", \"กิจกรรมทางวัฒนธรรม\", 'ชั้นเรียน', 'รถรับส่ง', 'อุปกรณ์ให้เช่า', 'ร้านขายของ']\n",
    "                for cur_check_word in not_attraction_keyword:\n",
    "                    if(cur_check_word in check_text):\n",
    "                        break\n",
    "\n",
    "                print(\"cur_attraction_url : \", cur_attraction_url)\n",
    "                res_url_by_page.append(cur_attraction_url)\n",
    "            \n",
    "            driver.quit()\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"retry find get_all_url_by_page ...\")\n",
    "            cnt_retry += 1\n",
    "            driver.quit()\n",
    "            continue\n",
    "\n",
    "    return res_url_by_page.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_attraction_by_province(province_url: str, province: str) -> pd.DataFrame:\n",
    "    # res_attraction_df = pd.DataFrame()\n",
    "    res_attraction_df = create_attraction_df(Attraction())\n",
    "    \n",
    "    cnt_for_debug = 0\n",
    "        \n",
    "    print(\"scraping attraction | province --> %s | page --> %s\" % (province, cnt_for_debug))\n",
    "\n",
    "    all_url_by_page = get_all_url_by_page(query_url=province_url)\n",
    "\n",
    "    # use data from 'res_get_data_by_page' to retrive data of specific attraction\n",
    "    for cur_attraction_url in all_url_by_page:\n",
    "        # if(cnt_for_debug == 3):\n",
    "        #     break\n",
    "        # continue scraping data for a specific resgtaurant\n",
    "        cur_attraction = scrape_single_attraction(\n",
    "            link_to_attraction = cur_attraction_url,\n",
    "            province_th = province\n",
    "        )\n",
    "        cnt_for_debug += 1\n",
    "        # cur_attraction = scrape_single_attraction(\n",
    "        #     link_to_attraction = \"https://th.tripadvisor.com/Attraction_Review-g297930-d3387563-Reviews-Jungceylon-Patong_Kathu_Phuket.html\",\n",
    "        #     province_th = 'ภูเก็ต'\n",
    "        # )\n",
    "\n",
    "        # create data frame represent data scrape from current attraction card\n",
    "        cur_attraction_df = create_attraction_df(attraction=cur_attraction)\n",
    "\n",
    "        # concat all data frame result\n",
    "        res_attraction_df = pd.concat([res_attraction_df, cur_attraction_df])\n",
    "    \n",
    "    return res_attraction_df.iloc[1:, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory res_attraction_scraping created successfully\n",
      "scraping attraction | province --> ภูเก็ต | page --> 0\n",
      "check current page url -->  https://th.tripadvisor.com/Attractions-g293920-Activities-a_allAttractions.true-Phuket.html\n",
      "b1\n",
      "debug get_all_url_by_page: attraction by one page section\n",
      "retry find get_all_url_by_page ...\n",
      "check current page url -->  https://th.tripadvisor.com/Attractions-g293920-Activities-a_allAttractions.true-Phuket.html\n",
      "b1\n",
      "debug get_all_url_by_page: attraction by one page section\n",
      "retry find get_all_url_by_page ...\n",
      "check current page url -->  https://th.tripadvisor.com/Attractions-g293920-Activities-a_allAttractions.true-Phuket.html\n",
      "b1\n",
      "debug get_all_url_by_page: attraction by one page section\n",
      "b2\n",
      "debug get_all_url_by_page: text\n",
      "b3\n",
      "check in loop ...\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297930-d1866109-Reviews-Bangla_Road-Patong_Kathu_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1389361-d2433844-Reviews-Big_Buddha_Phuket-Chalong_Phuket_Town_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1224343-d8550982-Reviews-Banana_Beach-Ko_He_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1224250-d13237105-Reviews-Green_Elephant_Sanctuary_Park-Choeng_Thale_Thalang_District_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1210687-d450974-Reviews-Kata_Beach-Kata_Beach_Karon_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297930-d2454044-Reviews-Patong_Beach-Patong_Kathu_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g10804710-d450973-Reviews-Karon_Beach-Karon_Beach_Karon_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297930-d3387563-Reviews-Jungceylon-Patong_Kathu_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1231757-d553203-Reviews-Nai_Harn_Beach-Nai_Harn_Rawai_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297931-d23965533-Reviews-Andamanda_Phuket-Kathu_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297930-d778266-Reviews-Freedom_Beach-Patong_Kathu_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2315818-d5995325-Reviews-Old_Phuket_Town-Talat_Yai_Phuket_Town_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1379324-d553530-Reviews-Kamala_Beach-Kamala_Kathu_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1389361-d553516-Reviews-Chaithararam_Temple_Wat_Chalong-Chalong_Phuket_Town_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1231756-d24186033-Reviews-Phuket_Elephant_Care-Nai_Thon_Thalang_District_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1215781-d2032284-Reviews-Naka_Market-Phuket_Town_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g293920-d6648473-Reviews-Soi_Dog_Foundation-Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2315814-d1965786-Reviews-Central_Phuket-Wichit_Phuket_Town_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1215780-d1161264-Reviews-Kata_Noi_Beach-Karon_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1224250-d1161253-Reviews-Surin_Beach-Choeng_Thale_Thalang_District_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297934-d553523-Reviews-Promthep_Cape-Rawai_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g293920-d592496-Reviews-Bang_Tao_Beach-Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g293920-d1149585-Reviews-Mai_Khao_Beach-Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1215781-d8776186-Reviews-Sunday_Walking_Street_Market_Lard_Yai-Phuket_Town_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1215773-d1036078-Reviews-Naiyang_Beach-Nai_Yang_Sakhu_Thalang_District_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1215780-d3448295-Reviews-Karon_Viewpoint-Karon_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1215781-d3830436-Reviews-Monkey_Hill-Phuket_Town_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297937-d553534-Reviews-Gibbon_Rehabilitation_Project-Thalang_District_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g293920-d631985-Reviews-Koh_Yao_Yai-Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297934-d1378849-Reviews-Rawai_Beach-Rawai_Phuket.html\n",
      "******************************************************\n",
      "scrape single attraction...\n",
      "for attraction :  https://th.tripadvisor.com/Attraction_Review-g297930-d1866109-Reviews-Bangla_Road-Patong_Kathu_Phuket.html\n",
      "debug scrape_single_attraction: common component section\n",
      "scrape data in adjust attraction page...\n",
      "for link :  https://th.tripadvisor.com/ImproveListing-d1866109.html\n",
      "debug option of adjust page: \n",
      "found target dropdown btn ...\n",
      "lat :  7.893388\n",
      "long :  98.29736\n",
      "can't find openingHours ...\n",
      "openingHours :  {}\n",
      "name ->  ถนนบางลา\n",
      "description ->  \n",
      "rating -->  4.0\n",
      "rating_count -->  21017\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "find image element ->  15\n",
      "cur img path ->  ['https://media-cdn.tripadvisor.com/media/photo-s/05/1b/97/47/bangla-road.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/ae/4c/07/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/ae/4c/06/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/ae/4c/05/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/ae/4c/04/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-p/2d/ae/4c/09/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/ae/4b/4f/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/ae/4b/4e/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/97/e4/4a/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/95/65/26/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/95/65/25/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/95/65/24/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/95/65/23/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/02/b9/e5/f4/bangla-road.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/02/10/38/e8/bangla-road-in-the-day.jpg']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m cur_province_url \u001b[38;5;241m=\u001b[39m cur_region_data[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# get dataframe result of all attraction in current province\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m cur_res_allAttractions_df \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_attraction_by_province\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovince_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcur_province_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovince\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcur_province_th\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# don't forget to remove row with lat/long be zero\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# remove duplicate attraction\u001b[39;00m\n\u001b[0;32m     22\u001b[0m cur_res_allAttractions_df\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[8], line 16\u001b[0m, in \u001b[0;36mscrape_attraction_by_province\u001b[1;34m(province_url, province)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# use data from 'res_get_data_by_page' to retrive data of specific attraction\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cur_attraction_url \u001b[38;5;129;01min\u001b[39;00m all_url_by_page:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# if(cnt_for_debug == 3):\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# continue scraping data for a specific resgtaurant\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     cur_attraction \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_single_attraction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlink_to_attraction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcur_attraction_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprovince_th\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprovince\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     cnt_for_debug \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# cur_attraction = scrape_single_attraction(\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m#     link_to_attraction = \"https://th.tripadvisor.com/Attraction_Review-g297930-d3387563-Reviews-Jungceylon-Patong_Kathu_Phuket.html\",\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m#     province_th = 'ภูเก็ต'\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# create data frame represent data scrape from current attraction card\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 128\u001b[0m, in \u001b[0;36mscrape_single_attraction\u001b[1;34m(link_to_attraction, province_th)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcur img path -> \u001b[39m\u001b[38;5;124m\"\u001b[39m, img_path)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# find location\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m location \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_location\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattraction_page_driver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattraction_page_driver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatitude\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlongitude\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovince_th\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprovince_th\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovince :\u001b[39m\u001b[38;5;124m\"\u001b[39m, location\u001b[38;5;241m.\u001b[39mget_province_code(), location\u001b[38;5;241m.\u001b[39mget_province())\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistrict :\u001b[39m\u001b[38;5;124m\"\u001b[39m, location\u001b[38;5;241m.\u001b[39mget_district_code(), location\u001b[38;5;241m.\u001b[39mget_district())\n",
      "Cell \u001b[1;32mIn[4], line 43\u001b[0m, in \u001b[0;36mscrape_location\u001b[1;34m(attraction_page_driver, latitude, longitude, province_th)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# options.add_argument(\"--headless=new\")\u001b[39;00m\n\u001b[0;32m     39\u001b[0m options\u001b[38;5;241m.\u001b[39madd_experimental_option(\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefs\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprofile.managed_default_content_settings.images\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m}\n\u001b[0;32m     41\u001b[0m )\n\u001b[1;32m---> 43\u001b[0m google_map_driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m google_map_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.google.com/maps/search/?api=1&query=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (latitude, longitude)\n\u001b[0;32m     46\u001b[0m google_map_driver\u001b[38;5;241m.\u001b[39mget(google_map_query)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\seleniumwire\\webdriver.py:218\u001b[0m, in \u001b[0;36mChrome.__init__\u001b[1;34m(self, seleniumwire_options, *args, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m         caps \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdesired_capabilities\u001b[39m\u001b[38;5;124m'\u001b[39m, DesiredCapabilities\u001b[38;5;241m.\u001b[39mCHROME\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[0;32m    216\u001b[0m         caps\u001b[38;5;241m.\u001b[39mupdate(config)\n\u001b[1;32m--> 218\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[0;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:55\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     52\u001b[0m     options\u001b[38;5;241m.\u001b[39mbrowser_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m finder\u001b[38;5;241m.\u001b[39mget_driver_path()\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m executor \u001b[38;5;241m=\u001b[39m ChromiumRemoteConnection(\n\u001b[0;32m     58\u001b[0m     remote_server_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mservice_url,\n\u001b[0;32m     59\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mbrowser_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m     ignore_proxy\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m_ignore_local_proxy,\n\u001b[0;32m     63\u001b[0m )\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py:103\u001b[0m, in \u001b[0;36mService.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_process_still_running()\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_connectable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m# sleep increasing: 0.01, 0.06, 0.11, 0.16, 0.21, 0.26, 0.31, 0.36, 0.41, 0.46, 0.5\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py:120\u001b[0m, in \u001b[0;36mService.is_connectable\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_connectable\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Establishes a socket connection to determine if the service running\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    on the port is accessible.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_connectable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\selenium\\webdriver\\common\\utils.py:101\u001b[0m, in \u001b[0;36mis_connectable\u001b[1;34m(port, host)\u001b[0m\n\u001b[0;32m     99\u001b[0m socket_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 101\u001b[0m     socket_ \u001b[38;5;241m=\u001b[39m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _is_connectable_exceptions:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\socket.py:837\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[0;32m    836\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m--> 837\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[0;32m    839\u001b[0m exceptions\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create directory 'res_attraction_scraping'\n",
    "createDirectory(fh.STORE_ATTRACTION_SCRAPING, 'res_attraction_scraping')\n",
    "\n",
    "# *** select one province from 'ALL_PROVINCE_TRIPADVISOR_DATA'\n",
    "# *** so, change \"Idx_of_region\" everytime when scrape another province\n",
    "Idx_of_region = 0\n",
    "cur_region_data = ALL_PROVINCE_TRIPADVISOR_DATA[Idx_of_region]\n",
    "\n",
    "cur_province_en = cur_region_data[0]\n",
    "cur_province_th = cur_region_data[1]\n",
    "cur_province_url = cur_region_data[2]\n",
    "\n",
    "# get dataframe result of all attraction in current province\n",
    "cur_res_allAttractions_df = scrape_attraction_by_province(\n",
    "    province_url = cur_province_url,\n",
    "    province = cur_province_th\n",
    ")\n",
    "\n",
    "# don't forget to remove row with lat/long be zero\n",
    "\n",
    "# remove duplicate attraction\n",
    "cur_res_allAttractions_df.drop_duplicates(subset=['name'], inplace=True)\n",
    "# set new index\n",
    "cur_res_allAttractions_df.set_index(['name'], inplace=True)\n",
    "\n",
    "# save result dataframe to .csv\n",
    "res_file_name = 'res_attraction_%s.csv' % (cur_province_en)\n",
    "res_path = os.path.join(fh.STORE_ATTRACTION_SCRAPING, 'res_attraction_scraping', res_file_name) \n",
    "cur_res_allAttractions_df.to_csv(res_path, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
