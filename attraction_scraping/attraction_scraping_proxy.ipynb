{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pyautogui\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import constants.constants as const\n",
    "import constants.file_handler_constants as fh\n",
    "from constants.attraction_constants import *\n",
    "\n",
    "from packages.attraction.Attraction import *\n",
    "from packages.file_handler_package.file_handler import *\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv, dotenv_values \n",
    "\n",
    "from selenium.webdriver import Remote, ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.actions.wheel_input import ScrollOrigin\n",
    "from selenium.webdriver import ActionChains\n",
    "\n",
    "from seleniumwire import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from selenium.webdriver.edge.options import Options\n",
    "\n",
    "\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attraction_df(attraction: Attraction) -> pd.DataFrame:\n",
    "    attraction_dict = {\n",
    "        'name' : [attraction.get_name()],\n",
    "        'type' : [attraction.get_type()],\n",
    "        'description' : [attraction.get_description()],\n",
    "        'latitude' : [attraction.get_latitude()],\n",
    "        'longitude' : [attraction.get_longitude()],\n",
    "        'imgPath' : [attraction.get_imgPath()],\n",
    "        'phone': [attraction.get_phone()],\n",
    "        'website': [attraction.get_website()],\n",
    "        'openingHour': [attraction.get_openingHour()],\n",
    "\n",
    "        # location\n",
    "        'address' : [attraction.get_location().get_address()],\n",
    "        'province' : [attraction.get_location().get_province()],\n",
    "        'district' : [attraction.get_location().get_district()],\n",
    "        'subDistrict' : [attraction.get_location().get_sub_district()],\n",
    "        'province_code' : [attraction.get_location().get_province_code()],\n",
    "        'district_code' : [attraction.get_location().get_district_code()],\n",
    "        'sub_district_code' : [attraction.get_location().get_sub_district_code()],\n",
    "\n",
    "        # rating\n",
    "        'score' : [attraction.get_rating().get_score()],\n",
    "        'ratingCount' : [attraction.get_rating().get_ratingCount()],\n",
    "    }\n",
    "\n",
    "    for cur_tag in ATTRACTION_TAG_SCORE:\n",
    "        attraction_dict[cur_tag] = attraction.get_attractionTag().get_tag_score(cur_tag)\n",
    "\n",
    "    attraction_df = pd.DataFrame(attraction_dict)\n",
    "    \n",
    "    return attraction_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_img(attraction_page_driver: webdriver) -> list[str]:\n",
    "    \n",
    "    res_imgPath = []\n",
    "\n",
    "    possible_click_img_xpath = [\n",
    "        '//*[@id=\"AR_ABOUT\"]/div[2]/div/div/div/div/div[1]/div/div/div/div[1]/div/div[7]/button',\n",
    "        '//*[@id=\"AR_ABOUT\"]/div/div/div/div/div/div[1]/div/div/div/div[1]/div/div[7]/button'\n",
    "    ]\n",
    "    \n",
    "    btn_img_xpath = \"\"\n",
    "    for cur_xpath in possible_click_img_xpath:\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, cur_xpath)))\n",
    "            btn_img_xpath = cur_xpath\n",
    "            break\n",
    "        \n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    if(not len(btn_img_xpath)):\n",
    "        print(\"can't scrape img (no img ?)\")\n",
    "        return ['']\n",
    "\n",
    "    # find button and click\n",
    "    # to see modal then scrape image address\n",
    "    try:\n",
    "        WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, btn_img_xpath)))\n",
    "        click_img_btn = attraction_page_driver.find_element(By.XPATH, btn_img_xpath)\n",
    "        click_img_btn.click()\n",
    "        is_end_scrape_img = False\n",
    "        cnt_retry = 0\n",
    "        while(not is_end_scrape_img):\n",
    "            if(cnt_retry == 10):\n",
    "                print(\"max retry for scrape image...\")\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                WebDriverWait(attraction_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'cfCAA')))\n",
    "                all_img_elements = attraction_page_driver.find_elements(By.CLASS_NAME, 'cfCAA')\n",
    "                print(\"find image element -> \", len(all_img_elements))\n",
    "                for cur_img_element in all_img_elements:\n",
    "                    cur_bgImg_val = cur_img_element.value_of_css_property('background-image')\n",
    "                    match = re.search(r'url\\(\"(.*?)\"\\)', cur_bgImg_val)\n",
    "                    if match:\n",
    "                        res_imgPath.append(match.group(1))\n",
    "\n",
    "                is_end_scrape_img = True\n",
    "\n",
    "            except Exception as e:\n",
    "                cnt_retry += 1\n",
    "                print(\"retry scrape img...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "            pass\n",
    "    \n",
    "\n",
    "    return res_imgPath.copy()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_location(attraction_page_driver: webdriver, latitude: float, longitude: float, province_th: str) -> Location:\n",
    "\n",
    "    # find better address description on wongnai\n",
    "    # for example: \"991 ถนนพระราม 1 Pathum Wan, กรุงเทพมหานคร (กทม.) 10330 ไทย\"\n",
    "    address_tripAdvisor = \"\"\n",
    "    possible_address_xpath = [\n",
    "        '//*[@id=\"tab-data-WebPresentation_PoiLocationSectionGroup\"]/div/div/div[2]/div[1]/div/div/div/div[1]/button/span',\n",
    "        '//*[@id=\"tab-data-WebPresentation_PoiLocationSectionGroup\"]/div/div/div[2]/div[1]/div/div/div/div/button/span'\n",
    "    ]\n",
    "\n",
    "\n",
    "    for cur_address_xpath in possible_address_xpath:\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, cur_address_xpath)))\n",
    "            address_element = attraction_page_driver.find_element(By.XPATH, cur_address_xpath)\n",
    "            address_tripAdvisor = address_element.text\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "\n",
    "    # start scrape location\n",
    "    res_location = Location()\n",
    "    cnt_retry = 0\n",
    "    try:\n",
    "        while(True):\n",
    "            if(cnt_retry == 10):\n",
    "                print(\"max retry for scrape Google Map ...\")\n",
    "                break\n",
    "            \n",
    "            # set up new webdriver to work googlemap url(query for specific lat/long)\n",
    "            possible_addressGoogleMap_elements = []\n",
    "            try:\n",
    "                # set Chrome options to run in headless mode\n",
    "                # options = Options()\n",
    "                options = webdriver.ChromeOptions()\n",
    "                options.add_argument(\"start-maximized\")\n",
    "                # options.add_argument(\"--headless=new\")\n",
    "                options.add_experimental_option(\n",
    "                    \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "                )\n",
    "\n",
    "                google_map_driver = webdriver.Chrome(options=options)\n",
    "                \n",
    "                google_map_query = \"https://www.google.com/maps/search/?api=1&query=%s,%s\" % (latitude, longitude)\n",
    "                google_map_driver.get(google_map_query)\n",
    "                print(\"scrape location data for, \", google_map_query)\n",
    "                \n",
    "                WebDriverWait(google_map_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'DkEaL')))\n",
    "                possible_addressGoogleMap_elements = google_map_driver.find_elements(By.CLASS_NAME, 'DkEaL')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"retry  scrape Google Map..\")\n",
    "                cnt_retry += 1\n",
    "                google_map_driver.close()\n",
    "                continue\n",
    "\n",
    "\n",
    "            # after init new webdriver -> continure scrape location data\n",
    "\n",
    "            # if found some wiered place that doesn't even have its address\n",
    "            # skip this case for now...\n",
    "            if(not len(possible_addressGoogleMap_elements)):\n",
    "                return res_location\n",
    "\n",
    "            subStrDistrict = \"อำเภอ\"\n",
    "            subStrSubDistrict = \"ตำบล\"\n",
    "\n",
    "            if province_th == \"กรุงเทพมหานคร\":\n",
    "                subStrDistrict = \"เขต\"\n",
    "                subStrSubDistrict = \"แขวง\"\n",
    "\n",
    "            district = 0\n",
    "            subDirstrict = 0\n",
    "\n",
    "            # find location\n",
    "            useData = None\n",
    "            for cur_element in possible_addressGoogleMap_elements:\n",
    "                if province_th in cur_element.text and cur_element.text.find(subStrDistrict) != -1:\n",
    "                    useData = cur_element.text.replace(\",\",\"\").replace(\"เเ\",\"แ\")\n",
    "                    break\n",
    "           \n",
    "            if(useData != None):\n",
    "                # print(\"Full Address :\",useData)\n",
    "                # another brute force way in case of province 'กรุงเทพหมานคร' not have word 'แขวง' in address\n",
    "                if(province_th == 'กรุงเทพมหานคร' and useData.find(subStrSubDistrict) == -1):\n",
    "                    subAddress_split = useData.split(' ')\n",
    "                    cur_province_Idx = subAddress_split.index(province_th)\n",
    "                    district = subAddress_split[cur_province_Idx - 1].replace(\"เขต\",\"\")\n",
    "\n",
    "                else:\n",
    "                    start_address_index = useData.find(subStrDistrict)\n",
    "                    subAddress = useData[start_address_index:]\n",
    "                    district = subAddress[subAddress.find(subStrDistrict)+len(subStrDistrict):subAddress.find(province_th)].replace(\" \",\"\")               \n",
    "\n",
    "                if district == \"เมือง\":\n",
    "                    district = district+province_th\n",
    "\n",
    "                # filter row to find 'ISO_3166_code', 'zip_code', 'geo_code'\n",
    "                geo_code_df = pd.read_csv(fh.PATH_TO_GEOCODE)\n",
    "                filtered_rows = geo_code_df[\n",
    "                    (geo_code_df['province_th'] == province_th) & (geo_code_df['district_th'] == district)\n",
    "                ]\n",
    "                filtered_rows.reset_index(inplace=True, drop=True)\n",
    "                \n",
    "                if not filtered_rows.empty:\n",
    "                    print(\"found province :\",filtered_rows.loc[0, 'ISO_3166_code'], province_th)\n",
    "                    print(\"found District :\",filtered_rows.loc[0, 'zip_code'], district)\n",
    "\n",
    "                    res_location.set_address(address_tripAdvisor if len(address_tripAdvisor) else useData)\n",
    "                    res_location.set_province(province_th)\n",
    "                    res_location.set_district(district)\n",
    "                    res_location.set_sub_district(\"\")\n",
    "                    res_location.set_province_code(filtered_rows.loc[0, 'ISO_3166_code'])\n",
    "                    res_location.set_district_code(filtered_rows.loc[0, 'zip_code'])\n",
    "                    res_location.set_sub_district_code(0)\n",
    "\n",
    "                else:\n",
    "                    print(\"not found province :\", province_th)\n",
    "                    print(\"not found District :\", district)\n",
    "\n",
    "                    res_location.set_address(address_tripAdvisor if len(address_tripAdvisor) else useData)\n",
    "                    res_location.set_province(province_th)\n",
    "                    res_location.set_district(district)\n",
    "                    res_location.set_sub_district(\"\")\n",
    "                    res_location.set_province_code(0)\n",
    "                    res_location.set_district_code(0)\n",
    "                    res_location.set_sub_district_code(0)\n",
    "\n",
    "            google_map_driver.close()\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"can't scrape location data\")\n",
    "\n",
    "    return res_location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape lat/long, openingHours, types (there are in another page of current attraction)\n",
    "def scrape_adjust_page(link_to_adjust_page: str) -> tuple[float, float, dict, list[str]]:\n",
    "    lat = 0\n",
    "    long = 0\n",
    "    openingHours = {}\n",
    "    types = []\n",
    "\n",
    "    # create new webdriver to continue scrape lat/long, openingHours in adjust attraction page\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape single attraction ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "\n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        adjust_page_driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # retry in case of web restrictions, some elements not loaded\n",
    "        try:\n",
    "            print(\"scrape data in adjust attraction page...\")\n",
    "            print(\"for link : \", link_to_adjust_page)\n",
    "            adjust_page_driver.get(link_to_adjust_page)\n",
    "\n",
    "            print(\"debug option of adjust page: \")\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'DiHOR')))\n",
    "\n",
    "            # find dropdown --> click display data below --> cick display lat/long input form\n",
    "            possible_target_btn = adjust_page_driver.find_elements(By.CLASS_NAME, 'DiHOR')\n",
    "            for cur_dropdown_btn in possible_target_btn:\n",
    "                cur_dropdown_text = cur_dropdown_btn.text\n",
    "                if(\"แนะนำการแก้ไขข้อมูลของสถานที่นี้\" in cur_dropdown_text):\n",
    "                    print(\"found target dropdown btn ...\")\n",
    "                    cur_dropdown_btn.click()\n",
    "                    WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/button')))\n",
    "                    # find button click to display lat/long input form\n",
    "                    display_lat_long_btn = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/button')\n",
    "                    display_lat_long_btn.click()\n",
    "\n",
    "        except Exception as e:\n",
    "            cnt_retry += 1\n",
    "            adjust_page_driver.quit()\n",
    "            print(\"retry adjust page...\")\n",
    "            continue\n",
    "\n",
    "      \n",
    "        # find lat/long\n",
    "        try:\n",
    "            WebDriverWait(adjust_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[1]/div/div[2]/div/div/div/span')))\n",
    "            WebDriverWait(adjust_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[2]/div/div[2]/div/div/div/span')))\n",
    "    \n",
    "            lat_input_container = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[1]/div/div[2]/div/div/div/span')\n",
    "            lat_input_element = lat_input_container.find_element(By.TAG_NAME, 'input')\n",
    "            lat = float(lat_input_element.get_attribute('value'))\n",
    "\n",
    "            long_input_container = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[2]/div/div[2]/div/div/div/span')\n",
    "            long_input_element = long_input_container.find_element(By.TAG_NAME, 'input')\n",
    "            long = float(long_input_element.get_attribute('value'))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find lat/long\")\n",
    "        \n",
    "        print(\"lat : \", lat)\n",
    "        print(\"long : \", long)\n",
    "\n",
    "        # **if can't find lat/long --> don't scrape this attaction\n",
    "        if(lat == 0 and long == 0):\n",
    "            print(\"in scrape_adjust_page --> can't find lat/long --> 0, 0\")\n",
    "            return lat, long, openingHours.copy(), types.copy()\n",
    "\n",
    "\n",
    "        # find openingHours\n",
    "        try:\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'dNAjp')))\n",
    "            all_openingHours_container = adjust_page_driver.find_elements(By.CLASS_NAME, 'dNAjp')\n",
    "            for cur_openingHours_container in all_openingHours_container:\n",
    "                cur_day_element = cur_openingHours_container.find_element(By.CLASS_NAME, 'ngXxk')\n",
    "                cur_day_text = cur_day_element.text.replace(\":\", \"\")\n",
    "\n",
    "                cur_time_element = cur_openingHours_container.find_element(By.CLASS_NAME, 'KxBGd')\n",
    "                cur_time_text = cur_time_element.text\n",
    "\n",
    "                openingHours[cur_day_text] = cur_time_text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find openingHours ...\")\n",
    "\n",
    "        print(\"openingHours : \", openingHours.copy())\n",
    "\n",
    "\n",
    "        # find types\n",
    "        try:\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'ZCWaz')))\n",
    "            all_type_elements = adjust_page_driver.find_elements(By.CLASS_NAME, 'ZCWaz')\n",
    "            for cur_element in all_type_elements:\n",
    "                cur_type_text = cur_element.text\n",
    "                types.append(cur_type_text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find types ...\")\n",
    "\n",
    "        print(\"types : \", types.copy())\n",
    "        \n",
    "\n",
    "        adjust_page_driver.quit()\n",
    "        break\n",
    "\n",
    "    return lat, long, openingHours.copy(), types.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_attraction(link_to_attraction: str, province_th: str) -> Attraction:\n",
    "    \n",
    "    attraction = Attraction()\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape single attraction ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", \n",
    "            {\n",
    "                \"profile.managed_default_content_settings.images\": 2, # Disable image\n",
    "                \"profile.default_content_setting_values.cookies\": 2,  # Block all cookies\n",
    "                \"profile.default_content_settings.popups\": 0,         # Disable popups\n",
    "                \"profile.managed_default_content_settings.cookies\": 2  # Disable third-party cookies\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        attraction_page_driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            print(\"******************************************************\")\n",
    "            print(\"scrape single attraction...\")\n",
    "            print(\"for attraction : \", link_to_attraction)\n",
    "            attraction_page_driver.get(link_to_attraction)\n",
    "\n",
    "            print(\"debug scrape_single_attraction: common component section\")\n",
    "            WebDriverWait(attraction_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'IDaDx')))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"retry single attraction case 1...\")\n",
    "            cnt_retry += 1\n",
    "            attraction_page_driver.quit()\n",
    "            continue\n",
    "        \n",
    "        # convert attraction url to adjust page url\n",
    "        # for example: from 'https://th.tripadvisor.com/Attraction_Review-g297930-d1866109-Reviews-Bangla_Road-Patong_Kathu_Phuket.html' to 'https://th.tripadvisor.com/ImproveListing-d1866109.html'\n",
    "        link_to_adjust_page = 'https://th.tripadvisor.com/ImproveListing-%s.html' % (link_to_attraction.split('-')[2])\n",
    "\n",
    "        # ** find lat/long, openingHours, types (there are in another page of current attraction)\n",
    "        # ** if this attraction not have lat/long\n",
    "        # ** don't continue to scrape\n",
    "        lat, long, openingHours, types = scrape_adjust_page(\n",
    "            link_to_adjust_page = link_to_adjust_page\n",
    "        )\n",
    "        \n",
    "        # **if can't find lat/long --> don't scrape this attaction\n",
    "        if(lat == 0 and long == 0):\n",
    "            print(\"in scrape_single_attraction --> can't find lat/long --> don't scrape this attraction ...\")\n",
    "            attraction_page_driver.quit()\n",
    "            return attraction\n",
    "\n",
    "        # find name\n",
    "        name = \"\"\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[1]/header/div[3]/div[1]/div/h1')))\n",
    "            name_element = attraction_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[1]/header/div[3]/div[1]/div/h1')\n",
    "            name = name_element.text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find name\")\n",
    "\n",
    "        print(\"name -> \", name)\n",
    "\n",
    "        # find description\n",
    "        description = \"\"\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"AR_ABOUT\"]/div[1]')))\n",
    "            \n",
    "            description_container = attraction_page_driver.find_element(By.XPATH, '//*[@id=\"AR_ABOUT\"]/div[1]')\n",
    "            header_element = description_container.find_element(By.CLASS_NAME, 'biGQs')\n",
    "            header_text = header_element.text\n",
    "            if(header_text == 'ข้อมูล'):\n",
    "                description_element = attraction_page_driver.find_element(By.CLASS_NAME, 'JguWG')\n",
    "                description = description_element.text\n",
    "                \n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find description\")\n",
    "\n",
    "        print(\"description -> \", description)\n",
    "        \n",
    "        # find rating\n",
    "        rating = 0\n",
    "        rating_count = 0\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[2]/div[2]/div/div[1]/section[1]/div/div/div/div/div[1]/div[1]/a/div')))\n",
    "            score_element = attraction_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[2]/div[2]/div/div[1]/section[1]/div/div/div/div/div[1]/div[1]/a/div')\n",
    "            score_text_list = score_element.get_attribute('aria-label').split(' ')\n",
    "            for Idx in range(1, len(score_text_list)):\n",
    "                # set rating\n",
    "                if(score_text_list[Idx - 1] == \"คะแนน\"):\n",
    "                    rating = float(score_text_list[Idx])\n",
    "\n",
    "                elif(score_text_list[Idx - 1] == \"รีวิว\"):\n",
    "                    rating_count = int(score_text_list[Idx].replace(',', ''))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find rating and rating_count\")\n",
    "\n",
    "        print(\"rating --> \", rating)\n",
    "        print(\"rating_count --> \", rating_count)\n",
    "\n",
    "        # find img_path\n",
    "        img_path = scrape_img(attraction_page_driver)\n",
    "        print(\"cur img path -> \", img_path)\n",
    "\n",
    "        # find location\n",
    "        location = scrape_location(\n",
    "            attraction_page_driver = attraction_page_driver,\n",
    "            latitude = lat,\n",
    "            longitude = long,\n",
    "            province_th = province_th\n",
    "        )\n",
    "        print(\"province :\", location.get_province_code(), location.get_province())\n",
    "        print(\"District :\", location.get_district_code(), location.get_district())\n",
    "\n",
    "        # set some of \"Attraction\" object properties\n",
    "        attraction.set_name(name)\n",
    "        attraction.set_type(types)\n",
    "        attraction.set_description(description)\n",
    "        attraction.set_latitude(lat)\n",
    "        attraction.set_longitude(long)\n",
    "        attraction.set_imgPath(img_path)\n",
    "        attraction.set_website(link_to_attraction)\n",
    "        attraction.set_openingHour(openingHours)\n",
    "        attraction.set_location(\n",
    "            address = location.get_address(),\n",
    "            province = location.get_province(),\n",
    "            district = location.get_district(),\n",
    "            sub_district = location.get_sub_district(),\n",
    "            province_code = location.get_province_code(),\n",
    "            district_code = location.get_district_code(),\n",
    "            sub_district_code = location.get_sub_district_code()\n",
    "        )\n",
    "        attraction.set_rating(\n",
    "            score = rating,\n",
    "            rating_count = rating_count\n",
    "        )\n",
    "\n",
    "\n",
    "        attraction_page_driver.quit()\n",
    "        break\n",
    "\n",
    "    return attraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_url_by_page(query_url: str) -> list[str]:\n",
    "\n",
    "    res_url_by_page = []\n",
    "\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        if(cnt_retry == 10):\n",
    "            print(\"max retry for scrape data by page ...\")\n",
    "            break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        # os.environ['proxy_port']\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "      \n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # just check for ip\n",
    "        # print(\"just check for ip :\")\n",
    "        # driver.get(\"https://httpbin.io/ip\")\n",
    "        # print(driver.page_source)\n",
    "\n",
    "        # find group of attraction on the nth page\n",
    "        all_attractions_card = []\n",
    "\n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            driver.get(query_url)\n",
    "            # scroll and wait for some msec\n",
    "            driver.execute_script('window.scrollBy(0, document.body.scrollHeight)')\n",
    "            \n",
    "            print(\"check current page url --> \", driver.current_url)\n",
    "\n",
    "            # wait for div (each attraction section) to be present and visible\n",
    "            print(\"b1\")\n",
    "            print(\"debug get_all_url_by_page: attraction by one page section\")\n",
    "            WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'XJlaI')))\n",
    "\n",
    "            print(\"b2\")\n",
    "            print(\"debug get_all_url_by_page: text\")\n",
    "            WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'BKifx')))\n",
    "            \n",
    "            # print(\"b3\")\n",
    "            # print(\"debug get_all_url_by_page: link to single attraction\")\n",
    "            # WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.TAG_NAME, 'a')))\n",
    "\n",
    "            print(\"b3\")\n",
    "            print(\"check in loop ...\")\n",
    "            all_attractions_card = driver.find_elements(By.CLASS_NAME, 'XJlaI')\n",
    "            for cur_attraction_card in all_attractions_card:\n",
    "\n",
    "                cur_attraction_url = cur_attraction_card.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "\n",
    "                check_text = cur_attraction_card.find_element(By.CLASS_NAME, 'BKifx').text  \n",
    "               \n",
    "                # check if cuurent card is for attraction ?\n",
    "                not_attraction_keyword = ['ทัวร์', \"สปา\", \"กิจกรรมทางวัฒนธรรม\", 'ชั้นเรียน', 'รถรับส่ง', 'อุปกรณ์ให้เช่า', 'ร้านขายของ']\n",
    "                for cur_check_word in not_attraction_keyword:\n",
    "                    if(cur_check_word in check_text):\n",
    "                        break\n",
    "\n",
    "                print(\"cur_attraction_url : \", cur_attraction_url)\n",
    "                res_url_by_page.append(cur_attraction_url)\n",
    "            \n",
    "            driver.quit()\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"retry find get_all_url_by_page ...\")\n",
    "            cnt_retry += 1\n",
    "            driver.quit()\n",
    "            continue\n",
    "\n",
    "    return res_url_by_page.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_attraction_by_province(province_url: str, province: str) -> pd.DataFrame:\n",
    "    # res_attraction_df = pd.DataFrame()\n",
    "    res_attraction_df = create_attraction_df(Attraction())\n",
    "    \n",
    "    cnt_for_debug = 0\n",
    "        \n",
    "    print(\"scraping attraction | province --> %s | page --> %s\" % (province, cnt_for_debug))\n",
    "\n",
    "    all_url_by_page = get_all_url_by_page(query_url=province_url)\n",
    "\n",
    "    # use data from 'res_get_data_by_page' to retrive data of specific attraction\n",
    "    for cur_attraction_url in all_url_by_page:\n",
    "        if(cnt_for_debug == 3):\n",
    "            break\n",
    "        # continue scraping data for a specific resgtaurant\n",
    "        cur_attraction = scrape_single_attraction(\n",
    "            link_to_attraction = cur_attraction_url,\n",
    "            province_th = province\n",
    "        )\n",
    "        cnt_for_debug += 1\n",
    "        # cur_attraction = scrape_single_attraction(\n",
    "        #     link_to_attraction = \"https://th.tripadvisor.com/Attraction_Review-g297930-d3387563-Reviews-Jungceylon-Patong_Kathu_Phuket.html\",\n",
    "        #     province_th = 'ภูเก็ต'\n",
    "        # )\n",
    "\n",
    "        # create data frame represent data scrape from current attraction card\n",
    "        cur_attraction_df = create_attraction_df(attraction=cur_attraction)\n",
    "\n",
    "        # concat all data frame result\n",
    "        res_attraction_df = pd.concat([res_attraction_df, cur_attraction_df])\n",
    "    \n",
    "    return res_attraction_df.iloc[1:, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory res_attraction_scraping created successfully\n",
      "scraping attraction | province --> ภูเก็ต | page --> 0\n",
      "check current page url -->  https://th.tripadvisor.com/Attractions-g293920-Activities-a_allAttractions.true-Phuket.html\n",
      "b1\n",
      "debug get_all_url_by_page: attraction by one page section\n",
      "b2\n",
      "debug get_all_url_by_page: text\n",
      "b3\n",
      "check in loop ...\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297930-d1866109-Reviews-Bangla_Road-Patong_Kathu_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1389361-d2433844-Reviews-Big_Buddha_Phuket-Chalong_Phuket_Town_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1224343-d8550982-Reviews-Banana_Beach-Ko_He_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1224250-d13237105-Reviews-Green_Elephant_Sanctuary_Park-Choeng_Thale_Thalang_District_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1210687-d450974-Reviews-Kata_Beach-Kata_Beach_Karon_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297930-d2454044-Reviews-Patong_Beach-Patong_Kathu_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g10804710-d450973-Reviews-Karon_Beach-Karon_Beach_Karon_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297930-d3387563-Reviews-Jungceylon-Patong_Kathu_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1231757-d553203-Reviews-Nai_Harn_Beach-Nai_Harn_Rawai_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297931-d23965533-Reviews-Andamanda_Phuket-Kathu_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297930-d778266-Reviews-Freedom_Beach-Patong_Kathu_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2315818-d5995325-Reviews-Old_Phuket_Town-Talat_Yai_Phuket_Town_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1379324-d553530-Reviews-Kamala_Beach-Kamala_Kathu_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1389361-d553516-Reviews-Chaithararam_Temple_Wat_Chalong-Chalong_Phuket_Town_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1231756-d24186033-Reviews-Phuket_Elephant_Care-Nai_Thon_Thalang_District_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1215781-d2032284-Reviews-Naka_Market-Phuket_Town_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g293920-d6648473-Reviews-Soi_Dog_Foundation-Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2315814-d1965786-Reviews-Central_Phuket-Wichit_Phuket_Town_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1215780-d1161264-Reviews-Kata_Noi_Beach-Karon_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1224250-d1161253-Reviews-Surin_Beach-Choeng_Thale_Thalang_District_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297934-d553523-Reviews-Promthep_Cape-Rawai_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g293920-d592496-Reviews-Bang_Tao_Beach-Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g293920-d1149585-Reviews-Mai_Khao_Beach-Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1215781-d8776186-Reviews-Sunday_Walking_Street_Market_Lard_Yai-Phuket_Town_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1215773-d1036078-Reviews-Naiyang_Beach-Nai_Yang_Sakhu_Thalang_District_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1215780-d3448295-Reviews-Karon_Viewpoint-Karon_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g1215781-d3830436-Reviews-Monkey_Hill-Phuket_Town_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297937-d553534-Reviews-Gibbon_Rehabilitation_Project-Thalang_District_Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g293920-d631985-Reviews-Koh_Yao_Yai-Phuket.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g297934-d1378849-Reviews-Rawai_Beach-Rawai_Phuket.html\n",
      "******************************************************\n",
      "scrape single attraction...\n",
      "for attraction :  https://th.tripadvisor.com/Attraction_Review-g297930-d1866109-Reviews-Bangla_Road-Patong_Kathu_Phuket.html\n",
      "debug scrape_single_attraction: common component section\n",
      "scrape data in adjust attraction page...\n",
      "for link :  https://th.tripadvisor.com/ImproveListing-d1866109.html\n",
      "debug option of adjust page: \n",
      "found target dropdown btn ...\n",
      "lat :  7.893388\n",
      "long :  98.29736\n",
      "can't find openingHours ...\n",
      "openingHours :  {}\n",
      "types :  ['สถานที่สำคัญ/จุดที่น่าสนใจ']\n",
      "name ->  ถนนบางลา\n",
      "description ->  \n",
      "rating -->  4.0\n",
      "rating_count -->  21021\n",
      "can't scrape img (no img ?)\n",
      "cur img path ->  ['']\n",
      "scrape location data for,  https://www.google.com/maps/search/?api=1&query=7.893388,98.29736\n",
      "found province : 83 ภูเก็ต\n",
      "found District : 8302 กะทู้\n",
      "province : 83 ภูเก็ต\n",
      "District : 8302 กะทู้\n",
      "******************************************************\n",
      "scrape single attraction...\n",
      "for attraction :  https://th.tripadvisor.com/Attraction_Review-g1389361-d2433844-Reviews-Big_Buddha_Phuket-Chalong_Phuket_Town_Phuket.html\n",
      "debug scrape_single_attraction: common component section\n",
      "scrape data in adjust attraction page...\n",
      "for link :  https://th.tripadvisor.com/ImproveListing-d2433844.html\n",
      "debug option of adjust page: \n",
      "found target dropdown btn ...\n",
      "lat :  7.827575\n",
      "long :  98.31284\n",
      "openingHours :  {'จันทร์': '08:00-18:30', 'อังคาร': '08:00-18:30', 'พุธ': '08:00-18:30', 'พฤหัสบดี': '08:00-18:30', 'ศุกร์': '08:00-18:30', 'เสาร์': '08:00-18:30', 'อาทิตย์': '08:00-18:30'}\n",
      "types :  ['สถานที่ทางศาสนาและสถานที่ศักดิ์สิทธิ์', 'สถานที่สำคัญ/จุดที่น่าสนใจ']\n",
      "name ->  พระใหญ่เมืองภูเก็ต\n",
      "description ->  \n",
      "rating -->  4.5\n",
      "rating_count -->  16875\n",
      "retry scrape img...\n",
      "find image element ->  15\n",
      "cur img path ->  ['https://media-cdn.tripadvisor.com/media/photo-s/16/a4/d0/06/photo0jpg.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/07/c8/08/fc/big-buddha.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/07/c8/08/fa/big-buddha.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/08/44/d3/4c/phuket-big-buddha.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/d8/99/2c/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/d8/99/2b/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/d8/99/27/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/d8/99/13/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/d8/99/1d/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/d8/99/18/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/d8/99/2a/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/d8/99/1c/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/d8/99/26/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/d8/99/12/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/d8/99/17/caption.jpg']\n",
      "scrape location data for,  https://www.google.com/maps/search/?api=1&query=7.827575,98.31284\n",
      "found province : 83 ภูเก็ต\n",
      "found District : 8301 เมืองภูเก็ต\n",
      "province : 83 ภูเก็ต\n",
      "District : 8301 เมืองภูเก็ต\n",
      "******************************************************\n",
      "scrape single attraction...\n",
      "for attraction :  https://th.tripadvisor.com/Attraction_Review-g1224343-d8550982-Reviews-Banana_Beach-Ko_He_Phuket.html\n",
      "debug scrape_single_attraction: common component section\n",
      "scrape data in adjust attraction page...\n",
      "for link :  https://th.tripadvisor.com/ImproveListing-d8550982.html\n",
      "debug option of adjust page: \n",
      "found target dropdown btn ...\n",
      "lat :  7.744772\n",
      "long :  98.38204\n",
      "can't find openingHours ...\n",
      "openingHours :  {}\n",
      "types :  ['ชายหาด']\n",
      "name ->  บานาน่า บีช\n",
      "description ->  \n",
      "rating -->  5.0\n",
      "rating_count -->  2391\n",
      "retry scrape img...\n",
      "find image element ->  15\n",
      "cur img path ->  ['https://media-cdn.tripadvisor.com/media/photo-s/18/22/c1/68/photo4jpg.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/c0/11/4b/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/c0/11/4a/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/bf/85/b9/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/bf/7e/67/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/bf/7e/66/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/bf/7e/65/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/bf/79/bf/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/08/c0/a5/42/banana-beach.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/08/c0/a5/23/banana-beach.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/08/c0/a5/14/banana-beach.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/08/c0/a4/bc/banana-beach.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/08/c0/a3/ec/banana-beach.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/08/c0/a3/b2/banana-beach.jpg', 'https://media-cdn.tripadvisor.com/media/photo-b/768x250/08/c0/a3/79/banana-beach.jpg']\n",
      "scrape location data for,  https://www.google.com/maps/search/?api=1&query=7.744772,98.38204\n",
      "found province : 83 ภูเก็ต\n",
      "found District : 8301 เมืองภูเก็ต\n",
      "province : 83 ภูเก็ต\n",
      "District : 8301 เมืองภูเก็ต\n"
     ]
    }
   ],
   "source": [
    "# create directory 'res_attraction_scraping'\n",
    "createDirectory(fh.STORE_ATTRACTION_SCRAPING, 'res_attraction_scraping')\n",
    "\n",
    "# *** select one province from 'ALL_PROVINCE_TRIPADVISOR_DATA'\n",
    "# *** so, change \"Idx_of_region\" everytime when scrape another province\n",
    "Idx_of_region = 0\n",
    "cur_region_data = ALL_PROVINCE_TRIPADVISOR_DATA[Idx_of_region]\n",
    "\n",
    "cur_province_en = cur_region_data[0]\n",
    "cur_province_th = cur_region_data[1]\n",
    "cur_province_url = cur_region_data[2]\n",
    "\n",
    "# get dataframe result of all attraction in current province\n",
    "cur_res_allAttractions_df = scrape_attraction_by_province(\n",
    "    province_url = cur_province_url,\n",
    "    province = cur_province_th\n",
    ")\n",
    "\n",
    "# don't forget to remove row with lat/long be zero\n",
    "\n",
    "# remove duplicate attraction\n",
    "cur_res_allAttractions_df.drop_duplicates(subset=['name'], inplace=True)\n",
    "# set new index\n",
    "cur_res_allAttractions_df.set_index(['name'], inplace=True)\n",
    "\n",
    "# save result dataframe to .csv\n",
    "res_file_name = 'res_attraction_%s.csv' % (cur_province_en)\n",
    "res_path = os.path.join(fh.STORE_ATTRACTION_SCRAPING, 'res_attraction_scraping', res_file_name) \n",
    "cur_res_allAttractions_df.to_csv(res_path, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
