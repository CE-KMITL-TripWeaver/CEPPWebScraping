{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pyautogui\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import constants.constants as const\n",
    "import constants.file_handler_constants as fh\n",
    "from constants.attraction_constants import *\n",
    "\n",
    "from packages.attraction.Attraction import *\n",
    "from packages.file_handler_package.file_handler import *\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv, dotenv_values \n",
    "\n",
    "from selenium.webdriver import Remote, ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.actions.wheel_input import ScrollOrigin\n",
    "from selenium.webdriver import ActionChains\n",
    "\n",
    "from seleniumwire import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from selenium.webdriver.edge.options import Options\n",
    "\n",
    "\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attraction_df(attraction: Attraction) -> pd.DataFrame:\n",
    "    attraction_dict = {\n",
    "        'name' : [attraction.get_name()],\n",
    "        'description' : [attraction.get_description()],\n",
    "        'latitude' : [attraction.get_latitude()],\n",
    "        'longitude' : [attraction.get_longitude()],\n",
    "        'imgPath' : [attraction.get_imgPath()],\n",
    "        'phone': [attraction.get_phone()],\n",
    "        'website': [attraction.get_website()],\n",
    "        'openingHour': [attraction.get_openingHour()],\n",
    "\n",
    "        # location\n",
    "        'address' : [attraction.get_location().get_address()],\n",
    "        'province' : [attraction.get_location().get_province()],\n",
    "        'district' : [attraction.get_location().get_district()],\n",
    "        'subDistrict' : [attraction.get_location().get_subDistrict()],\n",
    "        'province_code' : [attraction.get_location().get_province_code()],\n",
    "        'district_code' : [attraction.get_location().get_district_code()],\n",
    "        'sub_district_code' : [attraction.get_location().get_sub_district_code()],\n",
    "\n",
    "        # rating\n",
    "        'score' : [attraction.get_rating().get_score()],\n",
    "        'ratingCount' : [attraction.get_rating().get_ratingCount()],\n",
    "    }\n",
    "\n",
    "    attraction_df = pd.DataFrame(attraction_dict)\n",
    "    \n",
    "    return attraction_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_img(attraction_page_driver: webdriver) -> list[str]:\n",
    "    \n",
    "    res_img = []\n",
    "\n",
    "    possible_click_img_xpath = [\n",
    "        '//*[@id=\"AR_ABOUT\"]/div[2]/div/div/div/div/div[1]/div/div/div/div[1]/div/div[7]/button',\n",
    "        '//*[@id=\"AR_ABOUT\"]/div/div/div/div/div/div[1]/div/div/div/div[1]/div/div[7]/button'\n",
    "    ]\n",
    "\n",
    "    # find button to click popup image section\n",
    "    # and save url to same site with image popup section -> use with new webdriver to find images with auto-retry\n",
    "    link_to_img_section = \"\"\n",
    "    for cur_xpath in possible_click_img_xpath:\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, cur_xpath)))\n",
    "            click_img_btn = attraction_page_driver.find_element(By.XPATH, cur_xpath)\n",
    "            click_img_btn.click()\n",
    "\n",
    "            # wait for page to load (change url from 'current url' to 'website with all reviewed images')\n",
    "            WebDriverWait(attraction_page_driver, 2).until(EC.url_changes(attraction_page_driver.current_url))\n",
    "            link_to_img_section = attraction_page_driver.current_url\n",
    "            break\n",
    "        except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    if(not len(link_to_img_section)):\n",
    "        return ['']\n",
    "\n",
    "    # continue scrape image with new driver with url 'link_to_img_section'   \n",
    "    while(True):\n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape single attraction ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "\n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        img_driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        img_driver.quit()\n",
    "        break\n",
    "    \n",
    "    return res_img.copy()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_location(attraction_page_driver: webdriver, attraction: Attraction, province_th: str):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_attraction(link_to_attraction: str, province_th: str) -> Attraction:\n",
    "    \n",
    "    attraction = Attraction()\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape single attraction ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "\n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        attraction_page_driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            print(\"scrape single attraction...\")\n",
    "            print(\"for attraction : \", link_to_attraction)\n",
    "            attraction_page_driver.get(link_to_attraction)\n",
    "\n",
    "            print(\"debug scrape_single_attraction: common component section\")\n",
    "            WebDriverWait(attraction_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'IDaDx')))\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            cnt_retry += 1\n",
    "            attraction_page_driver.quit()\n",
    "            print(\"retry single attraction...\")\n",
    "            continue\n",
    "\n",
    "        # find name\n",
    "        name = \"\"\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[1]/header/div[3]/div[1]/div/h1')))\n",
    "            name_element = attraction_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[1]/header/div[3]/div[1]/div/h1')\n",
    "            name = name_element.text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find name\")\n",
    "\n",
    "        print(\"name -> \", name)\n",
    "\n",
    "        # find description\n",
    "        description = \"\"\n",
    "        try:\n",
    "           pass\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        print(\"description -> \", description)\n",
    "        \n",
    "        # find rating\n",
    "        rating = 0\n",
    "        ratingCount = 0\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[2]/div[2]/div/div[1]/section[1]/div/div/div/div/div[1]/div[1]/a/div')))\n",
    "            score_element = attraction_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[2]/div[2]/div/div[1]/section[1]/div/div/div/div/div[1]/div[1]/a/div')\n",
    "            score_text_list = score_element.get_attribute('aria-label').split(' ')\n",
    "            for Idx in range(1, len(score_text_list)):\n",
    "                # set rating\n",
    "                if(score_text_list[Idx - 1] == \"คะแนน\"):\n",
    "                    rating = float(score_text_list[Idx])\n",
    "\n",
    "                elif(score_text_list[Idx - 1] == \"รีวิว\"):\n",
    "                    ratingCount = float(score_text_list[Idx].replace(',', ''))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find rating and ratingCount\")\n",
    "\n",
    "        print(\"rating --> \", rating)\n",
    "        print(\"ratingCount --> \", ratingCount)\n",
    "        \n",
    "\n",
    "        # find opening hours\n",
    "        \n",
    "        # scrape img_path\n",
    "        img_path = scrape_img(attraction_page_driver)\n",
    "        print(\"cur img path -> \", img_path)\n",
    "\n",
    "        attraction_page_driver.quit()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_url_by_page(query_url: str) -> list[str]:\n",
    "\n",
    "    res_url_by_page = []\n",
    "\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape data by page ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        # os.environ['proxy_port']\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "      \n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # just check for ip\n",
    "        # print(\"just check for ip :\")\n",
    "        # driver.get(\"https://httpbin.io/ip\")\n",
    "        # print(driver.page_source)\n",
    "\n",
    "        # find group of restaurant on the nth page\n",
    "        all_attractions_card = []\n",
    "\n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            driver.get(query_url)\n",
    "            # scroll and wait for some msec\n",
    "            driver.execute_script('window.scrollBy(0, document.body.scrollHeight)')\n",
    "\n",
    "            print(\"check current page url --> \", driver.current_url)\n",
    "\n",
    "            # wait for div (each attraction section) to be present and visible\n",
    "            print(\"debug get_all_url_by_page: attraction by one page section\")\n",
    "            WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'XJlaI')))\n",
    "            all_attractions_card = driver.find_elements(By.CLASS_NAME, 'XJlaI')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"retry find all_restaurants_card ...\")\n",
    "            cnt_retry += 1\n",
    "            driver.quit()\n",
    "            continue\n",
    "\n",
    "        for cur_attraction_card in all_attractions_card:\n",
    "            WebDriverWait(driver, 1).until(EC.presence_of_element_located((By.TAG_NAME, 'a')))\n",
    "            cur_attraction_url = cur_attraction_card.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "            \n",
    "            WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'BKifx')))\n",
    "            check_text = cur_attraction_card.find_element(By.CLASS_NAME, 'BKifx').text\n",
    "            \n",
    "            # check if cuurent card is for attraction ?\n",
    "            is_attraction = True\n",
    "            not_attraction_keyword = ['ทัวร์', \"สปา\", \"กิจกรรมทางวัฒนธรรม\", 'ชั้นเรียน', 'รถรับส่ง', 'อุปกรณ์ให้เช่า', 'ร้านขายของ']\n",
    "            for cur_check_word in not_attraction_keyword:\n",
    "                if(cur_check_word in check_text):\n",
    "                    is_attraction = False\n",
    "                    break\n",
    "            \n",
    "            if(not is_attraction):\n",
    "                # print(\"not prn : \", cur_attraction_url)\n",
    "                continue\n",
    "\n",
    "            print(\"prn : \", cur_attraction_url)\n",
    "            res_url_by_page.append(cur_attraction_url)\n",
    "\n",
    "        driver.quit()\n",
    "        break\n",
    "\n",
    "    return res_url_by_page.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_attraction_by_province(province_url: str, province: str) -> pd.DataFrame:\n",
    "    # res_attraction_df = pd.DataFrame()\n",
    "    res_attraction_df = create_attraction_df(Attraction())\n",
    "    \n",
    "    cnt_for_debug = 0\n",
    "\n",
    "    while(True):\n",
    "        if(cnt_for_debug == 1):\n",
    "            break\n",
    "        cnt_for_debug += 1\n",
    "        \n",
    "        print(\"scraping attraction | province --> %s | page --> %s\" % (province, cnt_for_debug))\n",
    "\n",
    "        try:\n",
    "            # get url of to all attraction in current page\n",
    "            all_url_by_page = get_all_url_by_page(query_url=province_url)\n",
    "        \n",
    "            # use data from 'res_get_data_by_page' to retrive data of specific attraction\n",
    "            for cur_attraction_url in all_url_by_page:\n",
    "                print(\"g\")\n",
    "                # continue scraping data for a specific resgtaurant\n",
    "                cur_attraction = scrape_single_attraction(\n",
    "                    link_to_attraction = cur_attraction_url,\n",
    "                    province_th = province\n",
    "                )\n",
    "                \n",
    "                # create data frame represent data scrape from current attraction card\n",
    "                cur_attraction_df = create_attraction_df(attraction=cur_attraction)\n",
    "\n",
    "                # concat all data frame result\n",
    "                res_attraction_df = pd.concat([res_attraction_df, cur_attraction_df])\n",
    "        \n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return res_attraction_df.iloc[1:, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory res_attraction_scraping created successfully\n",
      "scraping attraction | province --> สุรินทร์ | page --> 1\n",
      "check current page url -->  https://th.tripadvisor.com/Attractions-g2099297-Activities-c47-Surin_Province.html\n",
      "debug get_all_url_by_page: attraction by one page section\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g303923-d4322926-Reviews-Surin_National_Museum-Surin_Surin_Province.html\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g303923-d4322931-Reviews-Ban_Tha_Sawang_Silk_Weaving_Village-Surin_Surin_Province.html\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g303923-d6379888-Reviews-Phanom_Sawai_Forest_Park-Surin_Surin_Province.html\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g303923-d4322930-Reviews-City_Pillar_Shrine-Surin_Surin_Province.html\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g303923-d4322932-Reviews-Wat_Burapharam-Surin_Surin_Province.html\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g2237594-d4322582-Reviews-Prasat_Si_Khoraphum-Sikhoraphum_Surin_Province.html\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g2237588-d6420101-Reviews-Prasat_Ban_Pluang-Prasat_Surin_Province.html\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g303923-d4322541-Reviews-Prasat_Mueang_Thi-Surin_Surin_Province.html\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g303923-d10202649-Reviews-Phraya_Surin_Pakdee_Srinarong_Jangwang_Pum_Monument-Surin_Surin_Province.html\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g2237591-d3814567-Reviews-Prasat_Phum_Pon-Sangkha_Surin_Province.html\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g303923-d6954129-Reviews-Wat_Salaloy-Surin_Surin_Province.html\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g2237580-d12855614-Reviews-Wat_Khaosala-Buachet_Surin_Province.html\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g2237583-d13817044-Reviews-Wat_Chang_Mop-Kap_Choeng_Surin_Province.html\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g2237588-d4322540-Reviews-Prasat_Ban_Phlai-Prasat_Surin_Province.html\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g2237591-d4322928-Reviews-Prasat_Yai_Ngao-Sangkha_Surin_Province.html\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g303923-d6669573-Reviews-Ban_Buthom_Basketry_Village-Surin_Surin_Province.html\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g303923-d6954116-Reviews-Wat_Nongbua-Surin_Surin_Province.html\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g2237584-d6669387-Reviews-Khwao_Sinarin_Handicraft_Village-Khwao_Sinarin_Surin_Province.html\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g303923-d14794412-Reviews-Wat_Thepsurin-Surin_Surin_Province.html\n",
      "prn :  https://th.tripadvisor.com/Attraction_Review-g2237587-d15592833-Reviews-Prasat_Ta_Muen-Phanom_Dong_Rak_Surin_Province.html\n",
      "g\n",
      "scrape single attraction...\n",
      "for attraction :  https://th.tripadvisor.com/Attraction_Review-g303923-d4322926-Reviews-Surin_National_Museum-Surin_Surin_Province.html\n",
      "debug scrape_single_attraction: common component section\n",
      "can't find name\n",
      "name ->  \n",
      "description ->  \n",
      "rating -->  4.5\n",
      "ratingCount -->  37.0\n"
     ]
    }
   ],
   "source": [
    "# create directory 'res_restaurant_scraping'\n",
    "createDirectory(fh.STORE_ATTRACTION_SCRAPING, 'res_attraction_scraping')\n",
    "\n",
    "# *** select one province from 'ALL_PROVINCE_TRIPADVISOR_DATA'\n",
    "# *** so, change \"Idx_of_region\" everytime when scrape another province\n",
    "Idx_of_region = 0\n",
    "cur_region_data = ALL_PROVINCE_TRIPADVISOR_DATA[Idx_of_region]\n",
    "\n",
    "cur_province_en = cur_region_data[0]\n",
    "cur_province_th = cur_region_data[1]\n",
    "cur_province_url = cur_region_data[2]\n",
    "\n",
    "# get dataframe result of all attraction in current province\n",
    "cur_res_allAttractions_df = scrape_attraction_by_province(\n",
    "    province_url = cur_province_url,\n",
    "    province = cur_province_th\n",
    ")\n",
    "# remove duplicate restaurant \n",
    "cur_res_allAttractions_df.drop_duplicates(subset=['name'], inplace=True)\n",
    "# set new index\n",
    "cur_res_allAttractions_df.set_index(['name'], inplace=True)\n",
    "\n",
    "# save result dataframe to .csv\n",
    "res_file_name = 'res_attraction_%s.csv' % (cur_province_en)\n",
    "res_path = os.path.join(fh.STORE_ATTRACTION_SCRAPING, 'res_attraction_scraping', res_file_name) \n",
    "cur_res_allAttractions_df.to_csv(res_path, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
