{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pyautogui\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import constants.constants as const\n",
    "import constants.file_handler_constants as fh\n",
    "from constants.attraction_constants import *\n",
    "\n",
    "from packages.attraction.Attraction import *\n",
    "from packages.file_handler_package.file_handler import *\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv, dotenv_values \n",
    "\n",
    "from selenium.webdriver import Remote, ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.actions.wheel_input import ScrollOrigin\n",
    "from selenium.webdriver import ActionChains\n",
    "\n",
    "from seleniumwire import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from selenium.webdriver.edge.options import Options\n",
    "\n",
    "\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attraction_df(attraction: Attraction) -> pd.DataFrame:\n",
    "    attraction_dict = {\n",
    "        'name' : [attraction.get_name()],\n",
    "        'description' : [attraction.get_description()],\n",
    "        'latitude' : [attraction.get_latitude()],\n",
    "        'longitude' : [attraction.get_longitude()],\n",
    "        'imgPath' : [attraction.get_imgPath()],\n",
    "        'phone': [attraction.get_phone()],\n",
    "        'website': [attraction.get_website()],\n",
    "        'openingHour': [attraction.get_openingHour()],\n",
    "\n",
    "        # location\n",
    "        'address' : [attraction.get_location().get_address()],\n",
    "        'province' : [attraction.get_location().get_province()],\n",
    "        'district' : [attraction.get_location().get_district()],\n",
    "        'subDistrict' : [attraction.get_location().get_subDistrict()],\n",
    "        'province_code' : [attraction.get_location().get_province_code()],\n",
    "        'district_code' : [attraction.get_location().get_district_code()],\n",
    "        'sub_district_code' : [attraction.get_location().get_sub_district_code()],\n",
    "\n",
    "        # rating\n",
    "        'score' : [attraction.get_rating().get_score()],\n",
    "        'ratingCount' : [attraction.get_rating().get_ratingCount()],\n",
    "    }\n",
    "\n",
    "    attraction_df = pd.DataFrame(attraction_dict)\n",
    "    \n",
    "    return attraction_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_img(attraction_page_driver: webdriver) -> list[str]:\n",
    "    \n",
    "    res_imgPath = []\n",
    "\n",
    "    possible_click_img_xpath = [\n",
    "        '//*[@id=\"AR_ABOUT\"]/div[2]/div/div/div/div/div[1]/div/div/div/div[1]/div/div[7]/button',\n",
    "        '//*[@id=\"AR_ABOUT\"]/div/div/div/div/div/div[1]/div/div/div/div[1]/div/div[7]/button'\n",
    "    ]\n",
    "\n",
    "    btn_img_xpath = \"\"\n",
    "    for cur_xpath in possible_click_img_xpath:\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, cur_xpath)))\n",
    "            btn_img_xpath = cur_xpath\n",
    "            break\n",
    "        \n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    if(not len(btn_img_xpath)):\n",
    "        print(\"can't scrape img (no img ?)\")\n",
    "        return ['']\n",
    "\n",
    "    # find button and click\n",
    "    # to see modal then scrape image address\n",
    "    try:\n",
    "        WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, cur_xpath)))\n",
    "        click_img_btn = attraction_page_driver.find_element(By.XPATH, cur_xpath)\n",
    "        click_img_btn.click()\n",
    "        is_end_scrape_img = False\n",
    "        while(not is_end_scrape_img):\n",
    "            try:\n",
    "                WebDriverWait(attraction_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'cfCAA')))\n",
    "                all_img_elements = attraction_page_driver.find_elements(By.CLASS_NAME, 'cfCAA')\n",
    "                print(\"find image element -> \", len(all_img_elements))\n",
    "                for cur_img_element in all_img_elements:\n",
    "                    cur_bgImg_val = cur_img_element.value_of_css_property('background-image')\n",
    "                    match = re.search(r'url\\(\"(.*?)\"\\)', cur_bgImg_val)\n",
    "                    if match:\n",
    "                        res_imgPath.append(match.group(1))\n",
    "\n",
    "                is_end_scrape_img = True\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"retry scrape img...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "            pass\n",
    "    \n",
    "\n",
    "    return res_imgPath.copy()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_location_helper() -> Location:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape lat/long, location data and openingHours (there are in another page of current attraction)\n",
    "def scrape_location_latlong_openingHours(attraction_page_driver: webdriver, province_th: str) -> tuple[float, float, dict, Location]:\n",
    "    \n",
    "    lat = 0\n",
    "    long = 0\n",
    "    openingHours = {}\n",
    "    location = Location()\n",
    "\n",
    "    # find link to get adjust page of current attraction\n",
    "    # for example: \"https://th.tripadvisor.com/ImproveListing-d8820434.html\"\n",
    "    possible_container_xpath = [\n",
    "        '//*[@id=\"AR_ABOUT\"]/div[1]',\n",
    "        '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[2]/div[2]/div/div[1]/section[2]/div/div/div[2]/div/div/div'\n",
    "    ]\n",
    "\n",
    "    link_to_adjust_page = \"\"\n",
    "    for cur_xpath in possible_container_xpath:\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, cur_xpath)))\n",
    "\n",
    "            adjust_page_container = attraction_page_driver.find_elements(By.XPATH, cur_xpath)\n",
    "            adjust_page_link_element = adjust_page_container.find_element(By.TAG_NAME, 'a')\n",
    "            link_to_adjust_page = adjust_page_link_element.get_attribute('href')\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    if(not len(link_to_adjust_page)):\n",
    "        print(\"no link to get adjust attraction page\")\n",
    "        return lat, long, openingHours.copy(), location\n",
    "    \n",
    "\n",
    "    # create new webdriver to continue scrape lat/long, openingHours and location in adjust attraction page\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape single attraction ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "\n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        adjust_page_driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            print(\"scrape data in adjust attraction page...\")\n",
    "            print(\"for link : \", link_to_adjust_page)\n",
    "            adjust_page_driver.get(link_to_adjust_page)\n",
    "\n",
    "            print(\"debug option of adjust page: \")\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'DiHOR')))\n",
    "\n",
    "        except Exception as e:\n",
    "            cnt_retry += 1\n",
    "            adjust_page_driver.quit()\n",
    "            print(\"retry adjust page...\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # find dropdown --> click display data below --> cick display lat/long input form\n",
    "        possible_target_btn = adjust_page_driver.find_elements(By.CLASS_NAME, 'DiHOR')\n",
    "        for cur_dropdown_btn in possible_target_btn:\n",
    "            cur_dropdown_text = cur_dropdown_btn.text\n",
    "            if(\"แนะนำการแก้ไขข้อมูลของสถานที่นี้\" in cur_dropdown_text):\n",
    "                print(\"found target dropdown btn ...\")\n",
    "                cur_dropdown_btn.click()\n",
    "                WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/button')))\n",
    "                # find button click to display lat/long input form\n",
    "                display_lat_long_btn = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/button')\n",
    "                display_lat_long_btn.click()\n",
    "            \n",
    "        # find lat/long\n",
    "        try:\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[1]/div/div[2]/div/div/div/span')))\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[2]/div/div[2]/div/div/div/span')))\n",
    "    \n",
    "            lat_input_container = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[1]/div/div[2]/div/div/div/span')\n",
    "            lat_input_element = lat_input_container.find_element(By.TAG_NAME, 'input')\n",
    "            lat = float(lat_input_element.get_attribute('value'))\n",
    "\n",
    "            long_input_container = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[2]/div/div[2]/div/div/div/span')\n",
    "            long_input_element = long_input_container.find_element(By.TAG_NAME, 'input')\n",
    "            long = float(long_input_element.get_attribute('value'))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find lat/long\")\n",
    "        \n",
    "        print(\"lat : \", lat)\n",
    "        print(\"long : \", long)\n",
    "\n",
    "        # **if can't find lat/long --> don't scrape this attaction\n",
    "        if(lat == 0 and long == 0):\n",
    "            print(\"in scrape_location_latlong_openingHours --> can't find lat/long --> 0, 0\")\n",
    "            return lat, long, openingHours.copy(), location\n",
    "\n",
    "        # find openingHours\n",
    "\n",
    "        # find location\n",
    "\n",
    "        adjust_page_driver.quit()\n",
    "        break\n",
    "\n",
    "    return lat, long, openingHours.copy(), location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_attraction(link_to_attraction: str, province_th: str) -> Attraction:\n",
    "    \n",
    "    attraction = Attraction()\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape single attraction ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "\n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        attraction_page_driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            print(\"scrape single attraction...\")\n",
    "            print(\"for attraction : \", link_to_attraction)\n",
    "            attraction_page_driver.get(link_to_attraction)\n",
    "\n",
    "            print(\"debug scrape_single_attraction: common component section\")\n",
    "            WebDriverWait(attraction_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'IDaDx')))\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            cnt_retry += 1\n",
    "            attraction_page_driver.quit()\n",
    "            print(\"retry single attraction...\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # ** find lat/long, location data and openingHours (there are in another page of current attraction)\n",
    "        # ** if this attraction not have lat/long\n",
    "        # ** don't continue to scrape\n",
    "        lat, long, openingHours, location = scrape_location_latlong_openingHours(\n",
    "            attraction_page_driver = attraction_page_driver,\n",
    "            province_th = province_th\n",
    "        )\n",
    "\n",
    "        # **if can't find lat/long --> don't scrape this attaction\n",
    "        if(lat == 0 and long == 0):\n",
    "            print(\"in scrape_single_attraction --> can't find lat/long --> don't scrape this attraction ...\")\n",
    "            attraction_page_driver.quit()\n",
    "            return attraction\n",
    "\n",
    "        # find name\n",
    "        name = \"\"\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[1]/header/div[3]/div[1]/div/h1')))\n",
    "            name_element = attraction_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[1]/header/div[3]/div[1]/div/h1')\n",
    "            name = name_element.text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find name\")\n",
    "\n",
    "        print(\"name -> \", name)\n",
    "\n",
    "        # find description\n",
    "        description = \"\"\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"AR_ABOUT\"]/div[1]')))\n",
    "            \n",
    "            description_container = attraction_page_driver.find_element(By.XPATH, '//*[@id=\"AR_ABOUT\"]/div[1]')\n",
    "            header_element = description_container.find_element(By.CLASS_NAME, 'biGQs')\n",
    "            header_text = header_element.text\n",
    "            if(header_text == 'ข้อมูล'):\n",
    "                description_element = attraction_page_driver.find_element(By.CLASS_NAME, 'JguWG')\n",
    "                description = description_element.text\n",
    "                \n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find description\")\n",
    "\n",
    "        print(\"description -> \", description)\n",
    "        \n",
    "        # find rating\n",
    "        rating = 0\n",
    "        ratingCount = 0\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[2]/div[2]/div/div[1]/section[1]/div/div/div/div/div[1]/div[1]/a/div')))\n",
    "            score_element = attraction_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[2]/div[2]/div/div[1]/section[1]/div/div/div/div/div[1]/div[1]/a/div')\n",
    "            score_text_list = score_element.get_attribute('aria-label').split(' ')\n",
    "            for Idx in range(1, len(score_text_list)):\n",
    "                # set rating\n",
    "                if(score_text_list[Idx - 1] == \"คะแนน\"):\n",
    "                    rating = float(score_text_list[Idx])\n",
    "\n",
    "                elif(score_text_list[Idx - 1] == \"รีวิว\"):\n",
    "                    ratingCount = float(score_text_list[Idx].replace(',', ''))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find rating and ratingCount\")\n",
    "\n",
    "        print(\"rating --> \", rating)\n",
    "        print(\"ratingCount --> \", ratingCount)\n",
    "        \n",
    "\n",
    "        # find img_path\n",
    "        img_path = scrape_img(attraction_page_driver)\n",
    "        print(\"cur img path -> \", img_path)\n",
    "\n",
    "\n",
    "        # set some of \"Attraction\" object properties\n",
    "\n",
    "\n",
    "        attraction_page_driver.quit()\n",
    "        break\n",
    "\n",
    "    return attraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_url_by_page(query_url: str) -> list[str]:\n",
    "\n",
    "    res_url_by_page = []\n",
    "\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape data by page ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        # os.environ['proxy_port']\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "      \n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # just check for ip\n",
    "        # print(\"just check for ip :\")\n",
    "        # driver.get(\"https://httpbin.io/ip\")\n",
    "        # print(driver.page_source)\n",
    "\n",
    "        # find group of restaurant on the nth page\n",
    "        all_attractions_card = []\n",
    "\n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            driver.get(query_url)\n",
    "            # scroll and wait for some msec\n",
    "            driver.execute_script('window.scrollBy(0, document.body.scrollHeight)')\n",
    "\n",
    "            print(\"check current page url --> \", driver.current_url)\n",
    "\n",
    "            # wait for div (each attraction section) to be present and visible\n",
    "            print(\"debug get_all_url_by_page: attraction by one page section\")\n",
    "            WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'XJlaI')))\n",
    "            all_attractions_card = driver.find_elements(By.CLASS_NAME, 'XJlaI')\n",
    "            print(\"b1\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"retry find all_restaurants_card ...\")\n",
    "            cnt_retry += 1\n",
    "            driver.quit()\n",
    "            continue\n",
    "\n",
    "        print(\"b2\")\n",
    "        for cur_attraction_card in all_attractions_card:\n",
    "\n",
    "            cur_attraction_url = cur_attraction_card.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "            check_text = cur_attraction_card.find_element(By.CLASS_NAME, 'BKifx').text\n",
    "\n",
    "            # check if cuurent card is for attraction ?\n",
    "            is_attraction = True\n",
    "            not_attraction_keyword = ['ทัวร์', \"สปา\", \"กิจกรรมทางวัฒนธรรม\", 'ชั้นเรียน', 'รถรับส่ง', 'อุปกรณ์ให้เช่า', 'ร้านขายของ']\n",
    "            for cur_check_word in not_attraction_keyword:\n",
    "                if(cur_check_word in check_text):\n",
    "                    is_attraction = False\n",
    "                    break\n",
    "            \n",
    "            if(not is_attraction):\n",
    "                # print(\"not prn : \", cur_attraction_url)\n",
    "                continue\n",
    "\n",
    "            print(\"cur_attraction_url : \", cur_attraction_url)\n",
    "            res_url_by_page.append(cur_attraction_url)\n",
    "\n",
    "        driver.quit()\n",
    "        break\n",
    "\n",
    "    return res_url_by_page.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_attraction_by_province(province_url: str, province: str) -> pd.DataFrame:\n",
    "    # res_attraction_df = pd.DataFrame()\n",
    "    res_attraction_df = create_attraction_df(Attraction())\n",
    "    \n",
    "    cnt_for_debug = 0\n",
    "\n",
    "    while(True):\n",
    "        if(cnt_for_debug == 1):\n",
    "            break\n",
    "        cnt_for_debug += 1\n",
    "        \n",
    "        print(\"scraping attraction | province --> %s | page --> %s\" % (province, cnt_for_debug))\n",
    "\n",
    "        try:\n",
    "            # get url of to all attraction in current page\n",
    "            all_url_by_page = get_all_url_by_page(query_url=province_url)\n",
    "        \n",
    "            # use data from 'res_get_data_by_page' to retrive data of specific attraction\n",
    "            for cur_attraction_url in all_url_by_page:\n",
    "                \n",
    "                # continue scraping data for a specific resgtaurant\n",
    "                # cur_attraction = scrape_single_attraction(\n",
    "                #     link_to_attraction = cur_attraction_url,\n",
    "                #     province_th = province\n",
    "                # )\n",
    "                \n",
    "                cur_attraction = scrape_single_attraction(\n",
    "                    link_to_attraction = \"https://th.tripadvisor.com/Attraction_Review-g297930-d3387563-Reviews-Jungceylon-Patong_Kathu_Phuket.html\",\n",
    "                    province_th = province\n",
    "                )\n",
    "\n",
    "                # create data frame represent data scrape from current attraction card\n",
    "                cur_attraction_df = create_attraction_df(attraction=cur_attraction)\n",
    "\n",
    "                # concat all data frame result\n",
    "                res_attraction_df = pd.concat([res_attraction_df, cur_attraction_df])\n",
    "        \n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return res_attraction_df.iloc[1:, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory res_attraction_scraping created successfully\n",
      "scraping attraction | province --> สุรินทร์ | page --> 1\n",
      "check current page url -->  https://th.tripadvisor.com/Attractions-g2099297-Activities-c47-Surin_Province.html\n",
      "debug get_all_url_by_page: attraction by one page section\n",
      "b1\n",
      "b2\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d4322926-Reviews-Surin_National_Museum-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d4322931-Reviews-Ban_Tha_Sawang_Silk_Weaving_Village-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d4322930-Reviews-City_Pillar_Shrine-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d4322932-Reviews-Wat_Burapharam-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d6379888-Reviews-Phanom_Sawai_Forest_Park-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2237594-d4322582-Reviews-Prasat_Si_Khoraphum-Sikhoraphum_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2237588-d6420101-Reviews-Prasat_Ban_Pluang-Prasat_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d4322541-Reviews-Prasat_Mueang_Thi-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d10202649-Reviews-Phraya_Surin_Pakdee_Srinarong_Jangwang_Pum_Monument-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2237591-d3814567-Reviews-Prasat_Phum_Pon-Sangkha_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2237580-d12855614-Reviews-Wat_Khaosala-Buachet_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d6954129-Reviews-Wat_Salaloy-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2237591-d4322928-Reviews-Prasat_Yai_Ngao-Sangkha_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2237584-d6669387-Reviews-Khwao_Sinarin_Handicraft_Village-Khwao_Sinarin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d6669573-Reviews-Ban_Buthom_Basketry_Village-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2237588-d4322540-Reviews-Prasat_Ban_Phlai-Prasat_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2237583-d13817044-Reviews-Wat_Chang_Mop-Kap_Choeng_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d6954116-Reviews-Wat_Nongbua-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d14794412-Reviews-Wat_Thepsurin-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2237587-d15592833-Reviews-Prasat_Ta_Muen-Phanom_Dong_Rak_Surin_Province.html\n",
      "scrape single attraction...\n",
      "for attraction :  https://th.tripadvisor.com/Attraction_Review-g297930-d3387563-Reviews-Jungceylon-Patong_Kathu_Phuket.html\n",
      "debug scrape_single_attraction: common component section\n",
      "name ->  จังซีลอน\n",
      "description ->  เปิดประสบการณ์ช้อปปิ้งที่แตกต่าง ตอบสนองทุกไลฟ์สไตล์ด้วยสินค้าแบรนด์ดังกว่า 300 ร้านค้า อิ่มอร่อยกับอาหารหลากรสนานาชาติ สัมผัสกับสุดยอดความบันเทิงครบครัน เพื่อเติมเต็มความสุขในวันพักผ่อนให้คุณและครอบครัว เดินทางสะดวกเพียง 5 นาทีจากหาดป่าตอง\n",
      "rating -->  4.0\n",
      "ratingCount -->  7237.0\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "retry scrape img...\n",
      "find image element ->  15\n",
      "cur img path ->  ['https://media-cdn.tripadvisor.com/media/photo-s/2b/31/a1/6d/the-bay-zone.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2b/85/85/bc/the-garden-is-the-hangout.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2b/85/85/ad/the-garden-zone.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2b/85/85/9a/relax-and-chill-at-the.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2b/85/85/97/love-eat-the-jungle-zone.jpg', 'https://media-cdn.tripadvisor.com/media/photo-p/2b/31/ad/2a/shopping-leisure.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2b/31/ad/1e/a-modern-and-spacious.jpg', 'https://media-cdn.tripadvisor.com/media/photo-p/2b/31/ad/0f/satisfy-with-food-and.jpg', 'https://media-cdn.tripadvisor.com/media/photo-p/12/f3/9d/20/fuel-a-full-day-of-shopping.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/12/f3/9d/19/discover-even-more-choices.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/12/f3/9d/0c/experience-new-gastronomic.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/12/f3/9d/05/indulge-in-a-feast-for.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/70/cb/dd/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-o/2d/70/cb/dc/caption.jpg', 'https://media-cdn.tripadvisor.com/media/photo-s/2d/70/cb/db/caption.jpg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Service.__del__ at 0x00000240F6E03EC0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 189, in __del__\n",
      "    self.stop()\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 146, in stop\n",
      "    self.send_remote_shutdown_command()\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 126, in send_remote_shutdown_command\n",
      "    request.urlopen(f\"{self.service_url}/shutdown\")\n",
      "  File \"c:\\Python312\\Lib\\urllib\\request.py\", line 215, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\urllib\\request.py\", line 515, in open\n",
      "    response = self._open(req, data)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\urllib\\request.py\", line 532, in _open\n",
      "    result = self._call_chain(self.handle_open, protocol, protocol +\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\urllib\\request.py\", line 492, in _call_chain\n",
      "    result = func(*args)\n",
      "             ^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\urllib\\request.py\", line 1373, in http_open\n",
      "    return self.do_open(http.client.HTTPConnection, req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\urllib\\request.py\", line 1344, in do_open\n",
      "    h.request(req.get_method(), req.selector, req.data, headers,\n",
      "  File \"c:\\Python312\\Lib\\http\\client.py\", line 1331, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"c:\\Python312\\Lib\\http\\client.py\", line 1377, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"c:\\Python312\\Lib\\http\\client.py\", line 1326, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"c:\\Python312\\Lib\\http\\client.py\", line 1085, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"c:\\Python312\\Lib\\http\\client.py\", line 1029, in send\n",
      "    self.connect()\n",
      "  File \"c:\\Python312\\Lib\\http\\client.py\", line 995, in connect\n",
      "    self.sock = self._create_connection(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\socket.py\", line 844, in create_connection\n",
      "    exceptions.clear()  # raise only the last error\n",
      "    ^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# create directory 'res_restaurant_scraping'\n",
    "createDirectory(fh.STORE_ATTRACTION_SCRAPING, 'res_attraction_scraping')\n",
    "\n",
    "# *** select one province from 'ALL_PROVINCE_TRIPADVISOR_DATA'\n",
    "# *** so, change \"Idx_of_region\" everytime when scrape another province\n",
    "Idx_of_region = 0\n",
    "cur_region_data = ALL_PROVINCE_TRIPADVISOR_DATA[Idx_of_region]\n",
    "\n",
    "cur_province_en = cur_region_data[0]\n",
    "cur_province_th = cur_region_data[1]\n",
    "cur_province_url = cur_region_data[2]\n",
    "\n",
    "# get dataframe result of all attraction in current province\n",
    "cur_res_allAttractions_df = scrape_attraction_by_province(\n",
    "    province_url = cur_province_url,\n",
    "    province = cur_province_th\n",
    ")\n",
    "\n",
    "# don't forget to remove row with lat/long be zero\n",
    "\n",
    "# remove duplicate restaurant \n",
    "cur_res_allAttractions_df.drop_duplicates(subset=['name'], inplace=True)\n",
    "# set new index\n",
    "cur_res_allAttractions_df.set_index(['name'], inplace=True)\n",
    "\n",
    "# save result dataframe to .csv\n",
    "res_file_name = 'res_attraction_%s.csv' % (cur_province_en)\n",
    "res_path = os.path.join(fh.STORE_ATTRACTION_SCRAPING, 'res_attraction_scraping', res_file_name) \n",
    "cur_res_allAttractions_df.to_csv(res_path, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
