{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pyautogui\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import constants.constants as const\n",
    "import constants.file_handler_constants as fh\n",
    "from constants.attraction_constants import *\n",
    "\n",
    "from packages.attraction.Attraction import *\n",
    "from packages.file_handler_package.file_handler import *\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv, dotenv_values \n",
    "\n",
    "from selenium.webdriver import Remote, ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.actions.wheel_input import ScrollOrigin\n",
    "from selenium.webdriver import ActionChains\n",
    "\n",
    "from seleniumwire import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from selenium.webdriver.edge.options import Options\n",
    "\n",
    "\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attraction_df(attraction: Attraction) -> pd.DataFrame:\n",
    "    attraction_dict = {\n",
    "        'name' : [attraction.get_name()],\n",
    "        'description' : [attraction.get_description()],\n",
    "        'latitude' : [attraction.get_latitude()],\n",
    "        'longitude' : [attraction.get_longitude()],\n",
    "        'imgPath' : [attraction.get_imgPath()],\n",
    "        'phone': [attraction.get_phone()],\n",
    "        'website': [attraction.get_website()],\n",
    "        'openingHour': [attraction.get_openingHour()],\n",
    "\n",
    "        # location\n",
    "        'address' : [attraction.get_location().get_address()],\n",
    "        'province' : [attraction.get_location().get_province()],\n",
    "        'district' : [attraction.get_location().get_district()],\n",
    "        'subDistrict' : [attraction.get_location().get_sub_district()],\n",
    "        'province_code' : [attraction.get_location().get_province_code()],\n",
    "        'district_code' : [attraction.get_location().get_district_code()],\n",
    "        'sub_district_code' : [attraction.get_location().get_sub_district_code()],\n",
    "\n",
    "        # rating\n",
    "        'score' : [attraction.get_rating().get_score()],\n",
    "        'ratingCount' : [attraction.get_rating().get_ratingCount()],\n",
    "    }\n",
    "\n",
    "    attraction_df = pd.DataFrame(attraction_dict)\n",
    "    \n",
    "    return attraction_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_img(attraction_page_driver: webdriver) -> list[str]:\n",
    "    \n",
    "    res_imgPath = []\n",
    "\n",
    "    possible_click_img_xpath = [\n",
    "        '//*[@id=\"AR_ABOUT\"]/div[2]/div/div/div/div/div[1]/div/div/div/div[1]/div/div[7]/button',\n",
    "        '//*[@id=\"AR_ABOUT\"]/div/div/div/div/div/div[1]/div/div/div/div[1]/div/div[7]/button'\n",
    "    ]\n",
    "\n",
    "    btn_img_xpath = \"\"\n",
    "    for cur_xpath in possible_click_img_xpath:\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, cur_xpath)))\n",
    "            btn_img_xpath = cur_xpath\n",
    "            break\n",
    "        \n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    if(not len(btn_img_xpath)):\n",
    "        print(\"can't scrape img (no img ?)\")\n",
    "        return ['']\n",
    "\n",
    "    # find button and click\n",
    "    # to see modal then scrape image address\n",
    "    try:\n",
    "        WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, cur_xpath)))\n",
    "        click_img_btn = attraction_page_driver.find_element(By.XPATH, cur_xpath)\n",
    "        click_img_btn.click()\n",
    "        is_end_scrape_img = False\n",
    "        while(not is_end_scrape_img):\n",
    "            try:\n",
    "                WebDriverWait(attraction_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'cfCAA')))\n",
    "                all_img_elements = attraction_page_driver.find_elements(By.CLASS_NAME, 'cfCAA')\n",
    "                print(\"find image element -> \", len(all_img_elements))\n",
    "                for cur_img_element in all_img_elements:\n",
    "                    cur_bgImg_val = cur_img_element.value_of_css_property('background-image')\n",
    "                    match = re.search(r'url\\(\"(.*?)\"\\)', cur_bgImg_val)\n",
    "                    if match:\n",
    "                        res_imgPath.append(match.group(1))\n",
    "\n",
    "                is_end_scrape_img = True\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"retry scrape img...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "            pass\n",
    "    \n",
    "\n",
    "    return res_imgPath.copy()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_location(attraction_page_driver: webdriver, latitude: float, longitude: float, province_th: str) -> Location:\n",
    "\n",
    "    # find better address description on wongnai\n",
    "    # for example: \"991 ถนนพระราม 1 Pathum Wan, กรุงเทพมหานคร (กทม.) 10330 ไทย\"\n",
    "    address_tripAdvisor = \"\"\n",
    "    possible_address_xpath = [\n",
    "        '//*[@id=\"tab-data-WebPresentation_PoiLocationSectionGroup\"]/div/div/div[2]/div[1]/div/div/div/div[1]/button/span',\n",
    "        '//*[@id=\"tab-data-WebPresentation_PoiLocationSectionGroup\"]/div/div/div[2]/div[1]/div/div/div/div/button/span'\n",
    "    ]\n",
    "\n",
    "\n",
    "    for cur_address_xpath in possible_address_xpath:\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, cur_address_xpath)))\n",
    "            address_element = attraction_page_driver.find_element(By.XPATH, cur_address_xpath)\n",
    "            address_tripAdvisor = address_element.text\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "\n",
    "    # start scrape location\n",
    "    res_location = Location()\n",
    "    cnt_retry = 0\n",
    "    try:\n",
    "        while(True):\n",
    "            if(cnt_retry == 10):\n",
    "                print(\"max retry for scrape Google Map ...\")\n",
    "                break\n",
    "            \n",
    "            # set up new webdriver to work googlemap url(query for specific lat/long)\n",
    "            possible_addressGoogleMap_elements = []\n",
    "            try:\n",
    "                # set Chrome options to run in headless mode\n",
    "                # options = Options()\n",
    "                options = webdriver.ChromeOptions()\n",
    "                options.add_argument(\"start-maximized\")\n",
    "                # options.add_argument(\"--headless=new\")\n",
    "                options.add_experimental_option(\n",
    "                    \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "                )\n",
    "\n",
    "                google_map_driver = webdriver.Chrome(options=options)\n",
    "                \n",
    "                google_map_query = \"https://www.google.com/maps/search/?api=1&query=%s,%s\" % (latitude, longitude)\n",
    "                google_map_driver.get(google_map_query)\n",
    "                print(\"scrape location data for, \", google_map_query)\n",
    "                \n",
    "                WebDriverWait(google_map_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'DkEaL')))\n",
    "                possible_addressGoogleMap_elements = google_map_driver.find_elements(By.CLASS_NAME, 'DkEaL')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"retry  scrape Google Map..\")\n",
    "                cnt_retry += 1\n",
    "                google_map_driver.close()\n",
    "                continue\n",
    "\n",
    "\n",
    "            # after init new webdriver -> continure scrape location data\n",
    "\n",
    "            # if found some wiered place that doesn't even have its address\n",
    "            # skip this case for now...\n",
    "            if(not len(possible_addressGoogleMap_elements)):\n",
    "                return res_location\n",
    "\n",
    "            subStrDistrict = \"อำเภอ\"\n",
    "            subStrSubDistrict = \"ตำบล\"\n",
    "\n",
    "            if province_th == \"กรุงเทพมหานคร\":\n",
    "                subStrDistrict = \"เขต\"\n",
    "                subStrSubDistrict = \"แขวง\"\n",
    "\n",
    "            district = 0\n",
    "            subDirstrict = 0\n",
    "\n",
    "            # find location\n",
    "            useData = None\n",
    "            for cur_element in possible_addressGoogleMap_elements:\n",
    "                if province_th in cur_element.text and cur_element.text.find(subStrDistrict) != -1:\n",
    "                    useData = cur_element.text.replace(\",\",\"\").replace(\"เเ\",\"แ\")\n",
    "                    break\n",
    "           \n",
    "            if(useData != None):\n",
    "                # print(\"Full Address :\",useData)\n",
    "                # another brute force way in case of province 'กรุงเทพหมานคร' not have word 'แขวง' in address\n",
    "                if(province_th == 'กรุงเทพมหานคร' and useData.find(subStrSubDistrict) == -1):\n",
    "                    subAddress_split = useData.split(' ')\n",
    "                    cur_province_Idx = subAddress_split.index(province_th)\n",
    "                    district = subAddress_split[cur_province_Idx - 1].replace(\"เขต\",\"\")\n",
    "\n",
    "                else:\n",
    "                    start_address_index = useData.find(subStrDistrict)\n",
    "                    subAddress = useData[start_address_index:]\n",
    "                    district = subAddress[subAddress.find(subStrDistrict)+len(subStrDistrict):subAddress.find(province_th)].replace(\" \",\"\")               \n",
    "\n",
    "                if district == \"เมือง\":\n",
    "                    district = district+province_th\n",
    "\n",
    "                # filter row to find 'ISO_3166_code', 'zip_code', 'geo_code'\n",
    "                geo_code_df = pd.read_csv(fh.PATH_TO_GEOCODE)\n",
    "                filtered_rows = geo_code_df[\n",
    "                    (geo_code_df['province_th'] == province_th) & (geo_code_df['district_th'] == district)\n",
    "                ]\n",
    "                filtered_rows.reset_index(inplace=True, drop=True)\n",
    "                \n",
    "                if not filtered_rows.empty:\n",
    "                    print(\"found province :\",filtered_rows.loc[0, 'ISO_3166_code'], province_th)\n",
    "                    print(\"found District :\",filtered_rows.loc[0, 'zip_code'], district)\n",
    "\n",
    "                    res_location.set_address(address_tripAdvisor if len(address_tripAdvisor) else useData)\n",
    "                    res_location.set_province(province_th)\n",
    "                    res_location.set_district(district)\n",
    "                    res_location.set_sub_district(\"\")\n",
    "                    res_location.set_province_code(filtered_rows.loc[0, 'ISO_3166_code'])\n",
    "                    res_location.set_district_code(filtered_rows.loc[0, 'zip_code'])\n",
    "                    res_location.set_sub_district_code(0)\n",
    "\n",
    "                else:\n",
    "                    print(\"not found province :\", province_th)\n",
    "                    print(\"not found District :\", district)\n",
    "\n",
    "                    res_location.set_address(address_tripAdvisor if len(address_tripAdvisor) else useData)\n",
    "                    res_location.set_province(province_th)\n",
    "                    res_location.set_district(district)\n",
    "                    res_location.set_sub_district(\"\")\n",
    "                    res_location.set_province_code(0)\n",
    "                    res_location.set_district_code(0)\n",
    "                    res_location.set_sub_district_code(0)\n",
    "\n",
    "            google_map_driver.close()\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"can't scrape location data\")\n",
    "\n",
    "    return res_location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape lat/long, and openingHours (there are in another page of current attraction)\n",
    "def scrape_location_latlong_openingHours(attraction_page_driver: webdriver, link_to_adjust_page: str) -> tuple[float, float, dict]:\n",
    "    lat = 0\n",
    "    long = 0\n",
    "    openingHours = {}\n",
    "    location = Location()\n",
    "\n",
    "    # # find link to get adjust page of current attraction\n",
    "    # # for example: \"https://th.tripadvisor.com/ImproveListing-d8820434.html\"\n",
    "    # possible_container_xpath = [\n",
    "    #     '//*[@id=\"AR_ABOUT\"]/div[1]/div/div/div[4]/div[2]/span/a',\n",
    "    #     '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[2]/div[2]/div/div[1]/section[2]/div/div/div[2]/div/div/div/span/a'\n",
    "    # ]\n",
    "\n",
    "    # link_to_adjust_page = \"\"\n",
    "    # for cur_xpath in possible_container_xpath:\n",
    "    #     try:\n",
    "    #         print(\"E1\")\n",
    "    #         WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, cur_xpath)))\n",
    "    #         print(\"E2\")\n",
    "    #         adjust_page_link_element = attraction_page_driver.find_element(By.XPATH, cur_xpath)\n",
    "    #         print(\"E3\")\n",
    "    #         link_to_adjust_page = adjust_page_link_element.get_attribute('href')\n",
    "    #         print(\"E4\")\n",
    "    #         break\n",
    "\n",
    "    #     except Exception as e:\n",
    "    #         pass\n",
    "    \n",
    "\n",
    "    # create new webdriver to continue scrape lat/long, openingHours in adjust attraction page\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape single attraction ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "\n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        adjust_page_driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # retry in case of web restrictions, some elements not loaded\n",
    "        try:\n",
    "            print(\"scrape data in adjust attraction page...\")\n",
    "            print(\"for link : \", link_to_adjust_page)\n",
    "            adjust_page_driver.get(link_to_adjust_page)\n",
    "\n",
    "            print(\"debug option of adjust page: \")\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'DiHOR')))\n",
    "\n",
    "            # find dropdown --> click display data below --> cick display lat/long input form\n",
    "            possible_target_btn = adjust_page_driver.find_elements(By.CLASS_NAME, 'DiHOR')\n",
    "            for cur_dropdown_btn in possible_target_btn:\n",
    "                cur_dropdown_text = cur_dropdown_btn.text\n",
    "                if(\"แนะนำการแก้ไขข้อมูลของสถานที่นี้\" in cur_dropdown_text):\n",
    "                    print(\"found target dropdown btn ...\")\n",
    "                    cur_dropdown_btn.click()\n",
    "                    WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/button')))\n",
    "                    # find button click to display lat/long input form\n",
    "                    display_lat_long_btn = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/button')\n",
    "                    display_lat_long_btn.click()\n",
    "\n",
    "        except Exception as e:\n",
    "            cnt_retry += 1\n",
    "            adjust_page_driver.quit()\n",
    "            print(\"retry adjust page...\")\n",
    "            continue\n",
    "\n",
    "              \n",
    "        # find lat/long\n",
    "        try:\n",
    "            WebDriverWait(adjust_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[1]/div/div[2]/div/div/div/span')))\n",
    "            WebDriverWait(adjust_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[2]/div/div[2]/div/div/div/span')))\n",
    "    \n",
    "            lat_input_container = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[1]/div/div[2]/div/div/div/span')\n",
    "            lat_input_element = lat_input_container.find_element(By.TAG_NAME, 'input')\n",
    "            lat = float(lat_input_element.get_attribute('value'))\n",
    "\n",
    "            long_input_container = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[2]/div/div[2]/div/div/div/span')\n",
    "            long_input_element = long_input_container.find_element(By.TAG_NAME, 'input')\n",
    "            long = float(long_input_element.get_attribute('value'))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find lat/long\")\n",
    "        \n",
    "        print(\"lat : \", lat)\n",
    "        print(\"long : \", long)\n",
    "\n",
    "        # **if can't find lat/long --> don't scrape this attaction\n",
    "        if(lat == 0 and long == 0):\n",
    "            print(\"in scrape_location_latlong_openingHours --> can't find lat/long --> 0, 0\")\n",
    "            return lat, long, openingHours.copy()\n",
    "\n",
    "        # find openingHours\n",
    "        try:\n",
    "            all_openingHours_container = adjust_page_driver.find_elements(By.CLASS_NAME, 'dNAjp')\n",
    "            for cur_openingHours_container in all_openingHours_container:\n",
    "                cur_day_element = cur_openingHours_container.find_element(By.CLASS_NAME, 'ngXxk')\n",
    "                cur_day_text = cur_day_element.text.replace(\":\", \"\")\n",
    "\n",
    "                cur_time_element = cur_openingHours_container.find_element(By.CLASS_NAME, 'KxBGd')\n",
    "                cur_time_text = cur_time_element.text\n",
    "\n",
    "                openingHours[cur_day_text] = cur_time_text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find openingHours ...\")\n",
    "\n",
    "        print(\"openingHours : \", openingHours.copy())\n",
    "\n",
    "        adjust_page_driver.quit()\n",
    "        break\n",
    "\n",
    "    return lat, long, openingHours.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_attraction(link_to_attraction: str, province_th: str) -> Attraction:\n",
    "    \n",
    "    attraction = Attraction()\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape single attraction ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "\n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        attraction_page_driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        link_to_adjust_page = \"\"\n",
    "        try:\n",
    "            print(\"******************************************************\")\n",
    "            print(\"scrape single attraction...\")\n",
    "            print(\"for attraction : \", link_to_attraction)\n",
    "            attraction_page_driver.get(link_to_attraction)\n",
    "\n",
    "            print(\"debug scrape_single_attraction: common component section\")\n",
    "            WebDriverWait(attraction_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'IDaDx')))\n",
    "            \n",
    "            #  retry incase link to adjust page not loaded\n",
    "            print(\"debug scrape_single_attraction: find link to adjust page fix thissssssss not to use xpath but a inside xpath\")\n",
    "            # find link to get adjust page of current attraction\n",
    "            # for example: \"https://th.tripadvisor.com/ImproveListing-d8820434.html\"\n",
    "            container_xpath = [\n",
    "                '//*[@id=\"AR_ABOUT\"]/div[1]',\n",
    "                '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[2]/div[2]/div/div[1]/section[2]/div/div/div[2]/div'\n",
    "            ]\n",
    "        \n",
    "            for cur_xpath in container_xpath:\n",
    "                try:\n",
    "                    WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, cur_xpath)))\n",
    "                    cur_container = attraction_page_driver.find_element(By.XPATH, cur_xpath)\n",
    "                    possible_adjust_page_container = cur_container.find_elements(By.TAG_NAME, 'a')\n",
    "                    for cur_link_element in possible_adjust_page_container:\n",
    "                        print(\"d\")\n",
    "                        cur_text = cur_link_element.text\n",
    "                        print(\"cur_text --> \", cur_text)\n",
    "                        if(cur_text == 'ปรับปรุงข้อมูลสถานที่ให้บริการนี้'):\n",
    "                            link_to_adjust_page = cur_link_element.get_attribute('href')\n",
    "\n",
    "                    print(\"-*-\")\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "            if(not len(link_to_adjust_page)):\n",
    "                raise Exception(\"no link to get adjust attraction page\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"retry single attraction case 1...\")\n",
    "            cnt_retry += 1\n",
    "            attraction_page_driver.quit()\n",
    "            continue\n",
    "        \n",
    "        # ** find lat/long, location data and openingHours (there are in another page of current attraction)\n",
    "        # ** if this attraction not have lat/long\n",
    "        # ** don't continue to scrape\n",
    "        lat, long, openingHours = scrape_location_latlong_openingHours(\n",
    "            attraction_page_driver = attraction_page_driver,\n",
    "            link_to_adjust_page = link_to_adjust_page\n",
    "        )\n",
    "        \n",
    "        # **if can't find lat/long --> don't scrape this attaction\n",
    "        if(lat == 0 and long == 0):\n",
    "            print(\"in scrape_single_attraction --> can't find lat/long --> don't scrape this attraction ...\")\n",
    "            attraction_page_driver.quit()\n",
    "            return attraction\n",
    "\n",
    "        # find name\n",
    "        name = \"\"\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[1]/header/div[3]/div[1]/div/h1')))\n",
    "            name_element = attraction_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[1]/header/div[3]/div[1]/div/h1')\n",
    "            name = name_element.text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find name\")\n",
    "\n",
    "        print(\"name -> \", name)\n",
    "\n",
    "        # find description\n",
    "        description = \"\"\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"AR_ABOUT\"]/div[1]')))\n",
    "            \n",
    "            description_container = attraction_page_driver.find_element(By.XPATH, '//*[@id=\"AR_ABOUT\"]/div[1]')\n",
    "            header_element = description_container.find_element(By.CLASS_NAME, 'biGQs')\n",
    "            header_text = header_element.text\n",
    "            if(header_text == 'ข้อมูล'):\n",
    "                description_element = attraction_page_driver.find_element(By.CLASS_NAME, 'JguWG')\n",
    "                description = description_element.text\n",
    "                \n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find description\")\n",
    "\n",
    "        print(\"description -> \", description)\n",
    "        \n",
    "        # find rating\n",
    "        rating = 0\n",
    "        ratingCount = 0\n",
    "        try:\n",
    "            WebDriverWait(attraction_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[2]/div[2]/div/div[1]/section[1]/div/div/div/div/div[1]/div[1]/a/div')))\n",
    "            score_element = attraction_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[1]/div[2]/div[2]/div[2]/div/div[1]/section[1]/div/div/div/div/div[1]/div[1]/a/div')\n",
    "            score_text_list = score_element.get_attribute('aria-label').split(' ')\n",
    "            for Idx in range(1, len(score_text_list)):\n",
    "                # set rating\n",
    "                if(score_text_list[Idx - 1] == \"คะแนน\"):\n",
    "                    rating = float(score_text_list[Idx])\n",
    "\n",
    "                elif(score_text_list[Idx - 1] == \"รีวิว\"):\n",
    "                    ratingCount = int(score_text_list[Idx].replace(',', ''))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find rating and ratingCount\")\n",
    "\n",
    "        print(\"rating --> \", rating)\n",
    "        print(\"ratingCount --> \", ratingCount)\n",
    "        \n",
    "        # find location\n",
    "        location = scrape_location(\n",
    "            attraction_page_driver = attraction_page_driver,\n",
    "            latitude = lat,\n",
    "            longitude = long,\n",
    "            province_th = province_th\n",
    "        )\n",
    "        print(\"province :\", location.get_province_code(), location.get_province())\n",
    "        print(\"District :\", location.get_district_code(), location.get_district())\n",
    "\n",
    "        # find img_path\n",
    "        img_path = scrape_img(attraction_page_driver)\n",
    "        print(\"cur img path -> \", img_path)\n",
    "\n",
    "        # set some of \"Attraction\" object properties\n",
    "        \n",
    "        attraction_page_driver.quit()\n",
    "        break\n",
    "\n",
    "    return attraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_url_by_page(query_url: str) -> list[str]:\n",
    "\n",
    "    res_url_by_page = []\n",
    "\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape data by page ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        # os.environ['proxy_port']\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "      \n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # just check for ip\n",
    "        # print(\"just check for ip :\")\n",
    "        # driver.get(\"https://httpbin.io/ip\")\n",
    "        # print(driver.page_source)\n",
    "\n",
    "        # find group of restaurant on the nth page\n",
    "        all_attractions_card = []\n",
    "\n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            driver.get(query_url)\n",
    "            # scroll and wait for some msec\n",
    "            driver.execute_script('window.scrollBy(0, document.body.scrollHeight)')\n",
    "            \n",
    "            print(\"check current page url --> \", driver.current_url)\n",
    "\n",
    "            # wait for div (each attraction section) to be present and visible\n",
    "            print(\"b1\")\n",
    "            print(\"debug get_all_url_by_page: attraction by one page section\")\n",
    "            WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'XJlaI')))\n",
    "\n",
    "            print(\"b2\")\n",
    "            print(\"debug get_all_url_by_page: text\")\n",
    "            WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'BKifx')))\n",
    "            \n",
    "            # print(\"b3\")\n",
    "            # print(\"debug get_all_url_by_page: link to single attraction\")\n",
    "            # WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.TAG_NAME, 'a')))\n",
    "\n",
    "            print(\"b3\")\n",
    "            print(\"check in loop ...\")\n",
    "            all_attractions_card = driver.find_elements(By.CLASS_NAME, 'XJlaI')\n",
    "            for cur_attraction_card in all_attractions_card:\n",
    "\n",
    "                cur_attraction_url = cur_attraction_card.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "\n",
    "                check_text = cur_attraction_card.find_element(By.CLASS_NAME, 'BKifx').text  \n",
    "               \n",
    "                # check if cuurent card is for attraction ?\n",
    "                not_attraction_keyword = ['ทัวร์', \"สปา\", \"กิจกรรมทางวัฒนธรรม\", 'ชั้นเรียน', 'รถรับส่ง', 'อุปกรณ์ให้เช่า', 'ร้านขายของ']\n",
    "                for cur_check_word in not_attraction_keyword:\n",
    "                    if(cur_check_word in check_text):\n",
    "                        break\n",
    "\n",
    "                print(\"cur_attraction_url : \", cur_attraction_url)\n",
    "                res_url_by_page.append(cur_attraction_url)\n",
    "            \n",
    "            driver.quit()\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"retry find all_restaurants_card ...\")\n",
    "            cnt_retry += 1\n",
    "            driver.quit()\n",
    "            continue\n",
    "\n",
    "    return res_url_by_page.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_attraction_by_province(province_url: str, province: str) -> pd.DataFrame:\n",
    "    # res_attraction_df = pd.DataFrame()\n",
    "    res_attraction_df = create_attraction_df(Attraction())\n",
    "    \n",
    "    cnt_for_debug = 0\n",
    "\n",
    "    while(True):\n",
    "        if(cnt_for_debug == 1):\n",
    "            break\n",
    "        cnt_for_debug += 1\n",
    "        \n",
    "        print(\"scraping attraction | province --> %s | page --> %s\" % (province, cnt_for_debug))\n",
    "\n",
    "        # try:\n",
    "        # get url of to all attraction in current page\n",
    "        all_url_by_page = get_all_url_by_page(query_url=province_url)\n",
    "    \n",
    "        # use data from 'res_get_data_by_page' to retrive data of specific attraction\n",
    "        for cur_attraction_url in all_url_by_page:\n",
    "            \n",
    "            # continue scraping data for a specific resgtaurant\n",
    "            cur_attraction = scrape_single_attraction(\n",
    "                link_to_attraction = cur_attraction_url,\n",
    "                province_th = province\n",
    "            )\n",
    "            \n",
    "            # cur_attraction = scrape_single_attraction(\n",
    "            #     link_to_attraction = \"https://th.tripadvisor.com/Attraction_Review-g297930-d3387563-Reviews-Jungceylon-Patong_Kathu_Phuket.html\",\n",
    "            #     province_th = 'ภูเก็ต'\n",
    "            # )\n",
    "\n",
    "            # create data frame represent data scrape from current attraction card\n",
    "            cur_attraction_df = create_attraction_df(attraction=cur_attraction)\n",
    "\n",
    "            # concat all data frame result\n",
    "            res_attraction_df = pd.concat([res_attraction_df, cur_attraction_df])\n",
    "        \n",
    "        # except Exception as e:\n",
    "        #     pass\n",
    "\n",
    "    return res_attraction_df.iloc[1:, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory res_attraction_scraping created successfully\n",
      "scraping attraction | province --> สุรินทร์ | page --> 1\n",
      "check current page url -->  https://th.tripadvisor.com/Attractions-g2099297-Activities-c47-Surin_Province.html\n",
      "b1\n",
      "debug get_all_url_by_page: attraction by one page section\n",
      "b2\n",
      "debug get_all_url_by_page: text\n",
      "b3\n",
      "check in loop ...\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d4322926-Reviews-Surin_National_Museum-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d4322931-Reviews-Ban_Tha_Sawang_Silk_Weaving_Village-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d4322930-Reviews-City_Pillar_Shrine-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d6379888-Reviews-Phanom_Sawai_Forest_Park-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d4322932-Reviews-Wat_Burapharam-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2237594-d4322582-Reviews-Prasat_Si_Khoraphum-Sikhoraphum_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2237588-d6420101-Reviews-Prasat_Ban_Pluang-Prasat_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d4322541-Reviews-Prasat_Mueang_Thi-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d10202649-Reviews-Phraya_Surin_Pakdee_Srinarong_Jangwang_Pum_Monument-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2237591-d3814567-Reviews-Prasat_Phum_Pon-Sangkha_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d6954129-Reviews-Wat_Salaloy-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2237580-d12855614-Reviews-Wat_Khaosala-Buachet_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2237583-d13817044-Reviews-Wat_Chang_Mop-Kap_Choeng_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2237591-d4322928-Reviews-Prasat_Yai_Ngao-Sangkha_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2237588-d4322540-Reviews-Prasat_Ban_Phlai-Prasat_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2237584-d6669387-Reviews-Khwao_Sinarin_Handicraft_Village-Khwao_Sinarin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d6669573-Reviews-Ban_Buthom_Basketry_Village-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d6954116-Reviews-Wat_Nongbua-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g303923-d14794412-Reviews-Wat_Thepsurin-Surin_Surin_Province.html\n",
      "cur_attraction_url :  https://th.tripadvisor.com/Attraction_Review-g2237587-d15592833-Reviews-Prasat_Ta_Muen-Phanom_Dong_Rak_Surin_Province.html\n",
      "******************************************************\n",
      "scrape single attraction...\n",
      "for attraction :  https://th.tripadvisor.com/Attraction_Review-g303923-d4322926-Reviews-Surin_National_Museum-Surin_Surin_Province.html\n",
      "debug scrape_single_attraction: common component section\n",
      "debug scrape_single_attraction: find link to adjust page fix thissssssss not to use xpath but a inside xpath\n",
      "d\n",
      "cur_text -->  โดย BaimaiBen\n",
      "“ชมประวัติศาสตร์เมืองสุรินทร์”\n",
      "คะแนน 4.0 จาก 5 แต้ม\n",
      "ธ.ค. ค.ศ. 2017\n",
      "ได้ความรู้เกี่ยวเมืองสุรินทร์ที่ไม่เคยรู้มาก่อน ตั้งแต่เริ่มต้นจนถึงปัจจุบันชอบมากๆ ถ้ามีโอกาสจะกลับไปเยี่ยมชมอีกครั้ง\n",
      "d\n",
      "retry single attraction case 1...\n",
      "******************************************************\n",
      "scrape single attraction...\n",
      "for attraction :  https://th.tripadvisor.com/Attraction_Review-g303923-d4322926-Reviews-Surin_National_Museum-Surin_Surin_Province.html\n",
      "debug scrape_single_attraction: common component section\n",
      "debug scrape_single_attraction: find link to adjust page fix thissssssss not to use xpath but a inside xpath\n",
      "d\n",
      "retry single attraction case 1...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 75\u001b[0m, in \u001b[0;36mscrape_single_attraction\u001b[1;34m(link_to_attraction, province_th)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(link_to_adjust_page)):\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno link to get adjust attraction page\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mException\u001b[0m: no link to get adjust attraction page",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python312\\Lib\\socket.py:837\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[0;32m    836\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m--> 837\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m cur_province_url \u001b[38;5;241m=\u001b[39m cur_region_data[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# get dataframe result of all attraction in current province\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m cur_res_allAttractions_df \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_attraction_by_province\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovince_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcur_province_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovince\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcur_province_th\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# don't forget to remove row with lat/long be zero\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# remove duplicate restaurant \u001b[39;00m\n\u001b[0;32m     22\u001b[0m cur_res_allAttractions_df\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[8], line 22\u001b[0m, in \u001b[0;36mscrape_attraction_by_province\u001b[1;34m(province_url, province)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# use data from 'res_get_data_by_page' to retrive data of specific attraction\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cur_attraction_url \u001b[38;5;129;01min\u001b[39;00m all_url_by_page:\n\u001b[0;32m     20\u001b[0m     \n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# continue scraping data for a specific resgtaurant\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     cur_attraction \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_single_attraction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlink_to_attraction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcur_attraction_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprovince_th\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprovince\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# cur_attraction = scrape_single_attraction(\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m#     link_to_attraction = \"https://th.tripadvisor.com/Attraction_Review-g297930-d3387563-Reviews-Jungceylon-Patong_Kathu_Phuket.html\",\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m#     province_th = 'ภูเก็ต'\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# create data frame represent data scrape from current attraction card\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     cur_attraction_df \u001b[38;5;241m=\u001b[39m create_attraction_df(attraction\u001b[38;5;241m=\u001b[39mcur_attraction)\n",
      "Cell \u001b[1;32mIn[6], line 80\u001b[0m, in \u001b[0;36mscrape_single_attraction\u001b[1;34m(link_to_attraction, province_th)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretry single attraction case 1...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m     cnt_retry \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 80\u001b[0m     \u001b[43mattraction_page_driver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# ** find lat/long, location data and openingHours (there are in another page of current attraction)\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# ** if this attraction not have lat/long\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# ** don't continue to scrape\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\seleniumwire\\webdriver.py:68\u001b[0m, in \u001b[0;36mDriverCommonMixin.quit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Shutdown Selenium Wire and then quit the webdriver.\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mshutdown()\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:193\u001b[0m, in \u001b[0;36mChromiumDriver.quit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py:146\u001b[0m, in \u001b[0;36mService.stop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_remote_shutdown_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py:131\u001b[0m, in \u001b[0;36mService.send_remote_shutdown_command\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[1;32m--> 131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_connectable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     sleep(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py:120\u001b[0m, in \u001b[0;36mService.is_connectable\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_connectable\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Establishes a socket connection to determine if the service running\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    on the port is accessible.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_connectable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\selenium\\webdriver\\common\\utils.py:101\u001b[0m, in \u001b[0;36mis_connectable\u001b[1;34m(port, host)\u001b[0m\n\u001b[0;32m     99\u001b[0m socket_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 101\u001b[0m     socket_ \u001b[38;5;241m=\u001b[39m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _is_connectable_exceptions:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\socket.py:844\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m error \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_errors:\n\u001b[1;32m--> 844\u001b[0m         \u001b[43mexceptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# raise only the last error\u001b[39;00m\n\u001b[0;32m    845\u001b[0m     exceptions\u001b[38;5;241m.\u001b[39mappend(exc)\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create directory 'res_restaurant_scraping'\n",
    "createDirectory(fh.STORE_ATTRACTION_SCRAPING, 'res_attraction_scraping')\n",
    "\n",
    "# *** select one province from 'ALL_PROVINCE_TRIPADVISOR_DATA'\n",
    "# *** so, change \"Idx_of_region\" everytime when scrape another province\n",
    "Idx_of_region = 0\n",
    "cur_region_data = ALL_PROVINCE_TRIPADVISOR_DATA[Idx_of_region]\n",
    "\n",
    "cur_province_en = cur_region_data[0]\n",
    "cur_province_th = cur_region_data[1]\n",
    "cur_province_url = cur_region_data[2]\n",
    "\n",
    "# get dataframe result of all attraction in current province\n",
    "cur_res_allAttractions_df = scrape_attraction_by_province(\n",
    "    province_url = cur_province_url,\n",
    "    province = cur_province_th\n",
    ")\n",
    "\n",
    "# don't forget to remove row with lat/long be zero\n",
    "\n",
    "# remove duplicate restaurant \n",
    "cur_res_allAttractions_df.drop_duplicates(subset=['name'], inplace=True)\n",
    "# set new index\n",
    "cur_res_allAttractions_df.set_index(['name'], inplace=True)\n",
    "\n",
    "# save result dataframe to .csv\n",
    "res_file_name = 'res_attraction_%s.csv' % (cur_province_en)\n",
    "res_path = os.path.join(fh.STORE_ATTRACTION_SCRAPING, 'res_attraction_scraping', res_file_name) \n",
    "cur_res_allAttractions_df.to_csv(res_path, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
