{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pyautogui\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import constants.constants as const\n",
    "import constants.file_handler_constants as fh\n",
    "from constants.attraction_constants import *\n",
    "\n",
    "from packages.attraction.Attraction import *\n",
    "from packages.file_handler_package.file_handler import *\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.actions.wheel_input import ScrollOrigin\n",
    "from selenium.webdriver import ActionChains\n",
    "# from selenium import webdriver\n",
    "from selenium.webdriver.chrome.webdriver import WebDriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "from selenium.webdriver.remote.webelement import WebElement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Formats a search parameter string for Google Maps by replacing spaces with '+'.\n",
    "\n",
    "Args:\n",
    "    searchParam: The search parameter string to format.\n",
    "\n",
    "Returns:\n",
    "    The formatted search parameter string with spaces replaced by '+'.\n",
    "\"\"\"\n",
    "def format_searchParam(searchParam) -> str:\n",
    "    return searchParam.replace(\" \", \"+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Simulates infinite scrolling on a Google Maps search page to retrieve all card elements (appeared in DOM)\n",
    "\n",
    "Args:\n",
    "    driver: Selenium WebDriver instance representing the open Google Chrome browser window\n",
    "\n",
    "Returns:\n",
    "    list of card element\n",
    "'''\n",
    "def GoogleMapsInfiniteScroller(driver:WebDriver) -> list[WebDriver]:\n",
    "    action = ActionChains(driver)\n",
    "    isFetchAllCard = False\n",
    "    wrapper_div = driver.find_elements(By.XPATH, '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]')\n",
    "    temp_list_card = []\n",
    "    while not isFetchAllCard:\n",
    "        scroll_origin = ScrollOrigin.from_element(wrapper_div[0])\n",
    "        action.scroll_from_origin(scroll_origin, 0, 5000).perform()        \n",
    "        time.sleep(const.SCROLL_PAUSE_TIME)\n",
    "        # temp_list_card = driver.find_elements(By.CLASS_NAME, \"hfpxzc\")\n",
    "        # ensure that there is no others card element(fetched from google every time scrolling) --> break the loop\n",
    "        # check whether scorlling untill found section 'คุณมาถึงส่วนท้ายของรายการแล้ว' or 'You've reached the end of the list.'\n",
    "        reach_end_section_span = driver.find_elements(By.CLASS_NAME, \"HlvSq\")\n",
    "        if(len(reach_end_section_span)):\n",
    "            for cur_span in reach_end_section_span:\n",
    "                if(cur_span.text == \"คุณมาถึงส่วนท้ายของรายการแล้ว\" or cur_span.text == \"You've reached the end of the list.\"):\n",
    "                    isFetchAllCard = True\n",
    "                    temp_list_card = driver.find_elements(By.CLASS_NAME, \"hfpxzc\")\n",
    "                    break\n",
    "    \n",
    "    return temp_list_card.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Simulates infinite scrolling on a Google Maps Images page to retrieve all card elements (appeared in DOM)\n",
    "\n",
    "Args:\n",
    "    driver: Selenium WebDriver instance representing the open Google Chrome browser window\n",
    "\n",
    "Returns:\n",
    "    \n",
    "'''\n",
    "def findImageInfiniteScroller(driver:WebDriver, max_len_img:int) -> list[str]:\n",
    "    res_imgPath = []\n",
    "    cnt_card = 0\n",
    "\n",
    "    while(True):\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"play\"]/div/button[2]')))\n",
    "        cur_right_arrow = driver.find_elements(By.XPATH, '//*[@id=\"play\"]/div/button[2]')[0]\n",
    "        cur_disabled_status = cur_right_arrow.get_attribute('disabled')\n",
    "        cnt_card += 1\n",
    "        if(cnt_card >= max_len_img or cur_disabled_status == 'true'):\n",
    "            break\n",
    "        cur_right_arrow.click()\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    # extract image from css property of all element in 'all_list_card'\n",
    "    all_list_card = driver.find_elements(By.CLASS_NAME, 'Uf0tqf')[:max_len_img+1]\n",
    "    for cur_card in all_list_card:\n",
    "        cur_bgImg_val = cur_card.value_of_css_property('background-image')\n",
    "        match = re.search(r'url\\(\"(.*?)\"\\)', cur_bgImg_val)\n",
    "        if match:\n",
    "            res_imgPath.append(match.group(1))\n",
    "            \n",
    "    # print(\"res_imgPath :\")\n",
    "    # print(res_imgPath)\n",
    "    return res_imgPath.copy() if len(res_imgPath) > 0 else ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findImgPathCurCard(driver:webdriver) -> list[str]:\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"start-maximized\")\n",
    "\n",
    "    # get the new URL\n",
    "    new_cur_card_url = driver.current_url\n",
    "    # print('new_cur_card_url -> ',new_cur_card_url)\n",
    "    # create a new WebDriver instance\n",
    "    new_cur_card_driver = webdriver.Chrome(options=options)\n",
    "    new_cur_card_driver.get(new_cur_card_url)\n",
    "    WebDriverWait(new_cur_card_driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'ZKCDEc')))\n",
    "    # WebDriverWait(new_cur_card_driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'aoRNLd kn2E5e NMjTrf lvtCsd')))\n",
    "    soup = BeautifulSoup(new_cur_card_driver.page_source, 'html.parser')\n",
    "\n",
    "    # find length of all reviewed images for current attraction(rendered at hovered thumbnail as '26 รูป', '32,667 รูป')\n",
    "    len_all_img = 0\n",
    "    find_img_len = soup.find_all('div', class_='YkuOqf')\n",
    "    if(len(find_img_len)):\n",
    "        len_all_img = int(find_img_len[0].text.replace(' รูป','').replace(',',''))\n",
    "    # in some case there is no reviewed images --> exit process(can not continue to read images)\n",
    "    else:\n",
    "        return ['']\n",
    "    # print('check len_all_img -> ', len_all_img)\n",
    "\n",
    "    # click at thumnail image button if there's some image of current attraction\n",
    "    btn_click_thumbnail_temp_1 = new_cur_card_driver.find_elements(By.XPATH, '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[3]/div/div[1]/div/div/div[2]/div[1]/div[1]/button')\n",
    "    btn_click_thumbnail_temp_2 = new_cur_card_driver.find_elements(By.XPATH, '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/button')\n",
    "    btn_click_thumbnail = btn_click_thumbnail_temp_1.copy() if len(btn_click_thumbnail_temp_1) > 0 else btn_click_thumbnail_temp_2.copy()\n",
    "    btn_click_thumbnail[0].click()\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------\n",
    "    \n",
    "    # process of reading at most 20 image\n",
    "    # wait for page to load (change url from 'current card url' to 'website with all reviewed images')\n",
    "    WebDriverWait(new_cur_card_driver, 10).until(EC.url_changes(new_cur_card_driver.current_url))\n",
    "\n",
    "    # get the new URL\n",
    "    new_readImg_url = new_cur_card_driver.current_url\n",
    "    # print('new_readImg_url -> ', new_readImg_url)\n",
    "    # create a new WebDriver instance\n",
    "    new_readImg_driver = webdriver.Chrome(options=options)\n",
    "    new_readImg_driver.get(new_readImg_url)\n",
    "    try:\n",
    "        WebDriverWait(new_readImg_driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]')))\n",
    "        WebDriverWait(new_readImg_driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'Uf0tqf')))\n",
    "        WebDriverWait(new_readImg_driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"play\"]/div/button[2]'))) \n",
    "        \n",
    "    except Exception as e:\n",
    "        return ['']\n",
    "    \n",
    "    # read the first fetched image\n",
    "    # then read at most XX images\n",
    "    max_len_img = min(20, len_all_img)\n",
    "    res_imgPath = findImageInfiniteScroller(\n",
    "        driver = new_readImg_driver,\n",
    "        max_len_img = max_len_img\n",
    "    )\n",
    "   \n",
    "    new_readImg_driver.close()\n",
    "    time.sleep(1)\n",
    "    new_cur_card_driver.close()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    return res_imgPath.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScorefromGeminiAPI(name:str, latitude:str, longtitude:str, imgPath:list[str]) -> dict:\n",
    "    # if there are some image address\n",
    "    # download that image to temp directory\n",
    "    tempImg_url = \"\"\n",
    "    isDownload = False\n",
    "    path_to_tempImg = os.path.join(fh.STORE_ATTRACTION_SCRAPING, 'temp', 'temp_img.jpeg')\n",
    "    if(len(imgPath)):\n",
    "        mainImg_url = imgPath[0]\n",
    "        \n",
    "    # send api to get score for current attraction (send with query and main image) \n",
    "    \n",
    "\n",
    "    # remove temp image\n",
    "    if(isDownload):\n",
    "        os.remove(path_to_tempImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDatafromGoogleMapCard(driver:webdriver, cur_card:WebElement, cur_attraction:Attraction, cur_province_th:str, cur_geo_code_by_province_df:pd.DataFrame) -> pd.DataFrame:\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    action = ActionChains(driver)\n",
    "\n",
    "    patternPhone = re.compile(r'\\d{3} \\d{3} \\d{4}')\n",
    "    patternPhoneService = re.compile(r'\\d{3} \\d{3} \\d{3}')\n",
    "\n",
    "    subStrDistrict = \"อำเภอ\"\n",
    "    subStrSubDistrict = \"ตำบล\"\n",
    "\n",
    "    if cur_province_th == \"กรุงเทพมหานคร\":\n",
    "        subStrDistrict = \"เขต\"\n",
    "        subStrSubDistrict = \"แขวง\"\n",
    "\n",
    "    try:\n",
    "        action.move_to_element(cur_card).perform()\n",
    "        cur_card.click()\n",
    "        \n",
    "    except Exception as e:\n",
    "        # print('can not click card element')\n",
    "        # print('when scrape for province -> {0}'.format(cur_province_th))\n",
    "        return\n",
    "\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    name = soup.find_all('h1', class_='DUwDvf lfPIob')\n",
    "    description = soup.find_all('div', class_='PYvSYb')\n",
    "    address = soup.find_all('div', class_='Io6YTe')\n",
    "    # loc = soup.find_all('div', class_='rogA2c')\n",
    "\n",
    "    time.sleep(5)\n",
    "    start_index_lat = driver.current_url.find(\"!3d\") + 3\n",
    "    end_index_lat = driver.current_url.find(\"!4d\")\n",
    "    lat = driver.current_url[start_index_lat:end_index_lat]\n",
    "    start_index_long = driver.current_url.find(\"!4d\") + 3\n",
    "    end_index_long = driver.current_url.find(\"!\", driver.current_url.find(\"!4d\") + 1)\n",
    "    long = driver.current_url[start_index_long:end_index_long]\n",
    "    \n",
    "    # found some wiered place that doesn't even have its address\n",
    "    # skip this case for now...\n",
    "    if(not len(address)):\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    district = 0\n",
    "    subDirstrict = 0\n",
    "\n",
    "    # find attraction name\n",
    "    # print('name: ',name[0].text)\n",
    "    cur_attraction.set_name(name[0].text)\n",
    "\n",
    "    # find lat, long\n",
    "    # print(\"lat, long: \" + lat + \" \" + long)\n",
    "    cur_attraction.set_latitude(float(lat))\n",
    "    cur_attraction.set_longitude(float(long))\n",
    "\n",
    "\n",
    "    # find description\n",
    "    if(len(description)):\n",
    "        # print(\"Description :\",description[0].text)\n",
    "        cur_attraction.set_description(description[0].text)\n",
    "    \n",
    "\n",
    "    # find location\n",
    "    useData = None\n",
    "    for div in address:\n",
    "        if cur_province_th in div.text and div.text.find(subStrDistrict) != -1:\n",
    "            useData = div.text.replace(\",\",\"\").replace(\"เเ\",\"แ\")\n",
    "    \n",
    "    if(useData != None):\n",
    "        # print(\"Full Address :\",useData)\n",
    "        # another brute force way in case of province 'กรุงเทพหมานคร' not have word 'แขวง' in address\n",
    "        if(cur_province_th == 'กรุงเทพมหานคร' and useData.find(subStrSubDistrict) == -1):\n",
    "            subAddress_split = useData.split(' ')\n",
    "            cur_province_Idx = subAddress_split.index(cur_province_th)\n",
    "            district = subAddress_split[cur_province_Idx - 1].replace(\"เขต\",\"\")\n",
    "            subDistrict = subAddress_split[cur_province_Idx - 2].replace(\"แขวง\",\"\")\n",
    "\n",
    "        else:\n",
    "            start_address_index = useData.find(subStrSubDistrict)\n",
    "            subAddress = useData[start_address_index:]\n",
    "            district = subAddress[subAddress.find(subStrDistrict)+len(subStrDistrict):subAddress.find(cur_province_th)].replace(\" \",\"\")               \n",
    "            subDistrict = subAddress[subAddress.find(subStrSubDistrict)+len(subStrSubDistrict):subAddress.find(subStrDistrict)].replace(\" \",\"\")\n",
    "\n",
    "        if district == \"เมือง\":\n",
    "            district = district+cur_province_th\n",
    "\n",
    "        # filter row to find 'ISO_3166_code', 'zip_code', 'geo_code'\n",
    "        filtered_rows = cur_geo_code_by_province_df[\n",
    "            (cur_geo_code_by_province_df['district_th'] == district) & (cur_geo_code_by_province_df['subDistrict_th'] == subDistrict)\n",
    "        ]\n",
    "        filtered_rows.reset_index(inplace=True, drop=True)\n",
    "        if not filtered_rows.empty:\n",
    "            # print(\"province :\",filtered_rows.loc[0, 'ISO_3166_code'],cur_province_th)\n",
    "            # print(\"District :\",filtered_rows.loc[0, 'zip_code'],district)\n",
    "            # print(\"SubDistrict :\",filtered_rows.loc[0, 'geo_code'],subDistrict)\n",
    "\n",
    "            cur_attraction.set_location(\n",
    "                address = useData,\n",
    "                province = cur_province_th,\n",
    "                district = district,\n",
    "                sub_district = subDistrict,\n",
    "                iso_code = filtered_rows.loc[0, 'ISO_3166_code'],\n",
    "                zip_code = filtered_rows.loc[0, 'zip_code'],\n",
    "                geo_code = filtered_rows.loc[0, 'geo_code']\n",
    "            )\n",
    "        else:\n",
    "            # print(\"province :\",cur_province_th)\n",
    "            # print(\"District :\",district)\n",
    "            # print(\"SubDistrict :\",subDistrict)\n",
    "\n",
    "            cur_attraction.set_location(\n",
    "                address = useData,\n",
    "                province = cur_province_th,\n",
    "                district = district,\n",
    "                sub_district = subDistrict,\n",
    "                iso_code = 0,\n",
    "                zip_code = 0,\n",
    "                geo_code = 0\n",
    "            )\n",
    "\n",
    "    # find rating\n",
    "    score_div = soup.find_all('span', class_='ceNzKf')\n",
    "    if len(score_div):\n",
    "        score = score_div[0].get('aria-label').replace(\" \",\"\").replace(\"ดาว\",\"\").replace(\",\", \"\")\n",
    "        review_count_div = driver.find_elements(By.XPATH, '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[3]/div/div[1]/div/div/div[2]/div[2]/div/div[1]/div[2]/div/div[1]/div[2]/span[2]/span/span')\n",
    "        review_count = review_count_div[0].text.replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \"\")\n",
    "        \n",
    "        # print(\"Rating score :\",score)\n",
    "        # print(\"Rating Count:\", review_count)\n",
    "      \n",
    "        cur_attraction.set_rating(\n",
    "            score = float(score),\n",
    "            rating_count = int(review_count)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    # find ticket rate\n",
    "    ticketRating = soup.find_all('div', class_='drwWxc')\n",
    "    if(len(ticketRating)):\n",
    "        # print(\"Ticket Price :\",ticketRating[0].text)\n",
    "        pass\n",
    "\n",
    "\n",
    "    # find phone number\n",
    "    divContact = soup.find_all('div', class_='Io6YTe fontBodyMedium kR99db')\n",
    "    for div in divContact:\n",
    "        if(re.match(patternPhone,div.text) or re.match(patternPhoneService,div.text)):\n",
    "            # print(\"Contact :\",div.text)\n",
    "            cur_attraction.set_phone(div.text)\n",
    "\n",
    "\n",
    "    # find website\n",
    "    divWebsite = soup.find_all('a', class_='CsEnBe')\n",
    "    if(len(divWebsite)):\n",
    "        for i in range(len(divWebsite)):\n",
    "            # check if it fit below case\n",
    "            # aria-label=\"เว็บไซต์: ABC.com \"\n",
    "            cur_divWebSite_aria_label = divWebsite[i].get('aria-label')\n",
    "            isWebsite = \"เว็บไซต์: \" in cur_divWebSite_aria_label\n",
    "            if(isWebsite):\n",
    "                website_href = divWebsite[i].get('href')\n",
    "                # print(\"cur_website -> \",website_href)\n",
    "                cur_attraction.set_website(website_href)\n",
    "                break\n",
    "\n",
    "\n",
    "    # find opening hours\n",
    "    openingHourCheck = soup.find_all(\"span\", class_=\"HlvSq\")\n",
    "\n",
    "    if(len(openingHourCheck) and openingHourCheck[0].text == \"ดูเวลาทำการเพิ่มเติม\"):\n",
    "        infoOpening = driver.find_elements(By.CLASS_NAME, \"HlvSq\")\n",
    "        for element in infoOpening:\n",
    "            element.click()\n",
    "        wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'y0skZc')))\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    openingTime = soup.find_all(\"tr\", class_=\"y0skZc\")\n",
    "\n",
    "    '''\n",
    "    update 28/7/2024: found this case in place 'พิพิธภัณฑ์ศิลปะไทยร่วมสมัย', 'วิหารเทพวิทยาคม'\n",
    "    \n",
    "    วันอาทิตย์ (วันเฉลิมพระชนมพรรษา พระบาทสมเด็จพระปรเมนทรรามาธิบดีศรีสินทรมหาวชิราลงกรณ พระวชิรเกล้าเจ้าอยู่หัว)                10:00–18:00เวลาทำการในวันหยุด\n",
    "    วันจันทร์ (วันเฉลิมพระชนมพรรษา พระบาทสมเด็จพระปรเมนทรรามาธิบดีศรีสินทรมหาวชิราลงกรณ พระวชิรเกล้าเจ้าอยู่หัว (วันหยุดชดเชย))    ปิดทำการเวลาทำการในวันหยุด\n",
    "    วันอังคาร      10:00–18:00\n",
    "    วันพุธ        10:00–18:00\n",
    "    วันพฤหัสบดี    10:00–18:00\n",
    "    วันศุกร์       10:00–18:00\n",
    "    วันเสาร์         10:00–18:00\n",
    "    '''\n",
    "\n",
    "    cur_openingHour = {}\n",
    "    for data in openingTime:\n",
    "        dateDiv = data.find(\"td\", class_=\"ylH6lf\")\n",
    "        timeDiv = data.find(\"td\", class_=\"mxowUb\")\n",
    "        # print(dateDiv.text,timeDiv.text)\n",
    "        cur_openingHour[dateDiv.text] = timeDiv.text\n",
    "        \n",
    "    cur_attraction.set_openingHour(cur_openingHour)\n",
    "\n",
    "\n",
    "    # find img path\n",
    "    cur_card_img_path = findImgPathCurCard(\n",
    "        driver = driver\n",
    "    )\n",
    "    # print(\"check res imgPath arr -> \",cur_card_img_path)\n",
    "    cur_attraction.set_imgPath(cur_card_img_path)\n",
    "\n",
    "    # find tag score using Gemini API\n",
    "\n",
    "\n",
    "    # create result data frame (dataframe represent data read from current card element)\n",
    "    res_dict = {\n",
    "        'name' : [cur_attraction.get_name()],\n",
    "        'description' : [cur_attraction.get_description()],\n",
    "        'latitude' : [cur_attraction.get_latitude()],\n",
    "        'longitude' : [cur_attraction.get_longitude()],\n",
    "        'imgPath' : [cur_attraction.get_imgPath()],\n",
    "        'phone': [cur_attraction.get_phone()],\n",
    "        'website': [cur_attraction.get_website()],\n",
    "        'openingHour': [cur_attraction.get_openingHour()],\n",
    "\n",
    "        # location\n",
    "        'address' : [cur_attraction.get_location().get_address()],\n",
    "        'province' : [cur_attraction.get_location().get_province()],\n",
    "        'district' : [cur_attraction.get_location().get_district()],\n",
    "        'subDistrict' : [cur_attraction.get_location().get_subDistrict()],\n",
    "        'ISO_3166_code' : [cur_attraction.get_location().get_ISO_3166_code()],\n",
    "        'zip_code' : [cur_attraction.get_location().get_zip_code()],\n",
    "        'geo_code' : [cur_attraction.get_location().get_geo_code()],\n",
    "\n",
    "        # rating\n",
    "        'score' : [cur_attraction.get_rating().get_score()],\n",
    "        'ratingCount' : [cur_attraction.get_rating().get_ratingCount()],\n",
    "    }\n",
    "\n",
    "    # append attraction tags score in 'dict'\n",
    "    for cur_tag in ATTRACTION_TAG_SCORE:\n",
    "        res_dict[cur_tag] = cur_attraction.get_attractionTag().get_tag_score(cur_tag)\n",
    "\n",
    "    # create result dataframe\n",
    "    res_df = pd.DataFrame(res_dict)\n",
    "    \n",
    "    return res_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory res_attraction_scraping created successfully\n",
      "cur_province -> Nakhon Ratchasima, cur_distrinct -> Ban Lueam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22292\\3144250210.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cur_geo_code_by_province_df.drop(columns=['province_th', 'province_en'], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check listCard 1\n",
      "8\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"9dc1ab8ed57293cdf24874fede61a507\", element=\"f.0E24FEBE7987E689127F99F83B037DEB.d.892DB91F8462C76CFC491F66BCD986AC.e.37\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"9dc1ab8ed57293cdf24874fede61a507\", element=\"f.0E24FEBE7987E689127F99F83B037DEB.d.892DB91F8462C76CFC491F66BCD986AC.e.38\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"9dc1ab8ed57293cdf24874fede61a507\", element=\"f.0E24FEBE7987E689127F99F83B037DEB.d.892DB91F8462C76CFC491F66BCD986AC.e.39\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"9dc1ab8ed57293cdf24874fede61a507\", element=\"f.0E24FEBE7987E689127F99F83B037DEB.d.892DB91F8462C76CFC491F66BCD986AC.e.40\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"9dc1ab8ed57293cdf24874fede61a507\", element=\"f.0E24FEBE7987E689127F99F83B037DEB.d.892DB91F8462C76CFC491F66BCD986AC.e.41\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"9dc1ab8ed57293cdf24874fede61a507\", element=\"f.0E24FEBE7987E689127F99F83B037DEB.d.892DB91F8462C76CFC491F66BCD986AC.e.42\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"9dc1ab8ed57293cdf24874fede61a507\", element=\"f.0E24FEBE7987E689127F99F83B037DEB.d.892DB91F8462C76CFC491F66BCD986AC.e.43\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"9dc1ab8ed57293cdf24874fede61a507\", element=\"f.0E24FEBE7987E689127F99F83B037DEB.d.892DB91F8462C76CFC491F66BCD986AC.e.44\")>]\n",
      "**************************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**************************\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Simulates infinite scrolling on a Google Maps search page to retrieve all card elements in the next step\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m list_card \u001b[38;5;241m=\u001b[39m \u001b[43mGoogleMapsInfiniteScroller\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheck listCard 2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnnbm. -> \u001b[39m\u001b[38;5;124m\"\u001b[39m,driver\u001b[38;5;241m.\u001b[39mcurrent_url)\n",
      "Cell \u001b[1;32mIn[24], line 18\u001b[0m, in \u001b[0;36mGoogleMapsInfiniteScroller\u001b[1;34m(driver)\u001b[0m\n\u001b[0;32m     16\u001b[0m scroll_origin \u001b[38;5;241m=\u001b[39m ScrollOrigin\u001b[38;5;241m.\u001b[39mfrom_element(wrapper_div[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     17\u001b[0m action\u001b[38;5;241m.\u001b[39mscroll_from_origin(scroll_origin, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5000\u001b[39m)\u001b[38;5;241m.\u001b[39mperform()        \n\u001b[1;32m---> 18\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSCROLL_PAUSE_TIME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# temp_list_card = driver.find_elements(By.CLASS_NAME, \"hfpxzc\")\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# ensure that there is no others card element(fetched from google every time scrolling) --> break the loop\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# check whether scorlling untill found section 'คุณมาถึงส่วนท้ายของรายการแล้ว' or 'You've reached the end of the list.'\u001b[39;00m\n\u001b[0;32m     22\u001b[0m reach_end_section_span \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHlvSq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "geo_code_df = pd.read_csv(fh.PATH_TO_GEOCODE)\n",
    "\n",
    "cnt_for_test = 1\n",
    "\n",
    "# create directory 'res_attraction_scraping'\n",
    "createDirectory(fh.STORE_ATTRACTION_SCRAPING, 'res_attraction_scraping')\n",
    "\n",
    "# create directory 'temp' to store temporary downloaded image to be used as a request to gemini api\n",
    "createDirectory(fh.STORE_ATTRACTION_SCRAPING, 'temp')\n",
    "\n",
    "cnt_test_province = 0\n",
    "for cur_province_th, cur_province_en in zip(ALL_PROVINCE_TH, ALL_PROVINCE_ENG):\n",
    "    # for testing workflow\n",
    "    if(cnt_test_province == 2):\n",
    "        break\n",
    "    cnt_test_province += 1\n",
    "    \n",
    "    cur_province_data_df = pd.DataFrame()\n",
    "\n",
    "    cur_geo_code_by_province_df = geo_code_df[geo_code_df['province_en'] == cur_province_en]\n",
    "    cur_geo_code_by_province_df.drop(columns=['province_th', 'province_en'], inplace=True)\n",
    "    cur_all_searchDistrict_th = np.unique(cur_geo_code_by_province_df.district_th.to_list(), axis=0)\n",
    "    cur_all_searchDistrict_en = np.unique(cur_geo_code_by_province_df.district_en.to_list(), axis=0)\n",
    "    \n",
    "    cnt_for_test_distirct = 0\n",
    "    for cur_search_district_th, cur_search_district_en in zip(cur_all_searchDistrict_th, cur_all_searchDistrict_en):\n",
    "        print(\"cur_province -> {0}, cur_distrinct -> {1}\".format(cur_province_en, cur_search_district_en))\n",
    "        # for testing workflow\n",
    "        if(cnt_for_test_distirct == 2):\n",
    "            break\n",
    "        cnt_for_test_distirct += 1        \n",
    "        \n",
    "        # สถาน ที่ ท่องเที่ยว near Mueang Nakhon Ratchasima District, Nakhon Ratchasima\n",
    "        cur_searchParam = format_searchParam(\n",
    "            searchParam=\"สถาน ที่ ท่องเที่ยว near {0} District, {1}\".format(cur_search_district_en, cur_province_en)\n",
    "        )\n",
    "        cur_query_url = \"https://www.google.com/maps/search/{0}\".format(cur_searchParam)\n",
    "        \n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        # create webdriver instance using option to mazimize current window\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(cur_query_url)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'hfpxzc')))\n",
    "        list_card = driver.find_elements(By.CLASS_NAME, \"hfpxzc\")\n",
    "        \n",
    "        print(\"check listCard 1\")\n",
    "        print(len(list_card))\n",
    "        print(list_card)\n",
    "        print(\"**************************\")\n",
    "\n",
    "        # Simulates infinite scrolling on a Google Maps search page to retrieve all card elements in the next step\n",
    "        list_card = GoogleMapsInfiniteScroller(\n",
    "            driver = driver,\n",
    "        )\n",
    "\n",
    "        print(\"check listCard 2\")\n",
    "        print(\"nnbm. -> \",driver.current_url)\n",
    "        print(len(list_card))\n",
    "        print(list_card)\n",
    "        print(\"**************************\")\n",
    "\n",
    "        # after google map have fetched all card elements\n",
    "        # read data from card elements\n",
    "        cur_district_df = pd.DataFrame()\n",
    "        for i in range(len(list_card)):\n",
    "            # print(\"cur Idx -> \", i)\n",
    "            # for debug puepose\n",
    "            if(i == 5):\n",
    "                break\n",
    "            # define attraction object\n",
    "            cur_attraction = Attraction()\n",
    "            cur_card_df = getDatafromGoogleMapCard(\n",
    "                driver = driver,\n",
    "                cur_card = list_card[i],\n",
    "                cur_attraction = cur_attraction,\n",
    "                cur_province_th = cur_province_th,\n",
    "                cur_geo_code_by_province_df = cur_geo_code_by_province_df.copy() \n",
    "            )\n",
    "            # concat all data frame of current distrcit to 'cur_province_data_df'\n",
    "            cur_district_df = pd.concat([cur_district_df, cur_card_df])\n",
    "\n",
    "        # concat all data frame of current province, distrcit to 'cur_province_data_df'\n",
    "        cur_province_data_df = pd.concat([cur_province_data_df, cur_district_df]) \n",
    "\n",
    "        # close current browser tab    \n",
    "        driver.close()  \n",
    "    \n",
    "    # remove duplicate attraction name of current province, distrcit, sub_district\n",
    "    cur_province_data_df.drop_duplicates(subset=['name', 'ISO_3166_code', 'zip_code', 'geo_code'], inplace=True)\n",
    "\n",
    "    # set new index\n",
    "    cur_province_data_df.set_index(['name', 'province', 'district', 'subDistrict'], inplace=True)\n",
    "\n",
    "    # save data to csv\n",
    "    # which migth be used for POST API save to database later...\n",
    "    res_file_name = 'res_attraction_{0}.csv'.format(cur_province_en)\n",
    "    res_path = os.path.join(fh.STORE_ATTRACTION_SCRAPING, 'res_attraction_scraping', res_file_name) \n",
    "    cur_province_data_df.to_csv(res_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
