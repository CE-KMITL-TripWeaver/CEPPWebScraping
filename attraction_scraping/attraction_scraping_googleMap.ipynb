{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import constants.constants as const\n",
    "import constants.file_handler_constants as fh\n",
    "from constants.attraction_constants import *\n",
    "\n",
    "from packages.attraction.Attraction import *\n",
    "from packages.file_handler_package.file_handler import *\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.actions.wheel_input import ScrollOrigin\n",
    "from selenium.webdriver import ActionChains\n",
    "# from selenium import webdriver\n",
    "from selenium.webdriver.chrome.webdriver import WebDriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "\n",
    "from dotenv import load_dotenv, dotenv_values \n",
    "import json\n",
    "import requests\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import ContentType\n",
    "from PIL import Image\n",
    "from IPython.display import Markdown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Formats a search parameter string for Google Maps by replacing spaces with '+'.\n",
    "\n",
    "Args:\n",
    "    searchParam: The search parameter string to format.\n",
    "\n",
    "Returns:\n",
    "    The formatted search parameter string with spaces replaced by '+'.\n",
    "\"\"\"\n",
    "def format_searchParam(searchParam) -> str:\n",
    "    return searchParam.replace(\" \", \"+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Simulates infinite scrolling on a Google Maps search page to retrieve all card elements (appeared in DOM)\n",
    "\n",
    "Args:\n",
    "    driver: Selenium WebDriver instance representing the open Google Chrome browser window\n",
    "\n",
    "Returns:\n",
    "    List of card element\n",
    "'''\n",
    "def GoogleMapsInfiniteScroller(driver:WebDriver) -> list[WebDriver]:\n",
    "    action = ActionChains(driver)\n",
    "    isFetchAllCard = False\n",
    "    wrapper_div = driver.find_elements(By.XPATH, '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]')\n",
    "    temp_list_card = []\n",
    "    while not isFetchAllCard:\n",
    "        scroll_origin = ScrollOrigin.from_element(wrapper_div[0])\n",
    "        action.scroll_from_origin(scroll_origin, 0, 5000).perform()        \n",
    "        time.sleep(const.SCROLL_PAUSE_TIME)\n",
    "        # temp_list_card = driver.find_elements(By.CLASS_NAME, \"hfpxzc\")\n",
    "        # ensure that there is no others card element(fetched from google every time scrolling) --> break the loop\n",
    "        # check whether scorlling untill found section 'คุณมาถึงส่วนท้ายของรายการแล้ว' or 'You've reached the end of the list.'\n",
    "        reach_end_section_span = driver.find_elements(By.CLASS_NAME, \"HlvSq\")\n",
    "        if(len(reach_end_section_span)):\n",
    "            for cur_span in reach_end_section_span:\n",
    "                if(cur_span.text == \"คุณมาถึงส่วนท้ายของรายการแล้ว\" or cur_span.text == \"You've reached the end of the list.\"):\n",
    "                    isFetchAllCard = True\n",
    "                    temp_list_card = driver.find_elements(By.CLASS_NAME, \"hfpxzc\")\n",
    "                    break\n",
    "    \n",
    "    return temp_list_card.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Simulates infinite scrolling on a Google Maps Images page to retrieve all card elements (appeared in DOM)\n",
    "\n",
    "Args:\n",
    "    driver: Selenium WebDriver instance representing the open Google Chrome browser window.\n",
    "    max_len_img: Maximum number of images to collect.\n",
    "\n",
    "Returns:\n",
    "    List of image URLs.\n",
    "'''\n",
    "def findImageInfiniteScroller(driver:WebDriver, max_len_img:int) -> list[str]:\n",
    "    res_imgPath = []\n",
    "    cnt_card = 0\n",
    "    \n",
    "    try:\n",
    "        while(True):\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"play\"]/div/button[2]')))\n",
    "            cur_right_arrow = driver.find_elements(By.XPATH, '//*[@id=\"play\"]/div/button[2]')[0]\n",
    "            cur_disabled_status = cur_right_arrow.get_attribute('disabled')\n",
    "            cnt_card += 1\n",
    "            if(cnt_card >= max_len_img or cur_disabled_status == 'true'):\n",
    "                break\n",
    "            cur_right_arrow.click()\n",
    "        \n",
    "        time.sleep(3)\n",
    "        \n",
    "        # extract image from css property of all element in 'all_list_card'\n",
    "        all_list_card = driver.find_elements(By.CLASS_NAME, 'Uf0tqf')[:max_len_img+1]\n",
    "        for cur_card in all_list_card:\n",
    "            cur_bgImg_val = cur_card.value_of_css_property('background-image')\n",
    "            match = re.search(r'url\\(\"(.*?)\"\\)', cur_bgImg_val)\n",
    "            if match:\n",
    "                res_imgPath.append(match.group(1))\n",
    "    \n",
    "    except Exception as e:\n",
    "        return ['']\n",
    "            \n",
    "    return res_imgPath.copy() if len(res_imgPath) > 0 else ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gets all images(url) of a given attraction\n",
    "\n",
    "Args:\n",
    "    driver: Selenium WebDriver instance representing the open Google Chrome browser window.\n",
    "\n",
    "Returns:\n",
    "    List of image URLs.\n",
    "'''\n",
    "def findImgPathCurCard(driver:webdriver) -> list[str]:\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"start-maximized\")\n",
    "\n",
    "    # get the new URL\n",
    "    new_cur_card_url = driver.current_url\n",
    "    # print('new_cur_card_url -> ',new_cur_card_url)\n",
    "    # create a new WebDriver instance\n",
    "    new_cur_card_driver = webdriver.Chrome(options=options)\n",
    "    new_cur_card_driver.get(new_cur_card_url)\n",
    "    WebDriverWait(new_cur_card_driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'ZKCDEc')))\n",
    "    # WebDriverWait(new_cur_card_driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'aoRNLd kn2E5e NMjTrf lvtCsd')))\n",
    "    soup = BeautifulSoup(new_cur_card_driver.page_source, 'html.parser')\n",
    "\n",
    "    # find length of all reviewed images for current attraction(rendered at hovered thumbnail as '26 รูป', '32,667 รูป')\n",
    "    len_all_img = 0\n",
    "    find_img_len = soup.find_all('div', class_='YkuOqf')\n",
    "    if(len(find_img_len)):\n",
    "        len_all_img = int(find_img_len[0].text.replace(' รูป','').replace(',',''))\n",
    "    # in some case there is no reviewed images --> exit process(can not continue to read images)\n",
    "    else:\n",
    "        return ['']\n",
    "    # print('check len_all_img -> ', len_all_img)\n",
    "\n",
    "    # click at thumnail image button if there's some image of current attraction\n",
    "    try:\n",
    "        btn_click_thumbnail_temp_1 = new_cur_card_driver.find_elements(By.XPATH, '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[3]/div/div[1]/div/div/div[2]/div[1]/div[1]/button')\n",
    "        btn_click_thumbnail_temp_2 = new_cur_card_driver.find_elements(By.XPATH, '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/button')\n",
    "        btn_click_thumbnail = btn_click_thumbnail_temp_1.copy() if len(btn_click_thumbnail_temp_1) > 0 else btn_click_thumbnail_temp_2.copy()\n",
    "        btn_click_thumbnail[0].click()\n",
    "    \n",
    "    except Exception as e:\n",
    "        return ['']\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------\n",
    "    \n",
    "    # process of reading at most 20 image\n",
    "    # wait for page to load (change url from 'current card url' to 'website with all reviewed images')\n",
    "    WebDriverWait(new_cur_card_driver, 10).until(EC.url_changes(new_cur_card_driver.current_url))\n",
    "\n",
    "    # get the new URL\n",
    "    new_readImg_url = new_cur_card_driver.current_url\n",
    "    # print('new_readImg_url -> ', new_readImg_url)\n",
    "    # create a new WebDriver instance\n",
    "    new_readImg_driver = webdriver.Chrome(options=options)\n",
    "    new_readImg_driver.get(new_readImg_url)\n",
    "    try:\n",
    "        WebDriverWait(new_readImg_driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]')))\n",
    "        WebDriverWait(new_readImg_driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'Uf0tqf')))\n",
    "        WebDriverWait(new_readImg_driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"play\"]/div/button[2]'))) \n",
    "        \n",
    "    except Exception as e:\n",
    "        return ['']\n",
    "    \n",
    "    # read the first fetched image\n",
    "    # then read at most XX images\n",
    "    max_len_img = min(20, len_all_img)\n",
    "    res_imgPath = findImageInfiniteScroller(\n",
    "        driver = new_readImg_driver,\n",
    "        max_len_img = max_len_img\n",
    "    )\n",
    "   \n",
    "    new_readImg_driver.close()\n",
    "    time.sleep(1)\n",
    "    new_cur_card_driver.close()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    return res_imgPath.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gets tag scores for a given attraction using the Gemini API.\n",
    "\n",
    "Args:\n",
    "    name: Name of the attraction.\n",
    "    latitude: Latitude of the attraction.\n",
    "    longitude: Longitude of the attraction.\n",
    "    all_img_url: List of image URLs of the attraction.\n",
    "\n",
    "Returns:\n",
    "    Dictionary representing scores for all tags.\n",
    "\"\"\"\n",
    "def getScorefromGeminiAPI(name:str, latitude:float, longitude:float, all_img_url:list[str]) -> dict:\n",
    "\n",
    "    # create directory 'temp' to store temporary downloaded images to be used as a request to gemini api\n",
    "    createDirectory(fh.STORE_ATTRACTION_SCRAPING, 'temp')\n",
    "\n",
    "    for Idx, cur_url in enumerate(all_img_url):\n",
    "        if(cur_url == ''):\n",
    "            break\n",
    "        response = requests.get(cur_url)\n",
    "        if response.status_code == 200:\n",
    "            filename = 'temp/temp_img_{0}.jpeg'.format(Idx)\n",
    "            with open(filename, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "\n",
    "    ## send api to get score for current attraction (send with query and main image)\n",
    "    genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "    model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "    text_prompt = \"Provide place name, latitude, and longitude. I will return a JSON string containing scores (0-1) for following attributes(nothing else no other sentences)\" + \\\n",
    "    \"\\nfor example: \\'{\\\"Tourism\\\":0,\\\"Adventure\\\":0,\\\"Meditation\\\":0,\\\"Art\\\":0,\\\"Cultural\\\":0,\\\"Landscape\\\":0,\\\"Nature\\\":0,\\\"Historical\\\":0,\\\"Cityscape\\\":0,\\\"Beach\\\":0,\\\"Mountain\\\":0,\\\"Architecture\\\":0,\\\"Temple\\\":0,\\\"WalkingStreet\\\":0,\\\"Market\\\":0,\\\"Village\\\":0,\\\"NationalPark\\\":0,\\\"Diving\\\":0,\\\"Snuggle\\\":0,\\\"Waterfall\\\":0,\\\"Island\\\":0,\\\"Shopping\\\":0,\\\"Camping\\\":0,\\\"Fog\\\":0,\\\"Cycling\\\":0,\\\"Monument\\\":0,\\\"Zoo\\\":0,\\\"Waterpark\\\":0,\\\"Hiking\\\":0,\\\"Museum\\\":0,\\\"Riverside\\\":0,\\\"NightLife\\\":0,\\\"Family\\\":0,\\\"Kid\\\":0,\\\"Landmark\\\":0,\\\"Forest\\\":0}\" + \\\n",
    "    \"\\n{0}, {1}, {2} give me score for this\".format(name, latitude, longitude)\n",
    "\n",
    "    # prompt the model\n",
    "    prompt = [text_prompt]\n",
    "    for Idx, cur_path_img in enumerate(glob.glob(os.path.join(fh.STORE_ATTRACTION_SCRAPING, 'temp', '*.jpeg'))):\n",
    "        # prompt with at most 3 images to reduce token usage\n",
    "        if(Idx == 3):\n",
    "            break\n",
    "        cur_img_prompt = Image.open(cur_path_img)\n",
    "        prompt.append(cur_img_prompt)\n",
    "    print(\"total_tokens: \", model.count_tokens(prompt))\n",
    "    \n",
    "    res_score_dict = {}\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        # remove directory 'temp'\n",
    "        removeNoneEmptyDir(os.path.join(fh.STORE_ATTRACTION_SCRAPING, 'temp'))\n",
    "        res_start_Idx = response.text.find('{')\n",
    "        res_end_Idx = response.text.find('}')\n",
    "        res_score_dict =  json.loads(response.text[res_start_Idx:res_end_Idx+1])\n",
    "\n",
    "    except Exception as e:\n",
    "        # remove directory 'temp'\n",
    "        removeNoneEmptyDir(os.path.join(fh.STORE_ATTRACTION_SCRAPING, 'temp'))\n",
    "        print(\"failed to use gemini api\")\n",
    "    \n",
    "    return res_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracts data from a given attraction card on Google Maps.\n",
    "\n",
    "Args:\n",
    "    driver: Selenium WebDriver instance representing the open Google Chrome browser window.\n",
    "    cur_card: The current card element.\n",
    "    cur_attraction: The current attraction object.\n",
    "    cur_province_th: The current province name in Thai language.\n",
    "    cur_geo_code_by_province_df: Dataframe containing geocodes for the given province.\n",
    "\n",
    "Returns:\n",
    "    Dataframe containing the scraped data from the given card.\n",
    "\"\"\"\n",
    "def getDatafromGoogleMapCard(driver:webdriver, cur_card:WebElement, cur_attraction:Attraction, cur_province_th:str, cur_district_th:str, cur_geo_code_by_province_df:pd.DataFrame) -> pd.DataFrame:\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    action = ActionChains(driver)\n",
    "\n",
    "    patternPhone = re.compile(r'\\d{3} \\d{3} \\d{4}')\n",
    "    patternPhoneService = re.compile(r'\\d{3} \\d{3} \\d{3}')\n",
    "\n",
    "    subStrDistrict = \"อำเภอ\"\n",
    "    subStrSubDistrict = \"ตำบล\"\n",
    "\n",
    "    if cur_province_th == \"กรุงเทพมหานคร\":\n",
    "        subStrDistrict = \"เขต\"\n",
    "        subStrSubDistrict = \"แขวง\"\n",
    "\n",
    "    try:\n",
    "        action.move_to_element(cur_card).perform()\n",
    "        cur_card.click()\n",
    "        \n",
    "    except Exception as e:\n",
    "        # print('can not click card element')\n",
    "        # print('when scrape for province -> {0}'.format(cur_province_th))\n",
    "        return\n",
    "\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    name = soup.find_all('h1', class_='DUwDvf lfPIob')\n",
    "    description = soup.find_all('div', class_='PYvSYb')\n",
    "    address = soup.find_all('div', class_='Io6YTe')\n",
    "    # loc = soup.find_all('div', class_='rogA2c')\n",
    "\n",
    "    time.sleep(5)\n",
    "    start_index_lat = driver.current_url.find(\"!3d\") + 3\n",
    "    end_index_lat = driver.current_url.find(\"!4d\")\n",
    "    lat = driver.current_url[start_index_lat:end_index_lat]\n",
    "    start_index_long = driver.current_url.find(\"!4d\") + 3\n",
    "    end_index_long = driver.current_url.find(\"!\", driver.current_url.find(\"!4d\") + 1)\n",
    "    long = driver.current_url[start_index_long:end_index_long]\n",
    "    \n",
    "    # found some wiered place that doesn't even have its address\n",
    "    # skip this case for now...\n",
    "    if(not len(address)):\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    district = 0\n",
    "    subDirstrict = 0\n",
    "\n",
    "    # find attraction name\n",
    "    print('name: ',name[0].text)\n",
    "    cur_attraction.set_name(name[0].text)\n",
    "\n",
    "    # find lat, long\n",
    "    print(\"lat, long: \" + lat + \" \" + long)\n",
    "    cur_attraction.set_latitude(float(lat))\n",
    "    cur_attraction.set_longitude(float(long))\n",
    "\n",
    "\n",
    "    # find description\n",
    "    if(len(description)):\n",
    "        # print(\"Description :\",description[0].text)\n",
    "        cur_attraction.set_description(description[0].text)\n",
    "    \n",
    "\n",
    "    # find location\n",
    "    useData = None\n",
    "    for div in address:\n",
    "        if cur_province_th in div.text and div.text.find(subStrDistrict) != -1:\n",
    "            useData = div.text.replace(\",\",\"\").replace(\"เเ\",\"แ\")\n",
    "    \n",
    "    if(useData != None):\n",
    "        # print(\"Full Address :\",useData)\n",
    "        # another brute force way in case of province 'กรุงเทพหมานคร' not have word 'แขวง' in address\n",
    "        if(cur_province_th == 'กรุงเทพมหานคร' and useData.find(subStrSubDistrict) == -1):\n",
    "            subAddress_split = useData.split(' ')\n",
    "            cur_province_Idx = subAddress_split.index(cur_province_th)\n",
    "            district = subAddress_split[cur_province_Idx - 1].replace(\"เขต\",\"\")\n",
    "            subDistrict = subAddress_split[cur_province_Idx - 2].replace(\"แขวง\",\"\")\n",
    "\n",
    "        else:\n",
    "            start_address_index = useData.find(subStrSubDistrict)\n",
    "            subAddress = useData[start_address_index:]\n",
    "            district = subAddress[subAddress.find(subStrDistrict)+len(subStrDistrict):subAddress.find(cur_province_th)].replace(\" \",\"\")               \n",
    "            subDistrict = subAddress[subAddress.find(subStrSubDistrict)+len(subStrSubDistrict):subAddress.find(subStrDistrict)].replace(\" \",\"\")\n",
    "\n",
    "        if district == \"เมือง\":\n",
    "            district = district+cur_province_th\n",
    "\n",
    "        # skip if 'district' not matched with 'cur_district_th'\n",
    "        # if(district != cur_district_th):\n",
    "        #     return\n",
    "\n",
    "        # filter row to find 'ISO_3166_code', 'zip_code', 'geo_code'\n",
    "        filtered_rows = cur_geo_code_by_province_df[\n",
    "            (cur_geo_code_by_province_df['district_th'] == district) & (cur_geo_code_by_province_df['subDistrict_th'] == subDistrict)\n",
    "        ]\n",
    "        filtered_rows.reset_index(inplace=True, drop=True)\n",
    "        if not filtered_rows.empty:\n",
    "            # print(\"province :\",filtered_rows.loc[0, 'ISO_3166_code'],cur_province_th)\n",
    "            # print(\"District :\",filtered_rows.loc[0, 'zip_code'],district)\n",
    "            # print(\"SubDistrict :\",filtered_rows.loc[0, 'geo_code'],subDistrict)\n",
    "\n",
    "            cur_attraction.set_location(\n",
    "                address = useData,\n",
    "                province = cur_province_th,\n",
    "                district = district,\n",
    "                sub_district = subDistrict,\n",
    "                province_code = filtered_rows.loc[0, 'ISO_3166_code'],\n",
    "                district_code = filtered_rows.loc[0, 'zip_code'],\n",
    "                sub_district_code = filtered_rows.loc[0, 'geo_code']\n",
    "            )\n",
    "        else:\n",
    "            # print(\"province :\",cur_province_th)\n",
    "            # print(\"District :\",district)\n",
    "            # print(\"SubDistrict :\",subDistrict)\n",
    "\n",
    "            cur_attraction.set_location(\n",
    "                address = useData,\n",
    "                province = cur_province_th,\n",
    "                district = district,\n",
    "                sub_district = subDistrict,\n",
    "                province_code = 0,\n",
    "                district_code = 0,\n",
    "                sub_district_code = 0\n",
    "            )\n",
    "\n",
    "    # find rating\n",
    "    score_div = soup.find_all('span', class_='ceNzKf')\n",
    "    if len(score_div):\n",
    "        score = score_div[0].get('aria-label').replace(\" \",\"\").replace(\"ดาว\",\"\").replace(\",\", \"\")\n",
    "        review_count_div = driver.find_elements(By.XPATH, '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[3]/div/div[1]/div/div/div[2]/div[2]/div/div[1]/div[2]/div/div[1]/div[2]/span[2]/span/span')\n",
    "        review_count = review_count_div[0].text.replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \"\")\n",
    "        \n",
    "        # print(\"Rating score :\",score)\n",
    "        # print(\"Rating Count:\", review_count)\n",
    "      \n",
    "        cur_attraction.set_rating(\n",
    "            score = float(score),\n",
    "            rating_count = int(review_count)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    # find ticket rate\n",
    "    ticketRating = soup.find_all('div', class_='drwWxc')\n",
    "    if(len(ticketRating)):\n",
    "        # print(\"Ticket Price :\",ticketRating[0].text)\n",
    "        pass\n",
    "\n",
    "\n",
    "    # find phone number\n",
    "    divContact = soup.find_all('div', class_='Io6YTe fontBodyMedium kR99db')\n",
    "    for div in divContact:\n",
    "        if(re.match(patternPhone,div.text) or re.match(patternPhoneService,div.text)):\n",
    "            # print(\"Contact :\",div.text)\n",
    "            cur_attraction.set_phone(div.text)\n",
    "\n",
    "\n",
    "    # find website\n",
    "    divWebsite = soup.find_all('a', class_='CsEnBe')\n",
    "    if(len(divWebsite)):\n",
    "        for i in range(len(divWebsite)):\n",
    "            # check if it fit below case\n",
    "            website_href = divWebsite[i].get('href')\n",
    "            if(website_href):\n",
    "                cur_attraction.set_website(website_href)\n",
    "                break\n",
    "\n",
    "\n",
    "    # find opening hours\n",
    "    openingHourCheck = soup.find_all(\"span\", class_=\"HlvSq\")\n",
    "\n",
    "    if(len(openingHourCheck) and openingHourCheck[0].text == \"ดูเวลาทำการเพิ่มเติม\"):\n",
    "        infoOpening = driver.find_elements(By.CLASS_NAME, \"HlvSq\")\n",
    "        for element in infoOpening:\n",
    "            element.click()\n",
    "        wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'y0skZc')))\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    openingTime = soup.find_all(\"tr\", class_=\"y0skZc\")\n",
    "\n",
    "    '''\n",
    "    update 28/7/2024: found this case in place 'พิพิธภัณฑ์ศิลปะไทยร่วมสมัย', 'วิหารเทพวิทยาคม'\n",
    "    \n",
    "    วันอาทิตย์ (วันเฉลิมพระชนมพรรษา พระบาทสมเด็จพระปรเมนทรรามาธิบดีศรีสินทรมหาวชิราลงกรณ พระวชิรเกล้าเจ้าอยู่หัว)                10:00–18:00เวลาทำการในวันหยุด\n",
    "    วันจันทร์ (วันเฉลิมพระชนมพรรษา พระบาทสมเด็จพระปรเมนทรรามาธิบดีศรีสินทรมหาวชิราลงกรณ พระวชิรเกล้าเจ้าอยู่หัว (วันหยุดชดเชย))    ปิดทำการเวลาทำการในวันหยุด\n",
    "    วันอังคาร      10:00–18:00\n",
    "    วันพุธ        10:00–18:00\n",
    "    วันพฤหัสบดี    10:00–18:00\n",
    "    วันศุกร์       10:00–18:00\n",
    "    วันเสาร์         10:00–18:00\n",
    "    '''\n",
    "\n",
    "    cur_openingHour = {}\n",
    "    for data in openingTime:\n",
    "        dateDiv = data.find(\"td\", class_=\"ylH6lf\")\n",
    "        timeDiv = data.find(\"td\", class_=\"mxowUb\")\n",
    "        # print(dateDiv.text,timeDiv.text)\n",
    "        cur_openingHour[dateDiv.text] = timeDiv.text\n",
    "        \n",
    "    cur_attraction.set_openingHour(cur_openingHour)\n",
    "\n",
    "\n",
    "    # find img path\n",
    "    cur_card_img_path = findImgPathCurCard(\n",
    "        driver = driver\n",
    "    )\n",
    "    # print(\"check res imgPath arr -> \",cur_card_img_path)\n",
    "    cur_attraction.set_imgPath(cur_card_img_path)\n",
    "\n",
    "    # find tag score using Gemini API\n",
    "    try:\n",
    "        all_tag_score = getScorefromGeminiAPI(\n",
    "            name = cur_attraction.get_name(),\n",
    "            latitude = cur_attraction.get_latitude(),\n",
    "            longitude = cur_attraction.get_longitude(),\n",
    "            all_img_url = cur_attraction.get_imgPath()\n",
    "        )\n",
    "        # set tag score to Attraction object\n",
    "        for cur_tag in all_tag_score.keys():\n",
    "            cur_attraction.set_tag_score(key=cur_tag, val=all_tag_score[cur_tag])\n",
    "            \n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    # create result data frame (dataframe represent data read from current card element)\n",
    "    res_dict = {\n",
    "        'name' : [cur_attraction.get_name()],\n",
    "        'description' : [cur_attraction.get_description()],\n",
    "        'latitude' : [cur_attraction.get_latitude()],\n",
    "        'longitude' : [cur_attraction.get_longitude()],\n",
    "        'imgPath' : [cur_attraction.get_imgPath()],\n",
    "        'phone': [cur_attraction.get_phone()],\n",
    "        'website': [cur_attraction.get_website()],\n",
    "        'openingHour': [cur_attraction.get_openingHour()],\n",
    "\n",
    "        # location\n",
    "        'address' : [cur_attraction.get_location().get_address()],\n",
    "        'province' : [cur_attraction.get_location().get_province()],\n",
    "        'district' : [cur_attraction.get_location().get_district()],\n",
    "        'subDistrict' : [cur_attraction.get_location().get_subDistrict()],\n",
    "        'province_code' : [cur_attraction.get_location().get_province_code()],\n",
    "        'district_code' : [cur_attraction.get_location().get_district_code()],\n",
    "        'sub_district_code' : [cur_attraction.get_location().get_sub_district_code()],\n",
    "\n",
    "        # rating\n",
    "        'score' : [cur_attraction.get_rating().get_score()],\n",
    "        'ratingCount' : [cur_attraction.get_rating().get_ratingCount()],\n",
    "    }\n",
    "\n",
    "    # append attraction tags score in 'dict'\n",
    "    for cur_tag in ATTRACTION_TAG_SCORE:\n",
    "        res_dict[cur_tag] = cur_attraction.get_attractionTag().get_tag_score(cur_tag)\n",
    "\n",
    "    # create result dataframe\n",
    "    res_df = pd.DataFrame(res_dict)\n",
    "    \n",
    "    return res_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory res_attraction_scraping created successfully\n",
      "cur_province -> Phuket, cur_distrinct -> Kathu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_32860\\2987183462.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cur_geo_code_by_province_df.drop(columns=['province_th', 'province_en'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# loading variables from .env file\n",
    "#load_dotenv() \n",
    "\n",
    "geo_code_df = pd.read_csv(fh.PATH_TO_GEOCODE)\n",
    "\n",
    "cnt_for_test = 1\n",
    "\n",
    "# create directory 'res_attraction_scraping'\n",
    "createDirectory(fh.STORE_ATTRACTION_SCRAPING, 'res_attraction_scraping')\n",
    "\n",
    "cnt_test_province = 0\n",
    "for cur_province_th, cur_province_en in zip(ALL_PROVINCE_TH, ALL_PROVINCE_ENG):\n",
    "    # for testing workflow\n",
    "    #if(cnt_test_province == 1):\n",
    "    #    break\n",
    "    #cnt_test_province += 1\n",
    "    \n",
    "    cur_province_data_df = pd.DataFrame()\n",
    "\n",
    "    cur_geo_code_by_province_df = geo_code_df[geo_code_df['province_en'] == cur_province_en]\n",
    "    cur_geo_code_by_province_df.drop(columns=['province_th', 'province_en'], inplace=True)\n",
    "    cur_all_searchDistrict_th = np.unique(cur_geo_code_by_province_df.district_th.to_list(), axis=0)\n",
    "    cur_all_searchDistrict_en = np.unique(cur_geo_code_by_province_df.district_en.to_list(), axis=0)\n",
    "    \n",
    "    cnt_for_test_distirct = 0\n",
    "    for cur_search_district_th, cur_search_district_en in zip(cur_all_searchDistrict_th, cur_all_searchDistrict_en):\n",
    "        print(\"cur_province -> {0}, cur_distrinct -> {1}\".format(cur_province_en, cur_search_district_en))\n",
    "        # for testing workflow\n",
    "        #if(cnt_for_test_distirct == 1):\n",
    "        #    break\n",
    "        #cnt_for_test_distirct += 1        \n",
    "        \n",
    "        # สถาน ที่ ท่องเที่ยว near Mueang Nakhon Ratchasima District, Nakhon Ratchasima\n",
    "        cur_searchParam = format_searchParam(\n",
    "            searchParam=\"สถาน ที่ ท่องเที่ยว near %s District, %s\" % (cur_search_district_en, cur_province_en)\n",
    "        )\n",
    "        cur_query_url = \"https://www.google.com/maps/search/%s\" % (cur_searchParam)\n",
    "        \n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        # create webdriver instance using option to mazimize current window\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(cur_query_url)\n",
    "        time.sleep(4444)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'hfpxzc')))\n",
    "        list_card = driver.find_elements(By.CLASS_NAME, \"hfpxzc\")\n",
    "    \n",
    "        print(\"check listCard 1\")\n",
    "        print(len(list_card))\n",
    "        print(list_card)\n",
    "        print(\"**************************\")\n",
    "\n",
    "        # Simulates infinite scrolling on a Google Maps search page to retrieve all card elements in the next step\n",
    "        list_card = GoogleMapsInfiniteScroller(\n",
    "            driver = driver,\n",
    "        )\n",
    "\n",
    "        print(\"check listCard 2\")\n",
    "        print(\"nnbm. -> \",driver.current_url)\n",
    "        print(len(list_card))\n",
    "        print(list_card)\n",
    "        print(\"**************************\")\n",
    "\n",
    "        # after google map have fetched all card elements\n",
    "        # read data from card elements\n",
    "        cur_district_df = pd.DataFrame()\n",
    "        for i in range(len(list_card)):\n",
    "            # print(\"cur Idx -> \", i)\n",
    "            # for debug puepose\n",
    "            # if(i == 5):\n",
    "            #     break\n",
    "            # define attraction object\n",
    "            cur_attraction = Attraction()\n",
    "            cur_card_df = getDatafromGoogleMapCard(\n",
    "                driver = driver,\n",
    "                cur_card = list_card[i],\n",
    "                cur_attraction = cur_attraction,\n",
    "                cur_province_th = cur_province_th,\n",
    "                cur_district_th=cur_search_district_th,\n",
    "                cur_geo_code_by_province_df = cur_geo_code_by_province_df.copy() \n",
    "            )\n",
    "            # concat all data frame of current distrcit to 'cur_province_data_df'\n",
    "            cur_district_df = pd.concat([cur_district_df, cur_card_df])\n",
    "\n",
    "        # concat all data frame of current province, distrcit to 'cur_province_data_df'\n",
    "        cur_province_data_df = pd.concat([cur_province_data_df, cur_district_df]) \n",
    "\n",
    "        # close current browser tab    \n",
    "        driver.close()  \n",
    "    \n",
    "    # remove duplicate attraction name of current province, distrcit, sub_district\n",
    "    cur_province_data_df.drop_duplicates(subset=['name', 'province_code', 'district_code', 'sub_district_code'], inplace=True)\n",
    "\n",
    "    # set new index\n",
    "    cur_province_data_df.set_index(['name', 'province', 'district', 'subDistrict'], inplace=True)\n",
    "\n",
    "    # save data to csv\n",
    "    # which migth be used for POST API save to database later...\n",
    "    res_file_name = 'res_attraction_{0}.csv'.format(cur_province_en)\n",
    "    res_path = os.path.join(fh.STORE_ATTRACTION_SCRAPING, 'res_attraction_scraping', res_file_name) \n",
    "    cur_province_data_df.to_csv(res_path, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
