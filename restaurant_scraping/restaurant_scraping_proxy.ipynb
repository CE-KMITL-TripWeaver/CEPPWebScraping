{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import constants.constants as const\n",
    "import constants.file_handler_constants as fh\n",
    "from constants.restaurant_constants import *\n",
    "\n",
    "from packages.restaurant.Restaurant import *\n",
    "from packages.file_handler_package.file_handler import *\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from selenium.webdriver import Remote, ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.actions.wheel_input import ScrollOrigin\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from seleniumwire import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from selenium.webdriver.edge.options import Options\n",
    "\n",
    "\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_restaurant_df(restaurant: Restaurant) -> pd.DataFrame:\n",
    "    restaurant_dict = {\n",
    "        'name' : [restaurant.get_name()],\n",
    "        'description' : [restaurant.get_description()],\n",
    "        'latitude' : [restaurant.get_latitude()],\n",
    "        'longitude' : [restaurant.get_longitude()],\n",
    "        'imgPath' : [restaurant.get_imgPath()],\n",
    "        'phone': [restaurant.get_phone()],\n",
    "        'website': [restaurant.get_website()],\n",
    "        'facility': [restaurant.get_facility()],\n",
    "        'type': [restaurant.get_type()],\n",
    "\n",
    "        # location\n",
    "        'address' : [restaurant.get_location().get_address()],\n",
    "        'province' : [restaurant.get_location().get_province()],\n",
    "        'district' : [restaurant.get_location().get_district()],\n",
    "        'subDistrict' : [restaurant.get_location().get_sub_district()],\n",
    "        'province_code' : [restaurant.get_location().get_province_code()],\n",
    "        'district_code' : [restaurant.get_location().get_district_code()],\n",
    "        'sub_district_code' : [restaurant.get_location().get_sub_district_code()],\n",
    "\n",
    "        # rating\n",
    "        'score' : [restaurant.get_rating().get_score()],\n",
    "        'ratingCount' : [restaurant.get_rating().get_ratingCount()],\n",
    "    }\n",
    "\n",
    "    restaurant_df = pd.DataFrame(restaurant_dict)\n",
    "    \n",
    "    return restaurant_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_url_by_page(link_to_restaurant: str, page: int) -> str:\n",
    "\n",
    "    # if(page == 1):\n",
    "    #     return link_to_restaurant\n",
    "    \n",
    "    first_page_url_split = link_to_restaurant.split('-')\n",
    "    nth_count_page = 'oa%s' % ((page - 1) * 30)\n",
    "    first_page_url_split[-1] = nth_count_page\n",
    "    res_page_url =  \"-\".join(first_page_url_split)\n",
    "\n",
    "    return res_page_url\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_img(restaurant_page_driver: webdriver) -> list[str]:\n",
    "    \n",
    "    res_imgPath = []\n",
    "\n",
    "    # find button and click\n",
    "    # to see image modal\n",
    "    try:\n",
    "        print(\"p2\")\n",
    "        # click_img_btn = restaurant_page_driver.find_element(By.CLASS_NAME, 'QXsnf')\n",
    "        WebDriverWait(restaurant_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'GuzzA')))\n",
    "        click_img_btn = restaurant_page_driver.find_element(By.CLASS_NAME, 'GuzzA')\n",
    "        print(\"p3\")\n",
    "        \n",
    "        # Move to the element and click\n",
    "        actions = ActionChains(restaurant_page_driver)\n",
    "        actions.move_to_element(click_img_btn).click().perform()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"can't open modal image\")\n",
    "        return res_imgPath\n",
    "\n",
    "    # scrape image address\n",
    "    try:\n",
    "        is_end_scrape_img = False\n",
    "        cnt_retry = 0\n",
    "        print(\"p7\")\n",
    "        while(not is_end_scrape_img):\n",
    "            if(cnt_retry == 20):\n",
    "                print(\"max retry for scrape image...\")\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                WebDriverWait(restaurant_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'cfCAA')))\n",
    "                all_img_elements = restaurant_page_driver.find_elements(By.CLASS_NAME, 'cfCAA')\n",
    "                print(\"find image element -> \", len(all_img_elements))\n",
    "                for cur_img_element in all_img_elements:\n",
    "                    cur_bgImg_val = cur_img_element.value_of_css_property('background-image')\n",
    "                    match = re.search(r'url\\(\"(.*?)\"\\)', cur_bgImg_val)\n",
    "                    if match:\n",
    "                        res_imgPath.append(match.group(1))\n",
    "\n",
    "                is_end_scrape_img = True\n",
    "\n",
    "            except Exception as e:\n",
    "                cnt_retry += 1\n",
    "                print(\"retry scrape img...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "            pass\n",
    "    \n",
    "\n",
    "    return res_imgPath.copy()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_location(restaurant_page_driver: webdriver, latitude: float, longitude: float, province_th: str) -> Location:\n",
    "\n",
    "    # find better address description on wongnai\n",
    "    # for example: \"991 ถนนพระราม 1 Pathum Wan, กรุงเทพมหานคร (กทม.) 10330 ไทย\"\n",
    "    address_tripAdvisor = \"\"\n",
    "    possible_address_xpath = [\n",
    "        '//*[@id=\"lithium-root\"]/main/div/div/div[2]/div[3]/div/div[3]/span[1]/span/div/span',\n",
    "    ]\n",
    "\n",
    "\n",
    "    for cur_address_xpath in possible_address_xpath:\n",
    "        try:\n",
    "            WebDriverWait(restaurant_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, cur_address_xpath)))\n",
    "            address_element = restaurant_page_driver.find_element(By.XPATH, cur_address_xpath)\n",
    "            address_tripAdvisor = address_element.text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"can't find address_tripAdvisor\")\n",
    "\n",
    "\n",
    "    # start scrape location\n",
    "    res_location = Location()\n",
    "    cnt_retry = 0\n",
    "    try:\n",
    "        while(True):\n",
    "            if(cnt_retry == 10):\n",
    "                print(\"max retry for scrape Google Map ...\")\n",
    "                break\n",
    "            \n",
    "            # set up new webdriver to work googlemap url(query for specific lat/long)\n",
    "            possible_addressGoogleMap_elements = []\n",
    "            try:\n",
    "                # set Chrome options to run in headless mode\n",
    "                # options = Options()\n",
    "                options = webdriver.ChromeOptions()\n",
    "                options.add_argument(\"start-maximized\")\n",
    "                # options.add_argument(\"--headless=new\")\n",
    "                options.add_experimental_option(\n",
    "                    \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "                )\n",
    "\n",
    "                google_map_driver = webdriver.Chrome(options=options)\n",
    "                \n",
    "                google_map_query = \"https://www.google.com/maps/search/?api=1&query=%s,%s\" % (latitude, longitude)\n",
    "                google_map_driver.get(google_map_query)\n",
    "                print(\"scrape location data for, \", google_map_query)\n",
    "                \n",
    "                WebDriverWait(google_map_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'DkEaL')))\n",
    "                possible_addressGoogleMap_elements = google_map_driver.find_elements(By.CLASS_NAME, 'DkEaL')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"retry  scrape Google Map..\")\n",
    "                cnt_retry += 1\n",
    "                google_map_driver.close()\n",
    "                continue\n",
    "\n",
    "\n",
    "            # after init new webdriver -> continure scrape location data\n",
    "\n",
    "            # if found some wiered place that doesn't even have its address\n",
    "            # skip this case for now...\n",
    "            if(not len(possible_addressGoogleMap_elements)):\n",
    "                return res_location\n",
    "\n",
    "            subStrDistrict = \"อำเภอ\"\n",
    "            subStrSubDistrict = \"ตำบล\"\n",
    "\n",
    "            if province_th == \"กรุงเทพมหานคร\":\n",
    "                subStrDistrict = \"เขต\"\n",
    "                subStrSubDistrict = \"แขวง\"\n",
    "\n",
    "            district = 0\n",
    "            subDirstrict = 0\n",
    "\n",
    "            # find location\n",
    "            useData = None\n",
    "            for cur_element in possible_addressGoogleMap_elements:\n",
    "                if province_th in cur_element.text and cur_element.text.find(subStrDistrict) != -1:\n",
    "                    useData = cur_element.text.replace(\",\",\"\").replace(\"เเ\",\"แ\")\n",
    "                    break\n",
    "           \n",
    "            if(useData != None):\n",
    "                # print(\"Full Address :\",useData)\n",
    "                # another brute force way in case of province 'กรุงเทพหมานคร' not have word 'แขวง' in address\n",
    "                if(province_th == 'กรุงเทพมหานคร' and useData.find(subStrSubDistrict) == -1):\n",
    "                    subAddress_split = useData.split(' ')\n",
    "                    cur_province_Idx = subAddress_split.index(province_th)\n",
    "                    district = subAddress_split[cur_province_Idx - 1].replace(\"เขต\",\"\")\n",
    "\n",
    "                else:\n",
    "                    start_address_index = useData.find(subStrDistrict)\n",
    "                    subAddress = useData[start_address_index:]\n",
    "                    district = subAddress[subAddress.find(subStrDistrict)+len(subStrDistrict):subAddress.find(province_th)].replace(\" \",\"\")               \n",
    "\n",
    "                if district == \"เมือง\":\n",
    "                    district = district+province_th\n",
    "\n",
    "                # filter row to find 'ISO_3166_code', 'zip_code', 'geo_code'\n",
    "                geo_code_df = pd.read_csv(fh.PATH_TO_GEOCODE)\n",
    "                filtered_rows = geo_code_df[\n",
    "                    (geo_code_df['province_th'] == province_th) & (geo_code_df['district_th'] == district)\n",
    "                ]\n",
    "                filtered_rows.reset_index(inplace=True, drop=True)\n",
    "                \n",
    "                if not filtered_rows.empty:\n",
    "                    print(\"found province :\",filtered_rows.loc[0, 'ISO_3166_code'], province_th)\n",
    "                    print(\"found District :\",filtered_rows.loc[0, 'zip_code'], district)\n",
    "\n",
    "                    res_location.set_address(address_tripAdvisor if len(address_tripAdvisor) else useData)\n",
    "                    res_location.set_province(province_th)\n",
    "                    res_location.set_district(district)\n",
    "                    res_location.set_sub_district(\"\")\n",
    "                    res_location.set_province_code(filtered_rows.loc[0, 'ISO_3166_code'])\n",
    "                    res_location.set_district_code(filtered_rows.loc[0, 'zip_code'])\n",
    "                    res_location.set_sub_district_code(0)\n",
    "\n",
    "                else:\n",
    "                    print(\"not found province :\", province_th)\n",
    "                    print(\"not found District :\", district)\n",
    "\n",
    "                    res_location.set_address(address_tripAdvisor if len(address_tripAdvisor) else useData)\n",
    "                    res_location.set_province(province_th)\n",
    "                    res_location.set_district(district)\n",
    "                    res_location.set_sub_district(\"\")\n",
    "                    res_location.set_province_code(0)\n",
    "                    res_location.set_district_code(0)\n",
    "                    res_location.set_sub_district_code(0)\n",
    "\n",
    "            google_map_driver.close()\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"can't scrape location data\")\n",
    "\n",
    "    return res_location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape lat/long, openingHours, types, facilities (there are in adjust page of current restaurant: 'https://th.tripadvisor.com/ImproveListing-d1792735.html')\n",
    "def scrape_adjust_page(restaurant_page_driver: webdriver, link_to_adjust_page: str) -> tuple[float, float, dict, list[str], list[str]]:\n",
    "    lat = 0\n",
    "    long = 0\n",
    "    openingHours = {}\n",
    "    types = []\n",
    "    facilities = []\n",
    "    \n",
    "    # create new webdriver to continue scrape lat/long, openingHours in adjust restaurant page\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape single restaurant ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", \n",
    "            {\n",
    "                \"profile.managed_default_content_settings.images\": 2, # Disable image\n",
    "                \"profile.default_content_setting_values.cookies\": 2,  # Block all cookies\n",
    "                \"profile.default_content_settings.popups\": 0,         # Disable popups\n",
    "                \"profile.managed_default_content_settings.cookies\": 2  # Disable third-party cookies\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        adjust_page_driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # retry in case of web restrictions, some elements not loaded\n",
    "        try:\n",
    "            print(\"scrape data in adjust restaurant page...\")\n",
    "            print(\"for link : \", link_to_adjust_page)\n",
    "            adjust_page_driver.get(link_to_adjust_page)\n",
    "\n",
    "            print(\"debug option of adjust page: \")\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'DiHOR')))\n",
    "\n",
    "            # find dropdown --> click display data below --> cick display lat/long input form\n",
    "            possible_target_btn = adjust_page_driver.find_elements(By.CLASS_NAME, 'DiHOR')\n",
    "            for cur_dropdown_btn in possible_target_btn:\n",
    "                cur_dropdown_text = cur_dropdown_btn.text\n",
    "                if(\"แนะนำการแก้ไขข้อมูลของสถานที่นี้\" in cur_dropdown_text):\n",
    "                    print(\"found target dropdown btn ...\")\n",
    "                    cur_dropdown_btn.click()\n",
    "                    WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/button')))\n",
    "                    # find button click to display lat/long input form\n",
    "                    display_lat_long_btn = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/button')\n",
    "                    display_lat_long_btn.click()\n",
    "\n",
    "        except Exception as e:\n",
    "            cnt_retry += 1\n",
    "            adjust_page_driver.quit()\n",
    "            print(\"retry adjust page...\")\n",
    "            continue\n",
    "\n",
    "      \n",
    "        # find lat/long\n",
    "        try:\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[1]/div/div[2]/div/div/div/span')))\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[2]/div/div[2]/div/div/div/span')))\n",
    "    \n",
    "            lat_input_container = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[1]/div/div[2]/div/div/div/span')\n",
    "            lat_input_element = lat_input_container.find_element(By.TAG_NAME, 'input')\n",
    "            lat = float(lat_input_element.get_attribute('value'))\n",
    "\n",
    "            long_input_container = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[2]/div/div[2]/div/div/div/span')\n",
    "            long_input_element = long_input_container.find_element(By.TAG_NAME, 'input')\n",
    "            long = float(long_input_element.get_attribute('value'))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find lat/long\")\n",
    "        \n",
    "        print(\"lat : \", lat)\n",
    "        print(\"long : \", long)\n",
    "\n",
    "        # **if can't find lat/long --> don't scrape this attaction\n",
    "        if(lat == 0 and long == 0):\n",
    "            print(\"in scrape_location_latlong_openingHours --> can't find lat/long --> 0, 0\")\n",
    "            return lat, long, openingHours.copy(), types.copy(), facilities.copy()\n",
    "\n",
    "        # find type\n",
    "        try:     \n",
    "            # find restaurant types container\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'IBVyx')))\n",
    "\n",
    "            possible_type_container = adjust_page_driver.find_elements(By.CLASS_NAME, 'IBVyx')\n",
    "            type_container = None\n",
    "            for cur_element in possible_type_container[::-1]:\n",
    "                cur_text = cur_element.text\n",
    "                if(\"หมวดหมู่อาหาร\" in cur_text):\n",
    "                    type_container = cur_element\n",
    "                    WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'RCAPL')))\n",
    "                    all_type_element = type_container.find_elements(By.CLASS_NAME, 'RCAPL')\n",
    "                    for cur_type in all_type_element:\n",
    "                        cur_text = cur_type.text\n",
    "                        types.append(cur_text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find types\")\n",
    "\n",
    "        print(\"types --> \", types)\n",
    "\n",
    "\n",
    "        # find facilities\n",
    "        try:\n",
    "            # find restaurant facilities container\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'AoddJ')))\n",
    "            \n",
    "            possible_facility_container = adjust_page_driver.find_elements(By.CLASS_NAME, 'AoddJ')\n",
    "            facility_container = None\n",
    "            for cur_element in possible_facility_container[::-1]:\n",
    "                cur_text = cur_element.text\n",
    "                print(cur_text)\n",
    "\n",
    "                if(\"สิ่งอำนวยความสะดวก\" in cur_text):\n",
    "                    facility_container = cur_day_element\n",
    "                    print(\"debug faci 1\")\n",
    "                    WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'PMWyE')))\n",
    "                    print(\"debug faci 2\")\n",
    "                    checkbox_containers = facility_container.find_elements(By.CLASS_NAME, 'PMWyE')\n",
    "                    print(\"debug faci 3\")\n",
    "                    for Idx in range(len(checkbox_containers)):\n",
    "                        cur_checkbox = checkbox_containers[Idx].find_element(By.TAG_NAME, 'span')\n",
    "                        is_check = True if cur_checkbox.get_attribute('class') != 'U' else False\n",
    "                        if(is_check):\n",
    "                            facilities.append(ALL_RESTAURANTS_TRIPADVISOR_FACILITIES[Idx])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find facilities\")\n",
    "\n",
    "        print(\"facilities --> \", facilities)\n",
    "\n",
    "\n",
    "        # find openingHours\n",
    "        try:\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'dNAjp')))\n",
    "            all_openingHours_container = adjust_page_driver.find_elements(By.CLASS_NAME, 'dNAjp')\n",
    "            for cur_openingHours_container in all_openingHours_container:\n",
    "                cur_day_element = cur_openingHours_container.find_element(By.CLASS_NAME, 'ngXxk')\n",
    "                cur_day_text = cur_day_element.text.replace(\":\", \"\")\n",
    "\n",
    "                cur_time_element = cur_openingHours_container.find_element(By.CLASS_NAME, 'KxBGd')\n",
    "                cur_time_text = cur_time_element.text\n",
    "\n",
    "                openingHours[cur_day_text] = cur_time_text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find openingHours ...\")\n",
    "\n",
    "        print(\"openingHours : \", openingHours)\n",
    "\n",
    "        adjust_page_driver.quit()\n",
    "        break\n",
    "\n",
    "    return lat, long, openingHours.copy(), types.copy(), facilities.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_restaurant(link_to_restaurant: str, province_th: str) -> Restaurant:\n",
    "    \n",
    "    restaurant = Restaurant()\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape single restaurant ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set web browser options to run\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", \n",
    "            {\n",
    "                \"profile.managed_default_content_settings.images\": 2, # Disable image\n",
    "                \"profile.default_content_setting_values.cookies\": 2,  # Block all cookies\n",
    "                \"profile.default_content_settings.popups\": 0,         # Disable popups\n",
    "                \"profile.managed_default_content_settings.cookies\": 2  # Disable third-party cookies\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # initialize the web driver with service, selenium-wire options, and web browser options\n",
    "        restaurant_page_driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            print(\"******************************************************\")\n",
    "            print(\"scrape single restaurant...\")\n",
    "            print(\"for restaurant : \", link_to_restaurant)\n",
    "            restaurant_page_driver.get(link_to_restaurant)\n",
    "            # restaurant_page_driver.add_cookie()\n",
    "\n",
    "            print(\"debug scrape_single_restaurant: top info component section\")\n",
    "            # WebDriverWait(restaurant_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/span/div[4]/div/div[1]/div[3]/div')))\n",
    "            # top_info_container = restaurant_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/span/div[4]/div/div[1]/div[3]/div')\n",
    "\n",
    "            print(\"debug scrape_single_restaurant: bottom info component section\")\n",
    "            WebDriverWait(restaurant_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div/div[6]/div/div[2]')))\n",
    "            bottom_info_container = restaurant_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div/div[6]/div/div[2]')\n",
    "\n",
    "            print(\"debug scrape_single_attraction: common component section\")\n",
    "            WebDriverWait(restaurant_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'IDaDx')))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"retry single restaurant case 1...\")\n",
    "            cnt_retry += 1\n",
    "            restaurant_page_driver.quit()\n",
    "            continue\n",
    "        \n",
    "    \n",
    "        # find name\n",
    "        name = \"\"\n",
    "        try:\n",
    "            WebDriverWait(restaurant_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'rRtyp')))\n",
    "            name_element = restaurant_page_driver.find_element(By.CLASS_NAME, 'rRtyp')\n",
    "            name = name_element.text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find name\")\n",
    "\n",
    "        print(\"name -> \", name)\n",
    "\n",
    "        # find description\n",
    "        # description = \"\"\n",
    "        # try:\n",
    "        #     try:\n",
    "        #         # find button to click readmore (if it exists, it likely to be the first elements of class 'lszDU')\n",
    "        #         WebDriverWait(restaurant_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'lszDU')))\n",
    "        #         click_readmore_btn = restaurant_page_driver.find_element(By.CLASS_NAME, 'lszDU')\n",
    "        #         click_readmore_btn.click()\n",
    "\n",
    "        #     except Exception as e:\n",
    "        #         pass\n",
    "\n",
    "        #     WebDriverWait(restaurant_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'zYHGB')))\n",
    "        #     all_description_elements = restaurant_page_driver.find_elements(By.CLASS_NAME, 'zYHGB')\n",
    "        #     for cur_element in all_description_elements:\n",
    "        #         cur_text =  cur_element.text\n",
    "        #         if(len(cur_text)):\n",
    "        #             description += cur_text + '\\n'\n",
    "            \n",
    "        # except Exception as e:\n",
    "        #     print(\"can't find description\")\n",
    "\n",
    "        # print(\"description -> \", description)\n",
    "        \n",
    "        # find phone\n",
    "        phone = \"\"\n",
    "        try:\n",
    "            WebDriverWait(restaurant_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div/div[3]/div/div[3]/span[2]/span[2]/a')))\n",
    "            phone_element = restaurant_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div/div[3]/div/div[3]/span[2]/span[2]/a')\n",
    "            phone_element_href = phone_element.get_attribute('href')\n",
    "            if(\"tel\" in phone_element_href):\n",
    "                phone = phone_element.text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find phone\")\n",
    "\n",
    "        print(\"phone --> \", phone)\n",
    "\n",
    "        # find rating\n",
    "        rating = 0\n",
    "        rating_count = 0\n",
    "        try:\n",
    "            WebDriverWait(restaurant_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'sOyfn')))\n",
    "            rating_container = restaurant_page_driver.find_element(By.CLASS_NAME, 'sOyfn')\n",
    "            \n",
    "            rating_element = rating_container.find_element(By.CLASS_NAME, 'uuBRH')\n",
    "            rating = float(rating_element.text)\n",
    "\n",
    "            rating_count_element = rating_container.find_element(By.CLASS_NAME, 'oXJmt')\n",
    "            rating_count = int(rating_count_element.text.replace(',', '').replace('รีวิว ', '').replace(' รายการ', ''))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find rating and rating_count\")\n",
    "\n",
    "        print(\"rating --> \", rating)\n",
    "        print(\"rating_count --> \", rating_count)\n",
    "\n",
    "\n",
    "        # find img_path\n",
    "        # img_path = scrape_img(restaurant_page_driver)\n",
    "        # print(\"cur img path -> \", img_path)\n",
    "\n",
    "\n",
    "        # convert restaurant url to adjust page url\n",
    "        # for example: from 'https://th.tripadvisor.com/Restaurant_Review-g1210687-d1792735-Reviews-Kwong_Shop_Seafood-Kata_Beach_Karon_Phuket.html' to 'https://th.tripadvisor.com/ImproveListing-d586602.html'\n",
    "        link_to_adjust_page = 'https://th.tripadvisor.com/ImproveListing-%s.html' % (link_to_restaurant.split('-')[2])\n",
    "\n",
    "        # ** find lat/long, location data and openingHours (there are in another page of current restaurant)\n",
    "        # ** if this restaurant not have lat/long\n",
    "        # ** don't continue to scrape\n",
    "        lat, long, openingHours, types, facilities = scrape_adjust_page(\n",
    "            restaurant_page_driver = restaurant_page_driver,\n",
    "            link_to_adjust_page = link_to_adjust_page\n",
    "        )\n",
    "        \n",
    "        # **if can't find lat/long --> don't scrape this attaction\n",
    "        if(lat == 0 and long == 0):\n",
    "            print(\"in scrape_single_restaurant --> can't find lat/long --> don't scrape this restaurant ...\")\n",
    "            restaurant_page_driver.quit()\n",
    "            return Restaurant()\n",
    "        \n",
    "\n",
    "        # find location\n",
    "        location = scrape_location(\n",
    "            restaurant_page_driver = restaurant_page_driver,\n",
    "            latitude = lat,\n",
    "            longitude = long,\n",
    "            province_th = province_th\n",
    "        )\n",
    "        print(\"province :\", location.get_province_code(), location.get_province())\n",
    "        print(\"District :\", location.get_district_code(), location.get_district())\n",
    "        print(\"Address : \", location.get_address())\n",
    "\n",
    "\n",
    "        # set some of \"restaurant\" object properties\n",
    "        restaurant.set_name(name)\n",
    "        # restaurant.set_description(description)\n",
    "        restaurant.set_phone(phone)\n",
    "        restaurant.set_latitude(lat)\n",
    "        restaurant.set_longitude(long)\n",
    "        # restaurant.set_imgPath(img_path)\n",
    "        restaurant.set_website(link_to_restaurant)\n",
    "        restaurant.set_openingHour(openingHours)\n",
    "        restaurant.set_type(types)\n",
    "        restaurant.set_facility(facilities)\n",
    "        # restaurant.set_priceRange(priceRange)\n",
    "        restaurant.set_location(\n",
    "            address = location.get_address(),\n",
    "            province = location.get_province(),\n",
    "            district = location.get_district(),\n",
    "            sub_district = location.get_sub_district(),\n",
    "            province_code = location.get_province_code(),\n",
    "            district_code = location.get_district_code(),\n",
    "            sub_district_code = location.get_sub_district_code()\n",
    "        )\n",
    "        restaurant.set_rating(\n",
    "            score = rating,\n",
    "            rating_count = rating_count\n",
    "        )\n",
    "\n",
    "        restaurant_page_driver.quit()\n",
    "        break\n",
    "\n",
    "    return restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_url_by_page(query_url: str, page: int) -> list[str]:\n",
    "\n",
    "    res_url_by_page = []\n",
    "\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        if(cnt_retry == 10):\n",
    "            print(\"max retry for scrape data by page ...\")\n",
    "            break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        # os.environ['proxy_port']\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "      \n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # just check for ip\n",
    "        # print(\"just check for ip :\")\n",
    "        # driver.get(\"https://httpbin.io/ip\")\n",
    "        # print(driver.page_source)\n",
    "\n",
    "        # find group of restaurant on the nth page\n",
    "        all_restaurants_card = []\n",
    "\n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            query_url_by_page = convert_url_by_page(\n",
    "                link_to_restaurant = query_url,\n",
    "                page = page\n",
    "            )\n",
    "            driver.get(query_url_by_page)\n",
    "            # scroll and wait for some msec\n",
    "            driver.execute_script('window.scrollBy(0, document.body.scrollHeight)')\n",
    "            \n",
    "            print(\"check current page url --> \", driver.current_url)\n",
    "\n",
    "            # wait for div (each restaurant section) to be present and visible\n",
    "            print(\"b1 part 1\")\n",
    "            print(\"debug get_all_url_by_page: restaurant by one page section\")\n",
    "            WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'yJIls')))\n",
    "            all_restaurants_card = driver.find_elements(By.CLASS_NAME, 'yJIls')\n",
    "\n",
    "\n",
    "            # check if all accomodation card can get tag a and its attribute for url\n",
    "            print(\"b2\")\n",
    "            print(\"check in loop ...\")\n",
    "            for cur_restaurant_card in all_restaurants_card:\n",
    "\n",
    "                cur_restaurant_url = cur_restaurant_card.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                print(\"cur_restaurant_url : \", cur_restaurant_url)\n",
    "                res_url_by_page.append(cur_restaurant_url)\n",
    "            \n",
    "            driver.quit()\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"retry find get_all_url_by_page ...\")\n",
    "            cnt_retry += 1\n",
    "            driver.quit()\n",
    "            continue\n",
    "\n",
    "    return res_url_by_page.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_restaurants_by_province(page: int, province_url: str, province: str) -> pd.DataFrame:\n",
    "    # res_restaurant_df = pd.DataFrame()\n",
    "    res_restaurant_df = create_restaurant_df(Restaurant())\n",
    "    \n",
    "    cnt_for_debug = 0\n",
    "        \n",
    "    print(\"scraping restaurant | province --> %s | page --> %s\" % (province, page))\n",
    "\n",
    "    all_url_by_page = get_all_url_by_page(query_url = province_url, page = page)\n",
    "\n",
    "    # use data from 'res_get_data_by_page' to retrive data of specific restaurant\n",
    "    for cur_restaurant_url in all_url_by_page:\n",
    "        # just use to limit amount of place --> will be removed \n",
    "        if(cnt_for_debug == 2):\n",
    "            break\n",
    "\n",
    "        # continue scraping data for a specific resgtaurant\n",
    "        cur_restaurant = scrape_single_restaurant(\n",
    "            link_to_restaurant = cur_restaurant_url,\n",
    "            province_th = province\n",
    "        )\n",
    "\n",
    "        cnt_for_debug += 1\n",
    "\n",
    "        # create data frame represent data scrape from current restaurant card\n",
    "        # cur_restaurant_df = create_restaurant_df(restaurant=cur_restaurant)\n",
    "\n",
    "        # concat all data frame result\n",
    "        # res_restaurant_df = pd.concat([res_restaurant_df, cur_restaurant_df])\n",
    "    \n",
    "    return res_restaurant_df.iloc[1:, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping restaurant | province --> ภูเก็ต | page --> 1\n",
      "check current page url -->  https://th.tripadvisor.com/Restaurants-g1215781-oa0-Phuket_Town_Phuket.html\n",
      "b1 part 1\n",
      "debug get_all_url_by_page: restaurant by one page section\n",
      "b2\n",
      "check in loop ...\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1389361-d19622658-Reviews-The_Distillery_Phuket_Home_of_Chalong_Bay-Chalong_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d15059216-Reviews-Three_Monkeys_Restaurant-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315818-d21238792-Reviews-Day_Night_of_Phuket-Talat_Yai_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d21173913-Reviews-Baikingu_Japanese_Buffet_Garden_Restaurant-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d1759741-Reviews-Blue_Elephant_Cooking_School_Restaurant_Phuket-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315818-d5818564-Reviews-One_Chun_Cafe_and_Restaurant-Talat_Yai_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d9722296-Reviews-Surf_and_Turf_by_Soul_Kitchen-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1389361-d1874082-Reviews-Kan_Eang_Pier-Chalong_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d13349831-Reviews-Tantitium_Restaurant-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d23839140-Reviews-Flamingo_Phuket-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315817-d12852621-Reviews-Piset_Restaurant_Phuket-Talat_Nuea_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d21138123-Reviews-Hong_Bao_Ramada_Phuket-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315818-d1172082-Reviews-Tukabkhao_Phuket_Local_Food_Restaurant-Talat_Yai_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1389361-d19622658-Reviews-The_Distillery_Phuket_Home_of_Chalong_Bay-Chalong_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315818-d13307440-Reviews-Amore_Mexican_Tapas_Bar-Talat_Yai_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d13989518-Reviews-Raya_Restaurant_Phuket_Old-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d26679753-Reviews-Beerfest_Restaurant_And_Craft_Beer_Brewery_Phuket-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d3168082-Reviews-Kopitiam_by_Wilai-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d23682010-Reviews-AKOYA_Star_Lounge-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d27469669-Reviews-Numnum_Cafe_Restaurant_Phuket_Old_Town-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1389361-d2714713-Reviews-Pizzeria_Agli_Amici_da_Michele_Jimmy-Chalong_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1389361-d7267941-Reviews-Baan_Noy_Restaurant-Chalong_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d23708006-Reviews-Sriwara_Bistro_Cafe-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d11810229-Reviews-The_Tent-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d23680636-Reviews-Yon_Ocean_House-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1389361-d25403279-Reviews-Lucha_Cantina_Phuket-Chalong_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d6155935-Reviews-Flavor_Phuket-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d1090092-Reviews-La_Gaetana-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d2218722-Reviews-Tunk_Ka_Cafe-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1893005-d3148567-Reviews-Laem_Hin_Seafood-Koh_Kaew_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d6697901-Reviews-Bollywood_Phuket_Restaurant_Bar-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d7107051-Reviews-Bang_Mud_Sea_Food_Phuket_floating_restaurant-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g297930-d25432456-Reviews-Atrio_Restaurant-Patong_Kathu_Phuket.html\n",
      "******************************************************\n",
      "scrape single restaurant...\n",
      "for restaurant :  https://th.tripadvisor.com/Restaurant_Review-g1389361-d19622658-Reviews-The_Distillery_Phuket_Home_of_Chalong_Bay-Chalong_Phuket_Town_Phuket.html\n",
      "debug scrape_single_restaurant: top info component section\n",
      "debug scrape_single_restaurant: bottom info component section\n",
      "debug scrape_single_attraction: common component section\n",
      "name ->  The Distillery Phuket - Home of Chalong Bay\n",
      "phone -->  +66 93 583 7322\n",
      "rating -->  5.0\n",
      "rating_count -->  35\n",
      "scrape data in adjust restaurant page...\n",
      "for link :  https://th.tripadvisor.com/ImproveListing-d19622658.html\n",
      "debug option of adjust page: \n",
      "found target dropdown btn ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1:56742: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\net\\tcp.py\", line 115, in read\n",
      "    data = self.o.read(rlen)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\server.py\", line 113, in handle\n",
      "    root_layer()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\modes\\http_proxy.py\", line 23, in __call__\n",
      "    layer()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\protocol\\tls.py\", line 285, in __call__\n",
      "    layer()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\protocol\\http1.py\", line 100, in __call__\n",
      "    layer()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\protocol\\http.py\", line 206, in __call__\n",
      "    if not self._process_flow(flow):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\protocol\\http.py\", line 288, in _process_flow\n",
      "    return self.handle_upstream_connect(f)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\protocol\\http.py\", line 247, in handle_upstream_connect\n",
      "    f.response = self.read_response_headers()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\protocol\\http1.py\", line 48, in read_response_headers\n",
      "    return http1.read_response_head(self.server_conn.rfile)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\net\\http\\http1\\read.py\", line 90, in read_response_head\n",
      "    http_version, status_code, message = _read_response_line(rfile)\n",
      "                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\net\\http\\http1\\read.py\", line 283, in _read_response_line\n",
      "    line = _get_first_line(rfile)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\net\\http\\http1\\read.py\", line 228, in _get_first_line\n",
      "    line = rfile.readline()\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\net\\tcp.py\", line 157, in readline\n",
      "    ch = self.read(1)\n",
      "         ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\net\\tcp.py\", line 133, in read\n",
      "    raise exceptions.TcpTimeout()\n",
      "seleniumwire.thirdparty.mitmproxy.exceptions.TcpTimeout: None\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat :  7.841498\n",
      "long :  98.35237\n",
      "ราคา: ร้านอาหารแห่งนี้ราคาแพงเท่าใด\n",
      "(ไม่บังคับ)\n",
      "ร้านนี้ให้บริการมื้ออาหารใดบ้าง\n",
      "(ไม่บังคับ)\n",
      "หมวดหมู่อาหาร (ไม่เกิน 5)\n",
      "(ไม่บังคับ)\n",
      "แก้ไขประเภทอาหาร\n",
      "เพิ่มเวลาเปิดทำการ\n",
      "(ไม่บังคับ)\n",
      "จันทร์:\n",
      "11:00-23:00\n",
      "อังคาร:\n",
      "11:00-23:00\n",
      "พุธ:\n",
      "11:00-23:00\n",
      "พฤหัสบดี:\n",
      "11:00-23:00\n",
      "ศุกร์:\n",
      "11:00-23:00\n",
      "เสาร์:\n",
      "11:00-23:00\n",
      "อาทิตย์:\n",
      "11:00-23:00\n",
      "แก้ไขเวลาทำการ\n",
      "types -->  ['ยุโรป', 'เอเชีย', 'ไทย']\n",
      "ร้านอาหารแห่งนี้มีคุณลักษณะและบริการและสิ่งอำนวยความสะดวกอะไรบ้าง\n",
      "(ไม่บังคับ)\n",
      "can't find facilities\n",
      "facilities -->  []\n",
      "openingHours :  {'จันทร์': '11:00-23:00', 'อังคาร': '11:00-23:00', 'พุธ': '11:00-23:00', 'พฤหัสบดี': '11:00-23:00', 'ศุกร์': '11:00-23:00', 'เสาร์': '11:00-23:00', 'อาทิตย์': '11:00-23:00'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored when trying to send to the signal wakeup fd:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3598, in run_code\n",
      "    result.error_in_exec = sys.exc_info()[1]\n",
      "                           ^^^^^^^^^^^^^^\n",
      "OSError: [WinError 10038] An operation was attempted on something that is not a socket\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\socket.py:836\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[0;32m    835\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m--> 836\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[0;32m    837\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 19\u001b[0m\n\u001b[0;32m     14\u001b[0m cur_province_url \u001b[38;5;241m=\u001b[39m cur_region_data[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# cur_res_allRestaurants_df = create_restaurant_df(Restaurant())\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# get dataframe result of all restaurant in current province\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m cur_res_allRestaurants_df \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_restaurants_by_province\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovince_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcur_province_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovince\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcur_province_th\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# don't forget to remove row with lat/long be zero\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# remove duplicate restaurant\u001b[39;00m\n\u001b[0;32m     28\u001b[0m cur_res_allRestaurants_df\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[61], line 18\u001b[0m, in \u001b[0;36mscrape_restaurants_by_province\u001b[1;34m(page, province_url, province)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# continue scraping data for a specific resgtaurant\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m cur_restaurant \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_single_restaurant\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlink_to_restaurant\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcur_restaurant_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovince_th\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprovince\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m cnt_for_debug \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# create data frame represent data scrape from current restaurant card\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# cur_restaurant_df = create_restaurant_df(restaurant=cur_restaurant)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# concat all data frame result\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# res_restaurant_df = pd.concat([res_restaurant_df, cur_restaurant_df])\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[59], line 151\u001b[0m, in \u001b[0;36mscrape_single_restaurant\u001b[1;34m(link_to_restaurant, province_th)\u001b[0m\n\u001b[0;32m    146\u001b[0m link_to_adjust_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://th.tripadvisor.com/ImproveListing-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (link_to_restaurant\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# ** find lat/long, location data and openingHours (there are in another page of current restaurant)\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# ** if this restaurant not have lat/long\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# ** don't continue to scrape\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m lat, long, openingHours, types, facilities \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_adjust_page\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrestaurant_page_driver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrestaurant_page_driver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlink_to_adjust_page\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlink_to_adjust_page\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# **if can't find lat/long --> don't scrape this attaction\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(lat \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m long \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "Cell \u001b[1;32mIn[58], line 167\u001b[0m, in \u001b[0;36mscrape_adjust_page\u001b[1;34m(restaurant_page_driver, link_to_adjust_page)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find openingHours ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopeningHours : \u001b[39m\u001b[38;5;124m\"\u001b[39m, openingHours)\n\u001b[1;32m--> 167\u001b[0m     \u001b[43madjust_page_driver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lat, long, openingHours\u001b[38;5;241m.\u001b[39mcopy(), types\u001b[38;5;241m.\u001b[39mcopy(), facilities\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\webdriver.py:68\u001b[0m, in \u001b[0;36mDriverCommonMixin.quit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Shutdown Selenium Wire and then quit the webdriver.\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mshutdown()\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:193\u001b[0m, in \u001b[0;36mChromiumDriver.quit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py:152\u001b[0m, in \u001b[0;36mService.stop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 152\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_remote_shutdown_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py:137\u001b[0m, in \u001b[0;36mService.send_remote_shutdown_command\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_connectable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     sleep(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py:126\u001b[0m, in \u001b[0;36mService.is_connectable\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_connectable\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Establishes a socket connection to determine if the service running\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03m    on the port is accessible.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_connectable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\common\\utils.py:101\u001b[0m, in \u001b[0;36mis_connectable\u001b[1;34m(port, host)\u001b[0m\n\u001b[0;32m     99\u001b[0m socket_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 101\u001b[0m     socket_ \u001b[38;5;241m=\u001b[39m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _is_connectable_exceptions:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\socket.py:843\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m error \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    842\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_errors:\n\u001b[1;32m--> 843\u001b[0m         exceptions\u001b[38;5;241m.\u001b[39mclear()  \u001b[38;5;66;03m# raise only the last error\u001b[39;00m\n\u001b[0;32m    844\u001b[0m     exceptions\u001b[38;5;241m.\u001b[39mappend(exc)\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1:56787: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\net\\tcp.py\", line 115, in read\n",
      "    data = self.o.read(rlen)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\server.py\", line 113, in handle\n",
      "    root_layer()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\modes\\http_proxy.py\", line 23, in __call__\n",
      "    layer()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\protocol\\tls.py\", line 285, in __call__\n",
      "    layer()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\protocol\\http1.py\", line 100, in __call__\n",
      "    layer()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\protocol\\http.py\", line 206, in __call__\n",
      "    if not self._process_flow(flow):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\protocol\\http.py\", line 288, in _process_flow\n",
      "    return self.handle_upstream_connect(f)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\protocol\\http.py\", line 247, in handle_upstream_connect\n",
      "    f.response = self.read_response_headers()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\protocol\\http1.py\", line 48, in read_response_headers\n",
      "    return http1.read_response_head(self.server_conn.rfile)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\net\\http\\http1\\read.py\", line 90, in read_response_head\n",
      "    http_version, status_code, message = _read_response_line(rfile)\n",
      "                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\net\\http\\http1\\read.py\", line 283, in _read_response_line\n",
      "    line = _get_first_line(rfile)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\net\\http\\http1\\read.py\", line 228, in _get_first_line\n",
      "    line = rfile.readline()\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\net\\tcp.py\", line 157, in readline\n",
      "    ch = self.read(1)\n",
      "         ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\net\\tcp.py\", line 133, in read\n",
      "    raise exceptions.TcpTimeout()\n",
      "seleniumwire.thirdparty.mitmproxy.exceptions.TcpTimeout: None\n",
      "\n",
      "127.0.0.1:56781: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\net\\tcp.py\", line 115, in read\n",
      "    data = self.o.read(rlen)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\server.py\", line 113, in handle\n",
      "    root_layer()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\modes\\http_proxy.py\", line 23, in __call__\n",
      "    layer()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\protocol\\tls.py\", line 285, in __call__\n",
      "    layer()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\protocol\\http1.py\", line 100, in __call__\n",
      "    layer()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\protocol\\http.py\", line 206, in __call__\n",
      "    if not self._process_flow(flow):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\protocol\\http.py\", line 288, in _process_flow\n",
      "    return self.handle_upstream_connect(f)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\protocol\\http.py\", line 247, in handle_upstream_connect\n",
      "    f.response = self.read_response_headers()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\server\\protocol\\http1.py\", line 48, in read_response_headers\n",
      "    return http1.read_response_head(self.server_conn.rfile)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\net\\http\\http1\\read.py\", line 90, in read_response_head\n",
      "    http_version, status_code, message = _read_response_line(rfile)\n",
      "                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\net\\http\\http1\\read.py\", line 283, in _read_response_line\n",
      "    line = _get_first_line(rfile)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\net\\http\\http1\\read.py\", line 228, in _get_first_line\n",
      "    line = rfile.readline()\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\net\\tcp.py\", line 157, in readline\n",
      "    ch = self.read(1)\n",
      "         ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\seleniumwire\\thirdparty\\mitmproxy\\net\\tcp.py\", line 133, in read\n",
      "    raise exceptions.TcpTimeout()\n",
      "seleniumwire.thirdparty.mitmproxy.exceptions.TcpTimeout: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# *** select one province from 'ALL_RESTAURANTS_TRIPADVISOR_DATA'\n",
    "# *** so, change \"Idx_of_region\" everytime when scrape another province\n",
    "Idx_of_region = 0\n",
    "cur_region_data = ALL_RESTAURANTS_TRIPADVISOR_DATA[Idx_of_region]\n",
    "\n",
    "# select first and last page to scrape\n",
    "# but in this module will not have any effect (just some dummy number to use with file name)\n",
    "# will have effect on module \"mulProcess_restaurant_scraping_proxy\"\n",
    "first_page = 444\n",
    "last_page = 888\n",
    "\n",
    "cur_province_en = cur_region_data[0]\n",
    "cur_province_th = cur_region_data[1]\n",
    "cur_province_url = cur_region_data[2]\n",
    "\n",
    "# cur_res_allRestaurants_df = create_restaurant_df(Restaurant())\n",
    "\n",
    "# get dataframe result of all restaurant in current province\n",
    "cur_res_allRestaurants_df = scrape_restaurants_by_province(\n",
    "    page = 1,\n",
    "    province_url = cur_province_url,\n",
    "    province = cur_province_th\n",
    ")\n",
    "\n",
    "# don't forget to remove row with lat/long be zero\n",
    "\n",
    "# remove duplicate restaurant\n",
    "cur_res_allRestaurants_df.drop_duplicates(subset=['name'], inplace=True)\n",
    "# set new index\n",
    "cur_res_allRestaurants_df.set_index(['name'], inplace=True)\n",
    "\n",
    "# create directory to store result of scraping restaurant\n",
    "# for example: 'restaurant_scraping\\res_restaurant_scraping\\res_restaurant_Phuket'\n",
    "createDirectory(fh.STORE_ACCOMM_SCRAPING, os.path.join('res_restaurant_scraping', 'res_restaurant_%s' % (cur_province_en)))\n",
    "\n",
    "# save result dataframe to .csv\n",
    "# for example: 'res_restaurant_Phuket_page_1_44.csv'\n",
    "res_file_name = 'res_restaurant_%s_page_%s_%s.csv' % (cur_province_en, first_page, last_page)\n",
    "res_path = os.path.join(fh.STORE_ACCOMM_SCRAPING, 'res_restaurant_scraping', 'res_restaurant_%s' % (cur_province_en), res_file_name)\n",
    "cur_res_allRestaurants_df.to_csv(res_path, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://th.tripadvisor.com/Hotel_Review-g297930-d315568-Reviews-Phuket_Marriott_Resort_Spa_Merlin_Beach-Patong_Kathu_Phuket.html#/media/315568/?albumid=101&type=0&category=101\n",
    "# https://th.tripadvisor.com/Hotel_Review-g1210687-d1379794-Reviews-Chanalai_Romantica_Resort-Kata_Beach_Karon_Phuket.html#/media/1379794/?albumid=101&type=0&category=101\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
