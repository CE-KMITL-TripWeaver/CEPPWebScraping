{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pyautogui\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import constants.constants as const\n",
    "import constants.file_handler_constants as fh\n",
    "from constants.restaurant_constants import *\n",
    "\n",
    "from packages.restaurant.Restaurant import *\n",
    "from packages.file_handler_package.file_handler import *\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv, dotenv_values \n",
    "\n",
    "from selenium.webdriver import Remote, ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.actions.wheel_input import ScrollOrigin\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from seleniumwire import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from selenium.webdriver.edge.options import Options\n",
    "\n",
    "\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_restaurant_df(restaurant: Restaurant) -> pd.DataFrame:\n",
    "    restaurant_dict = {\n",
    "        'name' : [restaurant.get_name()],\n",
    "        'description' : [restaurant.get_description()],\n",
    "        'latitude' : [restaurant.get_latitude()],\n",
    "        'longitude' : [restaurant.get_longitude()],\n",
    "        'imgPath' : [restaurant.get_imgPath()],\n",
    "        'phone': [restaurant.get_phone()],\n",
    "        'website': [restaurant.get_website()],\n",
    "        'facility': [restaurant.get_facility()],\n",
    "        'type': [restaurant.get_type()],\n",
    "\n",
    "        # location\n",
    "        'address' : [restaurant.get_location().get_address()],\n",
    "        'province' : [restaurant.get_location().get_province()],\n",
    "        'district' : [restaurant.get_location().get_district()],\n",
    "        'subDistrict' : [restaurant.get_location().get_sub_district()],\n",
    "        'province_code' : [restaurant.get_location().get_province_code()],\n",
    "        'district_code' : [restaurant.get_location().get_district_code()],\n",
    "        'sub_district_code' : [restaurant.get_location().get_sub_district_code()],\n",
    "\n",
    "        # rating\n",
    "        'score' : [restaurant.get_rating().get_score()],\n",
    "        'ratingCount' : [restaurant.get_rating().get_ratingCount()],\n",
    "    }\n",
    "\n",
    "    restaurant_df = pd.DataFrame(restaurant_dict)\n",
    "    \n",
    "    return restaurant_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_url_by_page(link_to_restaurant: str, page: int) -> str:\n",
    "\n",
    "    # if(page == 1):\n",
    "    #     return link_to_restaurant\n",
    "    \n",
    "    first_page_url_split = link_to_restaurant.split('-')\n",
    "    nth_count_page = 'oa%s' % ((page - 1) * 30)\n",
    "    first_page_url_split[-1] = nth_count_page\n",
    "    res_page_url =  \"-\".join(first_page_url_split)\n",
    "\n",
    "    return res_page_url\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_img(restaurant_page_driver: webdriver) -> list[str]:\n",
    "    \n",
    "    res_imgPath = []\n",
    "\n",
    "    # find button and click\n",
    "    # to see image modal\n",
    "    try:\n",
    "        print(\"p2\")\n",
    "        # click_img_btn = restaurant_page_driver.find_element(By.CLASS_NAME, 'QXsnf')\n",
    "        WebDriverWait(restaurant_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'GuzzA')))\n",
    "        click_img_btn = restaurant_page_driver.find_element(By.CLASS_NAME, 'GuzzA')\n",
    "        print(\"p3\")\n",
    "        \n",
    "        # Move to the element and click\n",
    "        actions = ActionChains(restaurant_page_driver)\n",
    "        actions.move_to_element(click_img_btn).click().perform()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"can't open modal image\")\n",
    "        return res_imgPath\n",
    "\n",
    "    # scrape image address\n",
    "    try:\n",
    "        is_end_scrape_img = False\n",
    "        cnt_retry = 0\n",
    "        print(\"p7\")\n",
    "        while(not is_end_scrape_img):\n",
    "            if(cnt_retry == 20):\n",
    "                print(\"max retry for scrape image...\")\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                WebDriverWait(restaurant_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'cfCAA')))\n",
    "                all_img_elements = restaurant_page_driver.find_elements(By.CLASS_NAME, 'cfCAA')\n",
    "                print(\"find image element -> \", len(all_img_elements))\n",
    "                for cur_img_element in all_img_elements:\n",
    "                    cur_bgImg_val = cur_img_element.value_of_css_property('background-image')\n",
    "                    match = re.search(r'url\\(\"(.*?)\"\\)', cur_bgImg_val)\n",
    "                    if match:\n",
    "                        res_imgPath.append(match.group(1))\n",
    "\n",
    "                is_end_scrape_img = True\n",
    "\n",
    "            except Exception as e:\n",
    "                cnt_retry += 1\n",
    "                print(\"retry scrape img...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "            pass\n",
    "    \n",
    "\n",
    "    return res_imgPath.copy()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_location(restaurant_page_driver: webdriver, latitude: float, longitude: float, province_th: str) -> Location:\n",
    "\n",
    "    # find better address description on wongnai\n",
    "    # for example: \"991 ถนนพระราม 1 Pathum Wan, กรุงเทพมหานคร (กทม.) 10330 ไทย\"\n",
    "    address_tripAdvisor = \"\"\n",
    "    possible_address_xpath = [\n",
    "        '//*[@id=\"lithium-root\"]/main/span/div[4]/div/div[1]/div[3]/div/div[3]/div[1]/div[2]/span[2]/span',\n",
    "        '//*[@id=\"lithium-root\"]/main/span/div[4]/div/div[1]/div[3]/div/div[3]/div[1]/div[1]/span[2]/span',\n",
    "    ]\n",
    "\n",
    "\n",
    "    for cur_address_xpath in possible_address_xpath:\n",
    "        try:\n",
    "            WebDriverWait(restaurant_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, cur_address_xpath)))\n",
    "            address_element = restaurant_page_driver.find_element(By.XPATH, cur_address_xpath)\n",
    "            address_tripAdvisor = address_element.text\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "\n",
    "    # start scrape location\n",
    "    res_location = Location()\n",
    "    cnt_retry = 0\n",
    "    try:\n",
    "        while(True):\n",
    "            if(cnt_retry == 10):\n",
    "                print(\"max retry for scrape Google Map ...\")\n",
    "                break\n",
    "            \n",
    "            # set up new webdriver to work googlemap url(query for specific lat/long)\n",
    "            possible_addressGoogleMap_elements = []\n",
    "            try:\n",
    "                # set Chrome options to run in headless mode\n",
    "                # options = Options()\n",
    "                options = webdriver.ChromeOptions()\n",
    "                options.add_argument(\"start-maximized\")\n",
    "                # options.add_argument(\"--headless=new\")\n",
    "                options.add_experimental_option(\n",
    "                    \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "                )\n",
    "\n",
    "                google_map_driver = webdriver.Chrome(options=options)\n",
    "                \n",
    "                google_map_query = \"https://www.google.com/maps/search/?api=1&query=%s,%s\" % (latitude, longitude)\n",
    "                google_map_driver.get(google_map_query)\n",
    "                print(\"scrape location data for, \", google_map_query)\n",
    "                \n",
    "                WebDriverWait(google_map_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'DkEaL')))\n",
    "                possible_addressGoogleMap_elements = google_map_driver.find_elements(By.CLASS_NAME, 'DkEaL')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"retry  scrape Google Map..\")\n",
    "                cnt_retry += 1\n",
    "                google_map_driver.close()\n",
    "                continue\n",
    "\n",
    "\n",
    "            # after init new webdriver -> continure scrape location data\n",
    "\n",
    "            # if found some wiered place that doesn't even have its address\n",
    "            # skip this case for now...\n",
    "            if(not len(possible_addressGoogleMap_elements)):\n",
    "                return res_location\n",
    "\n",
    "            subStrDistrict = \"อำเภอ\"\n",
    "            subStrSubDistrict = \"ตำบล\"\n",
    "\n",
    "            if province_th == \"กรุงเทพมหานคร\":\n",
    "                subStrDistrict = \"เขต\"\n",
    "                subStrSubDistrict = \"แขวง\"\n",
    "\n",
    "            district = 0\n",
    "            subDirstrict = 0\n",
    "\n",
    "            # find location\n",
    "            useData = None\n",
    "            for cur_element in possible_addressGoogleMap_elements:\n",
    "                if province_th in cur_element.text and cur_element.text.find(subStrDistrict) != -1:\n",
    "                    useData = cur_element.text.replace(\",\",\"\").replace(\"เเ\",\"แ\")\n",
    "                    break\n",
    "           \n",
    "            if(useData != None):\n",
    "                # print(\"Full Address :\",useData)\n",
    "                # another brute force way in case of province 'กรุงเทพหมานคร' not have word 'แขวง' in address\n",
    "                if(province_th == 'กรุงเทพมหานคร' and useData.find(subStrSubDistrict) == -1):\n",
    "                    subAddress_split = useData.split(' ')\n",
    "                    cur_province_Idx = subAddress_split.index(province_th)\n",
    "                    district = subAddress_split[cur_province_Idx - 1].replace(\"เขต\",\"\")\n",
    "\n",
    "                else:\n",
    "                    start_address_index = useData.find(subStrDistrict)\n",
    "                    subAddress = useData[start_address_index:]\n",
    "                    district = subAddress[subAddress.find(subStrDistrict)+len(subStrDistrict):subAddress.find(province_th)].replace(\" \",\"\")               \n",
    "\n",
    "                if district == \"เมือง\":\n",
    "                    district = district+province_th\n",
    "\n",
    "                # filter row to find 'ISO_3166_code', 'zip_code', 'geo_code'\n",
    "                geo_code_df = pd.read_csv(fh.PATH_TO_GEOCODE)\n",
    "                filtered_rows = geo_code_df[\n",
    "                    (geo_code_df['province_th'] == province_th) & (geo_code_df['district_th'] == district)\n",
    "                ]\n",
    "                filtered_rows.reset_index(inplace=True, drop=True)\n",
    "                \n",
    "                if not filtered_rows.empty:\n",
    "                    print(\"found province :\",filtered_rows.loc[0, 'ISO_3166_code'], province_th)\n",
    "                    print(\"found District :\",filtered_rows.loc[0, 'zip_code'], district)\n",
    "\n",
    "                    res_location.set_address(address_tripAdvisor if len(address_tripAdvisor) else useData)\n",
    "                    res_location.set_province(province_th)\n",
    "                    res_location.set_district(district)\n",
    "                    res_location.set_sub_district(\"\")\n",
    "                    res_location.set_province_code(filtered_rows.loc[0, 'ISO_3166_code'])\n",
    "                    res_location.set_district_code(filtered_rows.loc[0, 'zip_code'])\n",
    "                    res_location.set_sub_district_code(0)\n",
    "\n",
    "                else:\n",
    "                    print(\"not found province :\", province_th)\n",
    "                    print(\"not found District :\", district)\n",
    "\n",
    "                    res_location.set_address(address_tripAdvisor if len(address_tripAdvisor) else useData)\n",
    "                    res_location.set_province(province_th)\n",
    "                    res_location.set_district(district)\n",
    "                    res_location.set_sub_district(\"\")\n",
    "                    res_location.set_province_code(0)\n",
    "                    res_location.set_district_code(0)\n",
    "                    res_location.set_sub_district_code(0)\n",
    "\n",
    "            google_map_driver.close()\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"can't scrape location data\")\n",
    "\n",
    "    return res_location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape lat/long, types, facilities, openingHours (there are in adjust page of current restaurant: 'https://th.tripadvisor.com/ImproveListing-d1792735.html')\n",
    "def scrape_adjust_page(restaurant_page_driver: webdriver, link_to_adjust_page: str) -> tuple[float, float, list[str]]:\n",
    "    lat = 0\n",
    "    long = 0\n",
    "\n",
    "    # create new webdriver to continue scrape lat/long, openingHours in adjust restaurant page\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape single restaurant ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "\n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        adjust_page_driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # retry in case of web restrictions, some elements not loaded\n",
    "        try:\n",
    "            print(\"scrape data in adjust restaurant page...\")\n",
    "            print(\"for link : \", link_to_adjust_page)\n",
    "            adjust_page_driver.get(link_to_adjust_page)\n",
    "\n",
    "            print(\"debug option of adjust page: \")\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'DiHOR')))\n",
    "\n",
    "            # find dropdown --> click display data below --> cick display lat/long input form\n",
    "            possible_target_btn = adjust_page_driver.find_elements(By.CLASS_NAME, 'DiHOR')\n",
    "            for cur_dropdown_btn in possible_target_btn:\n",
    "                cur_dropdown_text = cur_dropdown_btn.text\n",
    "                if(\"แนะนำการแก้ไขข้อมูลของสถานที่นี้\" in cur_dropdown_text):\n",
    "                    print(\"found target dropdown btn ...\")\n",
    "                    cur_dropdown_btn.click()\n",
    "                    WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/button')))\n",
    "                    # find button click to display lat/long input form\n",
    "                    display_lat_long_btn = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/button')\n",
    "                    display_lat_long_btn.click()\n",
    "\n",
    "        except Exception as e:\n",
    "            cnt_retry += 1\n",
    "            adjust_page_driver.quit()\n",
    "            print(\"retry adjust page...\")\n",
    "            continue\n",
    "\n",
    "      \n",
    "        # find lat/long\n",
    "        try:\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[1]/div/div[2]/div/div/div/span')))\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[2]/div/div[2]/div/div/div/span')))\n",
    "    \n",
    "            lat_input_container = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[1]/div/div[2]/div/div/div/span')\n",
    "            lat_input_element = lat_input_container.find_element(By.TAG_NAME, 'input')\n",
    "            lat = float(lat_input_element.get_attribute('value'))\n",
    "\n",
    "            long_input_container = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[2]/div/div[2]/div/div/div/span')\n",
    "            long_input_element = long_input_container.find_element(By.TAG_NAME, 'input')\n",
    "            long = float(long_input_element.get_attribute('value'))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find lat/long\")\n",
    "        \n",
    "        print(\"lat : \", lat)\n",
    "        print(\"long : \", long)\n",
    "\n",
    "        # **if can't find lat/long --> don't scrape this attaction\n",
    "        if(lat == 0 and long == 0):\n",
    "            print(\"in scrape_location_latlong_openingHours --> can't find lat/long --> 0, 0\")\n",
    "            return lat, long, [\"ไม่รู้จัก\"]\n",
    "\n",
    "        # find type\n",
    "        types = []\n",
    "        all_accomodation_types = [\"โรงแรม\", \"โมเตล\", \"รีสอร์ท\", \"ที่พักพร้อมอาหารเช้า\", \"โรงแรมขนาดเล็ก\", \"Condominium/Apartment\", \"วิลล่า\", \"พื้นที่ตั้งแคมป์\", \"โฮสเทล\", \"Vacation Rental House\", \"ไม่รู้จัก\"] \n",
    "        try:\n",
    "            # PMWyE\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'PMWyE')))\n",
    "            all_checkbox_containers = adjust_page_driver.find_elements(By.CLASS_NAME, 'PMWyE')\n",
    "            for i in range(len(all_checkbox_containers)):\n",
    "                cur_checkbox = all_checkbox_containers[i].find_element(By.TAG_NAME, 'span')\n",
    "                is_check = True if cur_checkbox.get_attribute('class') != 'U' else False\n",
    "                if(is_check):\n",
    "                    types.append(all_accomodation_types[i])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find type\")\n",
    "\n",
    "        print(\"types --> \", types)\n",
    "\n",
    "        adjust_page_driver.quit()\n",
    "        break\n",
    "\n",
    "    return lat, long, types.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_restaurant(link_to_restaurant: str, province_th: str) -> Restaurant:\n",
    "    \n",
    "    restaurant = Restaurant()\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape single restaurant ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set web browser options to run\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", \n",
    "            {\n",
    "                \"profile.managed_default_content_settings.images\": 2, # Disable image\n",
    "                \"profile.default_content_setting_values.cookies\": 2,  # Block all cookies\n",
    "                \"profile.default_content_settings.popups\": 0,         # Disable popups\n",
    "                \"profile.managed_default_content_settings.cookies\": 2  # Disable third-party cookies\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # initialize the web driver with service, selenium-wire options, and web browser options\n",
    "        restaurant_page_driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            print(\"******************************************************\")\n",
    "            print(\"scrape single restaurant...\")\n",
    "            print(\"for restaurant : \", link_to_restaurant)\n",
    "            restaurant_page_driver.get(link_to_restaurant)\n",
    "            # restaurant_page_driver.add_cookie()\n",
    "\n",
    "            print(\"debug scrape_single_restaurant: top info component section\")\n",
    "            # WebDriverWait(restaurant_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/span/div[4]/div/div[1]/div[3]/div')))\n",
    "            # top_info_container = restaurant_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/span/div[4]/div/div[1]/div[3]/div')\n",
    "\n",
    "            print(\"debug scrape_single_restaurant: bottom info component section\")\n",
    "            WebDriverWait(restaurant_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div/div[6]/div/div[2]')))\n",
    "            bottom_info_container = restaurant_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div/div[6]/div/div[2]')\n",
    "\n",
    "            print(\"debug scrape_single_attraction: common component section\")\n",
    "            WebDriverWait(restaurant_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'IDaDx')))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"retry single restaurant case 1...\")\n",
    "            cnt_retry += 1\n",
    "            restaurant_page_driver.quit()\n",
    "            continue\n",
    "        \n",
    "\n",
    "        # convert restaurant url to adjust page url\n",
    "        # for example: from 'https://th.tripadvisor.com/Restaurant_Review-g1210687-d1792735-Reviews-Kwong_Shop_Seafood-Kata_Beach_Karon_Phuket.html' to 'https://th.tripadvisor.com/ImproveListing-d586602.html'\n",
    "        link_to_adjust_page = 'https://th.tripadvisor.com/ImproveListing-%s.html' % (link_to_restaurant.split('-')[2])\n",
    "\n",
    "        # ** find lat/long, location data and openingHours (there are in another page of current restaurant)\n",
    "        # ** if this restaurant not have lat/long\n",
    "        # ** don't continue to scrape\n",
    "        lat, long, types = scrape_adjust_page(\n",
    "            restaurant_page_driver = restaurant_page_driver,\n",
    "            link_to_adjust_page = link_to_adjust_page\n",
    "        )\n",
    "        \n",
    "        # **if can't find lat/long --> don't scrape this attaction\n",
    "        if(lat == 0 and long == 0):\n",
    "            print(\"in scrape_single_restaurant --> can't find lat/long --> don't scrape this restaurant ...\")\n",
    "            restaurant_page_driver.quit()\n",
    "            return Restaurant()\n",
    "\n",
    "\n",
    "        # find name\n",
    "        name = \"\"\n",
    "        try:\n",
    "            WebDriverWait(restaurant_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'rRtyp')))\n",
    "            name_element = restaurant_page_driver.find_element(By.CLASS_NAME, 'rRtyp')\n",
    "            name = name_element.text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find name\")\n",
    "\n",
    "        print(\"name -> \", name)\n",
    "\n",
    "        # find description\n",
    "        # description = \"\"\n",
    "        # try:\n",
    "        #     try:\n",
    "        #         # find button to click readmore (if it exists, it likely to be the first elements of class 'lszDU')\n",
    "        #         WebDriverWait(restaurant_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'lszDU')))\n",
    "        #         click_readmore_btn = restaurant_page_driver.find_element(By.CLASS_NAME, 'lszDU')\n",
    "        #         click_readmore_btn.click()\n",
    "\n",
    "        #     except Exception as e:\n",
    "        #         pass\n",
    "\n",
    "        #     WebDriverWait(restaurant_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'zYHGB')))\n",
    "        #     all_description_elements = restaurant_page_driver.find_elements(By.CLASS_NAME, 'zYHGB')\n",
    "        #     for cur_element in all_description_elements:\n",
    "        #         cur_text =  cur_element.text\n",
    "        #         if(len(cur_text)):\n",
    "        #             description += cur_text + '\\n'\n",
    "            \n",
    "        # except Exception as e:\n",
    "        #     print(\"can't find description\")\n",
    "\n",
    "        # print(\"description -> \", description)\n",
    "        \n",
    "        # find phone\n",
    "        phone = \"\"\n",
    "        try:\n",
    "            WebDriverWait(restaurant_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div/div[3]/div/div[3]/span[2]/span[2]/a')))\n",
    "            phone_element = restaurant_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div/div[3]/div/div[3]/span[2]/span[2]/a')\n",
    "            phone_element_href = phone_element.get_attribute('href')\n",
    "            if(\"tel\" in phone_element_href):\n",
    "                phone = phone_element.text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find phone\")\n",
    "\n",
    "        print(\"phone --> \", phone)\n",
    "\n",
    "        # find rating\n",
    "        rating = 0\n",
    "        rating_count = 0\n",
    "        try:\n",
    "            WebDriverWait(restaurant_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'sOyfn')))\n",
    "            rating_container = restaurant_page_driver.find_element(By.CLASS_NAME, 'sOyfn')\n",
    "            \n",
    "            rating_element = rating_container.find_element(By.CLASS_NAME, 'uuBRH')\n",
    "            rating = float(rating_element.text)\n",
    "\n",
    "            rating_count_element = rating_container.find_element(By.CLASS_NAME, 'oXJmt')\n",
    "            rating_count = int(rating_count_element.text.replace(',', '').replace('รีวิว ', '').replace(' รายการ', ''))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find rating and rating_count\")\n",
    "\n",
    "        print(\"rating --> \", rating)\n",
    "        print(\"rating_count --> \", rating_count)\n",
    "\n",
    "\n",
    "        # find location\n",
    "        # location = scrape_location(\n",
    "        #     restaurant_page_driver = restaurant_page_driver,\n",
    "        #     latitude = lat,\n",
    "        #     longitude = long,\n",
    "        #     province_th = province_th\n",
    "        # )\n",
    "        # print(\"province :\", location.get_province_code(), location.get_province())\n",
    "        # print(\"District :\", location.get_district_code(), location.get_district())\n",
    "        # print(\"Address : \", location.get_address())\n",
    "\n",
    "\n",
    "        # find img_path\n",
    "        # img_path = scrape_img(restaurant_page_driver)\n",
    "        # print(\"cur img path -> \", img_path)\n",
    "\n",
    "        # set some of \"restaurant\" object properties\n",
    "        # restaurant.set_name(name)\n",
    "        # restaurant.set_description(description)\n",
    "        # restaurant.set_phone(phone)\n",
    "        # restaurant.set_latitude(lat)\n",
    "        # restaurant.set_longitude(long)\n",
    "        # # restaurant.set_imgPath(img_path)\n",
    "        # restaurant.set_website(link_to_restaurant)\n",
    "        # # restaurant.set_facility(facilities)\n",
    "        # # restaurant.set_priceRange(priceRange)\n",
    "        # restaurant.set_type(types)\n",
    "        # restaurant.set_location(\n",
    "        #     address = location.get_address(),\n",
    "        #     province = location.get_province(),\n",
    "        #     district = location.get_district(),\n",
    "        #     sub_district = location.get_sub_district(),\n",
    "        #     province_code = location.get_province_code(),\n",
    "        #     district_code = location.get_district_code(),\n",
    "        #     sub_district_code = location.get_sub_district_code()\n",
    "        # )\n",
    "        # restaurant.set_rating(\n",
    "        #     score = rating,\n",
    "        #     rating_count = rating_count\n",
    "        # )\n",
    "\n",
    "        restaurant_page_driver.quit()\n",
    "        break\n",
    "\n",
    "    return restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_url_by_page(query_url: str, page: int) -> list[str]:\n",
    "\n",
    "    res_url_by_page = []\n",
    "\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        if(cnt_retry == 10):\n",
    "            print(\"max retry for scrape data by page ...\")\n",
    "            break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        # os.environ['proxy_port']\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "      \n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # just check for ip\n",
    "        # print(\"just check for ip :\")\n",
    "        # driver.get(\"https://httpbin.io/ip\")\n",
    "        # print(driver.page_source)\n",
    "\n",
    "        # find group of restaurant on the nth page\n",
    "        all_restaurants_card = []\n",
    "\n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            query_url_by_page = convert_url_by_page(\n",
    "                link_to_restaurant = query_url,\n",
    "                page = page\n",
    "            )\n",
    "            driver.get(query_url_by_page)\n",
    "            # scroll and wait for some msec\n",
    "            driver.execute_script('window.scrollBy(0, document.body.scrollHeight)')\n",
    "            \n",
    "            print(\"check current page url --> \", driver.current_url)\n",
    "\n",
    "            # wait for div (each restaurant section) to be present and visible\n",
    "            print(\"b1 part 1\")\n",
    "            print(\"debug get_all_url_by_page: restaurant by one page section\")\n",
    "            WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'yJIls')))\n",
    "            all_restaurants_card = driver.find_elements(By.CLASS_NAME, 'yJIls')\n",
    "\n",
    "\n",
    "            # check if all accomodation card can get tag a and its attribute for url\n",
    "            print(\"b2\")\n",
    "            print(\"check in loop ...\")\n",
    "            for cur_restaurant_card in all_restaurants_card:\n",
    "\n",
    "                cur_restaurant_url = cur_restaurant_card.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                print(\"cur_restaurant_url : \", cur_restaurant_url)\n",
    "                res_url_by_page.append(cur_restaurant_url)\n",
    "            \n",
    "            driver.quit()\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"retry find get_all_url_by_page ...\")\n",
    "            cnt_retry += 1\n",
    "            driver.quit()\n",
    "            continue\n",
    "\n",
    "    return res_url_by_page.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_restaurants_by_province(page: int, province_url: str, province: str) -> pd.DataFrame:\n",
    "    # res_restaurant_df = pd.DataFrame()\n",
    "    res_restaurant_df = create_restaurant_df(Restaurant())\n",
    "    \n",
    "    cnt_for_debug = 0\n",
    "        \n",
    "    print(\"scraping restaurant | province --> %s | page --> %s\" % (province, page))\n",
    "\n",
    "    all_url_by_page = get_all_url_by_page(query_url = province_url, page = page)\n",
    "\n",
    "    # use data from 'res_get_data_by_page' to retrive data of specific restaurant\n",
    "    for cur_restaurant_url in all_url_by_page:\n",
    "        # just use to limit amount of place --> will be removed \n",
    "        if(cnt_for_debug == 2):\n",
    "            break\n",
    "\n",
    "        # continue scraping data for a specific resgtaurant\n",
    "        cur_restaurant = scrape_single_restaurant(\n",
    "            link_to_restaurant = cur_restaurant_url,\n",
    "            province_th = province\n",
    "        )\n",
    "\n",
    "        cnt_for_debug += 1\n",
    "\n",
    "        # create data frame represent data scrape from current restaurant card\n",
    "        # cur_restaurant_df = create_restaurant_df(restaurant=cur_restaurant)\n",
    "\n",
    "        # concat all data frame result\n",
    "        # res_restaurant_df = pd.concat([res_restaurant_df, cur_restaurant_df])\n",
    "    \n",
    "    return res_restaurant_df.iloc[1:, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping restaurant | province --> ภูเก็ต | page --> 1\n",
      "check current page url -->  https://th.tripadvisor.com/Restaurants-g1215781-oa0-Phuket_Town_Phuket.html\n",
      "b1 part 1\n",
      "debug get_all_url_by_page: restaurant by one page section\n",
      "b2\n",
      "check in loop ...\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d11810229-Reviews-The_Tent-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d15059216-Reviews-Three_Monkeys_Restaurant-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315818-d21238792-Reviews-Day_Night_of_Phuket-Talat_Yai_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d21173913-Reviews-Baikingu_Japanese_Buffet_Garden_Restaurant-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d1759741-Reviews-Blue_Elephant_Cooking_School_Restaurant_Phuket-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d9722296-Reviews-Surf_and_Turf_by_Soul_Kitchen-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315817-d12852621-Reviews-Piset_Restaurant_Phuket-Talat_Nuea_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d13349831-Reviews-Tantitium_Restaurant-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315818-d5818564-Reviews-One_Chun_Cafe_and_Restaurant-Talat_Yai_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1389361-d1874082-Reviews-Kan_Eang_Pier-Chalong_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d21138123-Reviews-Hong_Bao_Ramada_Phuket-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1389361-d19622658-Reviews-The_Distillery_Phuket_Home_of_Chalong_Bay-Chalong_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315817-d12852621-Reviews-Piset_Restaurant_Phuket-Talat_Nuea_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d23839140-Reviews-Flamingo_Phuket-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d13989518-Reviews-Raya_Restaurant_Phuket_Old-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315818-d1172082-Reviews-Tukabkhao_Phuket_Local_Food_Restaurant-Talat_Yai_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d3168082-Reviews-Kopitiam_by_Wilai-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d26679753-Reviews-Beerfest_Restaurant_And_Craft_Beer_Brewery_Phuket-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1389361-d7267941-Reviews-Baan_Noy_Restaurant-Chalong_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315818-d13307440-Reviews-Amore_Mexican_Tapas_Bar-Talat_Yai_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d27469669-Reviews-Numnum_Cafe_Restaurant_Phuket_Old_Town-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d23682010-Reviews-AKOYA_Star_Lounge-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d2218722-Reviews-Tunk_Ka_Cafe-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d11810229-Reviews-The_Tent-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1389361-d2714713-Reviews-Pizzeria_Agli_Amici_da_Michele_Jimmy-Chalong_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d23708006-Reviews-Sriwara_Bistro_Cafe-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d8713804-Reviews-Fisherman_s_Wharf-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d1090092-Reviews-La_Gaetana-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d4352068-Reviews-Prego_by_the_Beach-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1389361-d25403279-Reviews-Lucha_Cantina_Phuket-Chalong_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d1981774-Reviews-Brasserie_Phuket-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d10219861-Reviews-Kabang_Restaurant_and_Beach_Bar-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d23680636-Reviews-Yon_Ocean_House-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315817-d23823296-Reviews-Cava_bien_marche-Talat_Nuea_Phuket_Town_Phuket.html\n",
      "******************************************************\n",
      "scrape single restaurant...\n",
      "for restaurant :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d11810229-Reviews-The_Tent-Phuket_Town_Phuket.html\n",
      "debug scrape_single_restaurant: top info component section\n",
      "debug scrape_single_restaurant: bottom info component section\n",
      "debug scrape_single_attraction: common component section\n",
      "name ->  เดอะ เต๊นท์\n",
      "phone -->  +66 76 217 775\n",
      "rating -->  4.5\n",
      "rating_count -->  93\n",
      "******************************************************\n",
      "scrape single restaurant...\n",
      "for restaurant :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d15059216-Reviews-Three_Monkeys_Restaurant-Wichit_Phuket_Town_Phuket.html\n",
      "debug scrape_single_restaurant: top info component section\n",
      "debug scrape_single_restaurant: bottom info component section\n",
      "debug scrape_single_attraction: common component section\n",
      "name ->  Three Monkeys Restaurant\n",
      "phone -->  +66 98 010 8838\n",
      "rating -->  5.0\n",
      "rating_count -->  4034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Service.__del__ at 0x00000176286260C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 189, in __del__\n",
      "    self.stop()\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 146, in stop\n",
      "    self.send_remote_shutdown_command()\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 126, in send_remote_shutdown_command\n",
      "    request.urlopen(f\"{self.service_url}/shutdown\")\n",
      "  File \"c:\\Python312\\Lib\\urllib\\request.py\", line 215, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\urllib\\request.py\", line 515, in open\n",
      "    response = self._open(req, data)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\urllib\\request.py\", line 532, in _open\n",
      "    result = self._call_chain(self.handle_open, protocol, protocol +\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\urllib\\request.py\", line 492, in _call_chain\n",
      "    result = func(*args)\n",
      "             ^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\urllib\\request.py\", line 1373, in http_open\n",
      "    return self.do_open(http.client.HTTPConnection, req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\urllib\\request.py\", line 1344, in do_open\n",
      "    h.request(req.get_method(), req.selector, req.data, headers,\n",
      "  File \"c:\\Python312\\Lib\\http\\client.py\", line 1331, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"c:\\Python312\\Lib\\http\\client.py\", line 1377, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"c:\\Python312\\Lib\\http\\client.py\", line 1326, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"c:\\Python312\\Lib\\http\\client.py\", line 1085, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"c:\\Python312\\Lib\\http\\client.py\", line 1029, in send\n",
      "    self.connect()\n",
      "  File \"c:\\Python312\\Lib\\http\\client.py\", line 995, in connect\n",
      "    self.sock = self._create_connection(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\socket.py\", line 844, in create_connection\n",
      "    exceptions.clear()  # raise only the last error\n",
      "    ^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory res_restaurant_scraping\\res_restaurant_Phuket created successfully\n"
     ]
    }
   ],
   "source": [
    "# *** select one province from 'ALL_RESTAURANTS_TRIPADVISOR_DATA'\n",
    "# *** so, change \"Idx_of_region\" everytime when scrape another province\n",
    "Idx_of_region = 0\n",
    "cur_region_data = ALL_RESTAURANTS_TRIPADVISOR_DATA[Idx_of_region]\n",
    "\n",
    "# select first and last page to scrape\n",
    "# but in this module will not have any effect (just some dummy number to use with file name)\n",
    "# will have effect on module \"mulProcess_restaurant_scraping_proxy\"\n",
    "first_page = 444\n",
    "last_page = 888\n",
    "\n",
    "cur_province_en = cur_region_data[0]\n",
    "cur_province_th = cur_region_data[1]\n",
    "cur_province_url = cur_region_data[2]\n",
    "\n",
    "# cur_res_allRestaurants_df = create_restaurant_df(Restaurant())\n",
    "\n",
    "# get dataframe result of all restaurant in current province\n",
    "cur_res_allRestaurants_df = scrape_restaurants_by_province(\n",
    "    page = 1,\n",
    "    province_url = cur_province_url,\n",
    "    province = cur_province_th\n",
    ")\n",
    "\n",
    "# don't forget to remove row with lat/long be zero\n",
    "\n",
    "# remove duplicate restaurant\n",
    "cur_res_allRestaurants_df.drop_duplicates(subset=['name'], inplace=True)\n",
    "# set new index\n",
    "cur_res_allRestaurants_df.set_index(['name'], inplace=True)\n",
    "\n",
    "# create directory to store result of scraping restaurant\n",
    "# for example: 'restaurant_scraping\\res_restaurant_scraping\\res_restaurant_Phuket'\n",
    "createDirectory(fh.STORE_ACCOMM_SCRAPING, os.path.join('res_restaurant_scraping', 'res_restaurant_%s' % (cur_province_en)))\n",
    "\n",
    "# save result dataframe to .csv\n",
    "# for example: 'res_restaurant_Phuket_page_1_44.csv'\n",
    "res_file_name = 'res_restaurant_%s_page_%s_%s.csv' % (cur_province_en, first_page, last_page)\n",
    "res_path = os.path.join(fh.STORE_ACCOMM_SCRAPING, 'res_restaurant_scraping', 'res_restaurant_%s' % (cur_province_en), res_file_name)\n",
    "cur_res_allRestaurants_df.to_csv(res_path, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://th.tripadvisor.com/Hotel_Review-g297930-d315568-Reviews-Phuket_Marriott_Resort_Spa_Merlin_Beach-Patong_Kathu_Phuket.html#/media/315568/?albumid=101&type=0&category=101\n",
    "# https://th.tripadvisor.com/Hotel_Review-g1210687-d1379794-Reviews-Chanalai_Romantica_Resort-Kata_Beach_Karon_Phuket.html#/media/1379794/?albumid=101&type=0&category=101\n",
    "# \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
