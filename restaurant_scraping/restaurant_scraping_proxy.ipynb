{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_34728\\134437315.py\", line 10, in <module>\n",
      "    from packages.file_handler_package.file_handler import *\n",
      "  File \"c:\\Users\\user\\git\\CEPPWebScraping\\restaurant_scraping\\..\\packages\\file_handler_package\\file_handler.py\", line 5, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 39, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\compat\\__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\compat\\pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_34728\\134437315.py\", line 10, in <module>\n",
      "    from packages.file_handler_package.file_handler import *\n",
      "  File \"c:\\Users\\user\\git\\CEPPWebScraping\\restaurant_scraping\\..\\packages\\file_handler_package\\file_handler.py\", line 5, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.dtypes import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\dtypes.py\", line 24, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_34728\\134437315.py\", line 10, in <module>\n",
      "    from packages.file_handler_package.file_handler import *\n",
      "  File \"c:\\Users\\user\\git\\CEPPWebScraping\\restaurant_scraping\\..\\packages\\file_handler_package\\file_handler.py\", line 5, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 50, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\numexpr\\__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_34728\\134437315.py\", line 10, in <module>\n",
      "    from packages.file_handler_package.file_handler import *\n",
      "  File \"c:\\Users\\user\\git\\CEPPWebScraping\\restaurant_scraping\\..\\packages\\file_handler_package\\file_handler.py\", line 5, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 64, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py\", line 60, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\bottleneck\\__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import constants.constants as const\n",
    "import constants.file_handler_constants as fh\n",
    "from constants.restaurant_constants import *\n",
    "\n",
    "from packages.restaurant.Restaurant import *\n",
    "from packages.file_handler_package.file_handler import *\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from selenium.webdriver import Remote, ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.actions.wheel_input import ScrollOrigin\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from seleniumwire import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.edge.options import Options\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_restaurant_df(restaurant: Restaurant) -> pd.DataFrame:\n",
    "    restaurant_dict = {\n",
    "        'name' : [restaurant.get_name()],\n",
    "        'description' : [restaurant.get_description()],\n",
    "        'latitude' : [restaurant.get_latitude()],\n",
    "        'longitude' : [restaurant.get_longitude()],\n",
    "        'imgPath' : [restaurant.get_imgPath()],\n",
    "        'phone': [restaurant.get_phone()],\n",
    "        'website': [restaurant.get_website()],\n",
    "        'facility': [restaurant.get_facility()],\n",
    "        'type': [restaurant.get_type()],\n",
    "\n",
    "        # location\n",
    "        'address' : [restaurant.get_location().get_address()],\n",
    "        'province' : [restaurant.get_location().get_province()],\n",
    "        'district' : [restaurant.get_location().get_district()],\n",
    "        'subDistrict' : [restaurant.get_location().get_sub_district()],\n",
    "        'province_code' : [restaurant.get_location().get_province_code()],\n",
    "        'district_code' : [restaurant.get_location().get_district_code()],\n",
    "        'sub_district_code' : [restaurant.get_location().get_sub_district_code()],\n",
    "\n",
    "        # rating\n",
    "        'score' : [restaurant.get_rating().get_score()],\n",
    "        'ratingCount' : [restaurant.get_rating().get_ratingCount()],\n",
    "    }\n",
    "\n",
    "    restaurant_df = pd.DataFrame(restaurant_dict)\n",
    "    \n",
    "    return restaurant_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_url_by_page(link_to_restaurant: str, page: int) -> str:\n",
    "\n",
    "    # if(page == 1):\n",
    "    #     return link_to_restaurant\n",
    "    \n",
    "    first_page_url_split = link_to_restaurant.split('-')\n",
    "    nth_count_page = 'oa%s' % ((page - 1) * 30)\n",
    "    first_page_url_split[-1] = nth_count_page\n",
    "    res_page_url =  \"-\".join(first_page_url_split)\n",
    "\n",
    "    return res_page_url\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_img(restaurant_page_driver: webdriver) -> list[str]:\n",
    "    \n",
    "    res_imgPath = []\n",
    "\n",
    "    # find button and click\n",
    "    # to see image modal\n",
    "    try:\n",
    "        print(\"p2\")\n",
    "        # click_img_btn = restaurant_page_driver.find_element(By.CLASS_NAME, 'QXsnf')\n",
    "        WebDriverWait(restaurant_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'GuzzA')))\n",
    "        click_img_btn = restaurant_page_driver.find_element(By.CLASS_NAME, 'GuzzA')\n",
    "        print(\"p3\")\n",
    "        \n",
    "        # Move to the element and click\n",
    "        actions = ActionChains(restaurant_page_driver)\n",
    "        actions.move_to_element(click_img_btn).click().perform()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"can't open modal image\")\n",
    "        return res_imgPath\n",
    "\n",
    "    # scrape image address\n",
    "    try:\n",
    "        is_end_scrape_img = False\n",
    "        cnt_retry = 0\n",
    "        print(\"p7\")\n",
    "        while(not is_end_scrape_img):\n",
    "            if(cnt_retry == 20):\n",
    "                print(\"max retry for scrape image...\")\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                WebDriverWait(restaurant_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'cfCAA')))\n",
    "                all_img_elements = restaurant_page_driver.find_elements(By.CLASS_NAME, 'cfCAA')\n",
    "                print(\"find image element -> \", len(all_img_elements))\n",
    "                for cur_img_element in all_img_elements:\n",
    "                    cur_bgImg_val = cur_img_element.value_of_css_property('background-image')\n",
    "                    match = re.search(r'url\\(\"(.*?)\"\\)', cur_bgImg_val)\n",
    "                    if match:\n",
    "                        res_imgPath.append(match.group(1))\n",
    "\n",
    "                is_end_scrape_img = True\n",
    "\n",
    "            except Exception as e:\n",
    "                cnt_retry += 1\n",
    "                print(\"retry scrape img...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "            pass\n",
    "    \n",
    "\n",
    "    return res_imgPath.copy()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_location(restaurant_page_driver: webdriver, latitude: float, longitude: float, province_th: str) -> Location:\n",
    "\n",
    "    # find better address description on wongnai\n",
    "    # for example: \"991 ถนนพระราม 1 Pathum Wan, กรุงเทพมหานคร (กทม.) 10330 ไทย\"\n",
    "    address_tripAdvisor = \"\"\n",
    "    possible_address_xpath = [\n",
    "        '//*[@id=\"lithium-root\"]/main/div/div[3]/div/div[3]/span[1]/span[2]/button/span',\n",
    "    ]\n",
    "\n",
    "    for cur_address_xpath in possible_address_xpath:\n",
    "        try:\n",
    "            WebDriverWait(restaurant_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, cur_address_xpath)))\n",
    "            address_element = restaurant_page_driver.find_element(By.XPATH, cur_address_xpath)\n",
    "            address_tripAdvisor = address_element.text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"can't find address_tripAdvisor\")\n",
    "\n",
    "\n",
    "    # start scrape location\n",
    "    res_location = Location()\n",
    "    cnt_retry = 0\n",
    "    try:\n",
    "        while(True):\n",
    "            if(cnt_retry == 10):\n",
    "                print(\"max retry for scrape Google Map ...\")\n",
    "                break\n",
    "            \n",
    "            # set up new webdriver to work googlemap url(query for specific lat/long)\n",
    "            possible_addressGoogleMap_elements = []\n",
    "            try:\n",
    "                # set Chrome options to run in headless mode\n",
    "                # options = Options()\n",
    "                options = webdriver.ChromeOptions()\n",
    "                options.add_argument(\"start-maximized\")\n",
    "                # options.add_argument(\"--headless=new\")\n",
    "                options.add_experimental_option(\n",
    "                    \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "                )\n",
    "\n",
    "                google_map_driver = webdriver.Chrome(options=options)\n",
    "                \n",
    "                google_map_query = \"https://www.google.com/maps/search/?api=1&query=%s,%s\" % (latitude, longitude)\n",
    "                google_map_driver.get(google_map_query)\n",
    "                print(\"scrape location data for, \", google_map_query)\n",
    "                \n",
    "                WebDriverWait(google_map_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'DkEaL')))\n",
    "                possible_addressGoogleMap_elements = google_map_driver.find_elements(By.CLASS_NAME, 'DkEaL')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"retry  scrape Google Map..\")\n",
    "                cnt_retry += 1\n",
    "                google_map_driver.close()\n",
    "                continue\n",
    "\n",
    "\n",
    "            # after init new webdriver -> continure scrape location data\n",
    "\n",
    "            # if found some wiered place that doesn't even have its address\n",
    "            # skip this case for now...\n",
    "            if(not len(possible_addressGoogleMap_elements)):\n",
    "                return res_location\n",
    "\n",
    "            subStrDistrict = \"อำเภอ\"\n",
    "            subStrSubDistrict = \"ตำบล\"\n",
    "\n",
    "            if province_th == \"กรุงเทพมหานคร\":\n",
    "                subStrDistrict = \"เขต\"\n",
    "                subStrSubDistrict = \"แขวง\"\n",
    "\n",
    "            district = 0\n",
    "            subDirstrict = 0\n",
    "\n",
    "            # find location\n",
    "            useData = None\n",
    "            for cur_element in possible_addressGoogleMap_elements:\n",
    "                if province_th in cur_element.text and cur_element.text.find(subStrDistrict) != -1:\n",
    "                    useData = cur_element.text.replace(\",\",\"\").replace(\"เเ\",\"แ\")\n",
    "                    break\n",
    "           \n",
    "            if(useData != None):\n",
    "                # print(\"Full Address :\",useData)\n",
    "                # another brute force way in case of province 'กรุงเทพหมานคร' not have word 'แขวง' in address\n",
    "                if(province_th == 'กรุงเทพมหานคร' and useData.find(subStrSubDistrict) == -1):\n",
    "                    subAddress_split = useData.split(' ')\n",
    "                    cur_province_Idx = subAddress_split.index(province_th)\n",
    "                    district = subAddress_split[cur_province_Idx - 1].replace(\"เขต\",\"\")\n",
    "\n",
    "                else:\n",
    "                    start_address_index = useData.find(subStrDistrict)\n",
    "                    subAddress = useData[start_address_index:]\n",
    "                    district = subAddress[subAddress.find(subStrDistrict)+len(subStrDistrict):subAddress.find(province_th)].replace(\" \",\"\")               \n",
    "\n",
    "                if district == \"เมือง\":\n",
    "                    district = district+province_th\n",
    "\n",
    "                # filter row to find 'ISO_3166_code', 'zip_code', 'geo_code'\n",
    "                geo_code_df = pd.read_csv(fh.PATH_TO_GEOCODE)\n",
    "                filtered_rows = geo_code_df[\n",
    "                    (geo_code_df['province_th'] == province_th) & (geo_code_df['district_th'] == district)\n",
    "                ]\n",
    "                filtered_rows.reset_index(inplace=True, drop=True)\n",
    "                \n",
    "                if not filtered_rows.empty:\n",
    "                    print(\"found province :\",filtered_rows.loc[0, 'ISO_3166_code'], province_th)\n",
    "                    print(\"found District :\",filtered_rows.loc[0, 'zip_code'], district)\n",
    "\n",
    "                    res_location.set_address(address_tripAdvisor if len(address_tripAdvisor) else useData)\n",
    "                    res_location.set_province(province_th)\n",
    "                    res_location.set_district(district)\n",
    "                    res_location.set_sub_district(\"\")\n",
    "                    res_location.set_province_code(filtered_rows.loc[0, 'ISO_3166_code'])\n",
    "                    res_location.set_district_code(filtered_rows.loc[0, 'zip_code'])\n",
    "                    res_location.set_sub_district_code(0)\n",
    "\n",
    "                else:\n",
    "                    print(\"not found province :\", province_th)\n",
    "                    print(\"not found District :\", district)\n",
    "\n",
    "                    res_location.set_address(address_tripAdvisor if len(address_tripAdvisor) else useData)\n",
    "                    res_location.set_province(province_th)\n",
    "                    res_location.set_district(district)\n",
    "                    res_location.set_sub_district(\"\")\n",
    "                    res_location.set_province_code(0)\n",
    "                    res_location.set_district_code(0)\n",
    "                    res_location.set_sub_district_code(0)\n",
    "\n",
    "            google_map_driver.close()\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"can't scrape location data\")\n",
    "\n",
    "    return res_location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape lat/long, openingHours, types, facilities (there are in adjust page of current restaurant: 'https://th.tripadvisor.com/ImproveListing-d1792735.html')\n",
    "def scrape_adjust_page(restaurant_page_driver: webdriver, link_to_adjust_page: str) -> tuple[float, float, dict, list[str], list[str]]:\n",
    "    lat = 0\n",
    "    long = 0\n",
    "    openingHours = {}\n",
    "    types = []\n",
    "    facilities = []\n",
    "    \n",
    "    # create new webdriver to continue scrape lat/long, openingHours in adjust restaurant page\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape single restaurant ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", \n",
    "            {\n",
    "                \"profile.managed_default_content_settings.images\": 2, # Disable image\n",
    "                # \"profile.default_content_setting_values.cookies\": 2,  # Block all cookies\n",
    "                \"profile.default_content_settings.popups\": 0,         # Disable popups\n",
    "                # \"profile.managed_default_content_settings.cookies\": 2  # Disable third-party cookies\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        adjust_page_driver = webdriver.Chrome(\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # retry in case of web restrictions, some elements not loaded\n",
    "        try:\n",
    "            print(\"scrape data in adjust restaurant page...\")\n",
    "            print(\"for link : \", link_to_adjust_page)\n",
    "            adjust_page_driver.get(link_to_adjust_page)\n",
    "\n",
    "            print(\"debug option of adjust page: \")\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'DiHOR')))\n",
    "\n",
    "            # find dropdown --> click display data below --> cick display lat/long input form\n",
    "            possible_target_btn = adjust_page_driver.find_elements(By.CLASS_NAME, 'DiHOR')\n",
    "            for cur_dropdown_btn in possible_target_btn:\n",
    "                cur_dropdown_text = cur_dropdown_btn.text\n",
    "                if(\"แนะนำการแก้ไขข้อมูลของสถานที่นี้\" in cur_dropdown_text):\n",
    "                    print(\"found target dropdown btn ...\")\n",
    "                    cur_dropdown_btn.click()\n",
    "                    WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/button')))\n",
    "                    # find button click to display lat/long input form\n",
    "                    display_lat_long_btn = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/button')\n",
    "                    display_lat_long_btn.click()\n",
    "\n",
    "        except Exception as e:\n",
    "            cnt_retry += 1\n",
    "            adjust_page_driver.quit()\n",
    "            print(\"retry adjust page...\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        # scroll and wait for some msec\n",
    "        adjust_page_driver.execute_script('window.scrollBy(0, document.body.scrollHeight)')\n",
    "\n",
    "        # find lat/long\n",
    "        try:\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[1]/div/div[2]/div/div/div/span')))\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[2]/div/div[2]/div/div/div/span')))\n",
    "    \n",
    "            lat_input_container = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[1]/div/div[2]/div/div/div/span')\n",
    "            lat_input_element = lat_input_container.find_element(By.TAG_NAME, 'input')\n",
    "            lat = float(lat_input_element.get_attribute('value'))\n",
    "\n",
    "            long_input_container = adjust_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div[2]/div[3]/div[1]/div/div[2]/div[2]/div/div/div[2]/div/div/div[3]/div[2]/div/div[2]/div/div/div/span')\n",
    "            long_input_element = long_input_container.find_element(By.TAG_NAME, 'input')\n",
    "            long = float(long_input_element.get_attribute('value'))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find lat/long\")\n",
    "        \n",
    "        print(\"lat : \", lat)\n",
    "        print(\"long : \", long)\n",
    "\n",
    "        # **if can't find lat/long --> don't scrape this attaction\n",
    "        if(lat == 0 and long == 0):\n",
    "            print(\"in scrape_location_latlong_openingHours --> can't find lat/long --> 0, 0\")\n",
    "            return lat, long, openingHours.copy(), types.copy(), facilities.copy()\n",
    "\n",
    "        # find type\n",
    "        try:     \n",
    "            # find restaurant types container\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'IBVyx')))\n",
    "\n",
    "            possible_type_container = adjust_page_driver.find_elements(By.CLASS_NAME, 'IBVyx')\n",
    "            type_container = None\n",
    "            for cur_element in possible_type_container[::-1]:\n",
    "                cur_text = cur_element.text\n",
    "                if(\"หมวดหมู่อาหาร\" in cur_text):\n",
    "                    type_container = cur_element\n",
    "                    WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'RCAPL')))\n",
    "                    all_type_element = type_container.find_elements(By.CLASS_NAME, 'RCAPL')\n",
    "                    for cur_type in all_type_element:\n",
    "                        cur_text = cur_type.text\n",
    "                        types.append(cur_text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find types\")\n",
    "\n",
    "        print(\"types --> \", types)\n",
    "\n",
    "\n",
    "        # find facilities\n",
    "        try:\n",
    "            # find restaurant facilities container\n",
    "            print(\"debug faci 1\")\n",
    "            # AoddJ\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'mowmC')))\n",
    "            print(\"debug faci 2\")\n",
    "            possible_facility_container = adjust_page_driver.find_elements(By.CLASS_NAME, 'mowmC')\n",
    "            print(\"debug faci 3\")\n",
    "            facility_container = None\n",
    "            for cur_element in possible_facility_container[::-1]:\n",
    "                cur_text = cur_element.text\n",
    "                print(\"check facility cur_text --> \", cur_text)\n",
    "\n",
    "                if(\"สิ่งอำนวยความสะดวก\" in cur_text):\n",
    "                    facility_container = cur_element\n",
    "                    print(\"debug faci 4\")\n",
    "                    WebDriverWait(adjust_page_driver, 3).until(EC.visibility_of_element_located((By.CLASS_NAME, 'PMWyE')))\n",
    "                    print(\"debug faci 5\")\n",
    "                    # checkbox_containers = facility_container.find_elements(By.CLASS_NAME, 'PMWyE')\n",
    "                    checkbox_containers = adjust_page_driver.find_elements(By.CLASS_NAME, 'PMWyE')\n",
    "                    # checkbox_containers = facility_container.find_elements(By.CLASS_NAME, 'opEmP')\n",
    "                    print(\"debug faci 6\")\n",
    "                    print(\"check len --> \", len(checkbox_containers))\n",
    "                    for Idx in range(len(checkbox_containers)):\n",
    "                        print(\"debug faci 7\")\n",
    "                        cur_checkbox = checkbox_containers[Idx].find_element(By.TAG_NAME, 'span')\n",
    "                        print(\"debug faci 8\")\n",
    "                        is_check = True if cur_checkbox.get_attribute('class') != 'U' else False\n",
    "                        print(\"debug faci 9\")\n",
    "                        if(is_check):\n",
    "                            print(\"debug faci 10\")\n",
    "                            facilities.append(ALL_RESTAURANTS_TRIPADVISOR_FACILITIES[Idx])\n",
    "\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find facilities\")\n",
    "\n",
    "        print(\"facilities --> \", facilities)\n",
    "\n",
    "\n",
    "        # find openingHours\n",
    "        try:\n",
    "            WebDriverWait(adjust_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'dNAjp')))\n",
    "            all_openingHours_container = adjust_page_driver.find_elements(By.CLASS_NAME, 'dNAjp')\n",
    "            for cur_openingHours_container in all_openingHours_container:\n",
    "                cur_day_element = cur_openingHours_container.find_element(By.CLASS_NAME, 'ngXxk')\n",
    "                cur_day_text = cur_day_element.text.replace(\":\", \"\")\n",
    "\n",
    "                cur_time_element = cur_openingHours_container.find_element(By.CLASS_NAME, 'KxBGd')\n",
    "                cur_time_text = cur_time_element.text\n",
    "\n",
    "                openingHours[cur_day_text] = cur_time_text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find openingHours ...\")\n",
    "\n",
    "        print(\"openingHours : \", openingHours)\n",
    "\n",
    "        time.sleep(50000)\n",
    "\n",
    "        adjust_page_driver.quit()\n",
    "        break\n",
    "\n",
    "    return lat, long, openingHours.copy(), types.copy(), facilities.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_restaurant(link_to_restaurant: str, province_th: str) -> Restaurant:\n",
    "    \n",
    "    restaurant = Restaurant()\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        # if(cnt_retry == 10):\n",
    "        #     print(\"max retry for scrape single restaurant ...\")\n",
    "        #     break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set web browser options to run\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", \n",
    "            {\n",
    "                \"profile.managed_default_content_settings.images\": 2, # Disable image\n",
    "                # \"profile.default_content_setting_values.cookies\": 2,  # Block all cookies\n",
    "                \"profile.default_content_settings.popups\": 0,         # Disable popups\n",
    "                # \"profile.managed_default_content_settings.cookies\": 2  # Disable third-party cookies\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # initialize the web driver with service, selenium-wire options, and web browser options\n",
    "        # update: just change browser to chrome driver since priceRange can not be found in Edge\n",
    "        restaurant_page_driver = webdriver.Chrome(\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            print(\"******************************************************\")\n",
    "            print(\"scrape single restaurant...\")\n",
    "            print(\"for restaurant : \", link_to_restaurant)\n",
    "            restaurant_page_driver.get(link_to_restaurant)\n",
    "            # restaurant_page_driver.add_cookie()\n",
    "\n",
    "            print(\"debug scrape_single_restaurant: top info component section\")\n",
    "            # WebDriverWait(restaurant_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/span/div[4]/div/div[1]/div[3]/div')))\n",
    "            # top_info_container = restaurant_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/span/div[4]/div/div[1]/div[3]/div')\n",
    "\n",
    "            print(\"debug scrape_single_restaurant: bottom info component section\")\n",
    "            WebDriverWait(restaurant_page_driver, 2).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div/div[6]/div/div[2]')))\n",
    "            bottom_info_container = restaurant_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div/div[6]/div/div[2]')\n",
    "\n",
    "            print(\"debug scrape_single_attraction: common component section\")\n",
    "            WebDriverWait(restaurant_page_driver, 2).until(EC.visibility_of_element_located((By.CLASS_NAME, 'IDaDx')))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"retry single restaurant case 1...\")\n",
    "            cnt_retry += 1\n",
    "            restaurant_page_driver.quit()\n",
    "            continue\n",
    "        \n",
    "    \n",
    "        # find name\n",
    "        name = \"\"\n",
    "        try:\n",
    "            WebDriverWait(restaurant_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'rRtyp')))\n",
    "            name_element = restaurant_page_driver.find_element(By.CLASS_NAME, 'rRtyp')\n",
    "            name = name_element.text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find name\")\n",
    "\n",
    "        print(\"name -> \", name)\n",
    "\n",
    "        # find description\n",
    "        # description = \"\"\n",
    "        # try:\n",
    "        #     try:\n",
    "        #         # find button to click readmore (if it exists, it likely to be the first elements of class 'lszDU')\n",
    "        #         WebDriverWait(restaurant_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'lszDU')))\n",
    "        #         click_readmore_btn = restaurant_page_driver.find_element(By.CLASS_NAME, 'lszDU')\n",
    "        #         click_readmore_btn.click()\n",
    "\n",
    "        #     except Exception as e:\n",
    "        #         pass\n",
    "\n",
    "        #     WebDriverWait(restaurant_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'zYHGB')))\n",
    "        #     all_description_elements = restaurant_page_driver.find_elements(By.CLASS_NAME, 'zYHGB')\n",
    "        #     for cur_element in all_description_elements:\n",
    "        #         cur_text =  cur_element.text\n",
    "        #         if(len(cur_text)):\n",
    "        #             description += cur_text + '\\n'\n",
    "            \n",
    "        # except Exception as e:\n",
    "        #     print(\"can't find description\")\n",
    "\n",
    "        # print(\"description -> \", description)\n",
    "        \n",
    "\n",
    "        # find priceRange\n",
    "        priceRange = \"\"\n",
    "        try:\n",
    "            WebDriverWait(restaurant_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'XKqDZ')))\n",
    "            detail_container = restaurant_page_driver.find_element(By.CLASS_NAME, 'XKqDZ')\n",
    "\n",
    "            possible_priceRange_container = detail_container.find_elements(By.TAG_NAME, 'div')\n",
    "            for cur_element in possible_priceRange_container:\n",
    "                cur_text = cur_element.text\n",
    "                print(\"check price cur_text --> \", cur_text)\n",
    "                if(\"ช่วงราคา\" in cur_text):\n",
    "                    priceRange_element = cur_element.find_element(By.CLASS_NAME, 'hmDzD')\n",
    "                    priceRange = priceRange_element.text\n",
    "                    print(\"found priceRange --> \", priceRange)\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find priceRange\")\n",
    "\n",
    "        print(\"priceRange --> \", priceRange)\n",
    "\n",
    "\n",
    "        # find phone\n",
    "        phone = \"\"\n",
    "        try:\n",
    "            WebDriverWait(restaurant_page_driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"lithium-root\"]/main/div/div[3]/div/div[3]/span[2]/span[2]/a')))\n",
    "            phone_element = restaurant_page_driver.find_element(By.XPATH, '//*[@id=\"lithium-root\"]/main/div/div[3]/div/div[3]/span[2]/span[2]/a')\n",
    "            phone_element_href = phone_element.get_attribute('href')\n",
    "            if(\"tel\" in phone_element_href):\n",
    "                phone = phone_element.text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find phone\")\n",
    "\n",
    "        print(\"phone --> \", phone)\n",
    "\n",
    "\n",
    "        # find rating\n",
    "        rating = 0\n",
    "        rating_count = 0\n",
    "        try:\n",
    "            WebDriverWait(restaurant_page_driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'sOyfn')))\n",
    "            rating_container = restaurant_page_driver.find_element(By.CLASS_NAME, 'sOyfn')\n",
    "            \n",
    "            rating_element = rating_container.find_element(By.CLASS_NAME, 'uuBRH')\n",
    "            rating = float(rating_element.text)\n",
    "\n",
    "            rating_count_element = rating_container.find_element(By.CLASS_NAME, 'oXJmt')\n",
    "            rating_count = int(rating_count_element.text.replace(',', '').replace('รีวิว ', '').replace(' รายการ', ''))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"can't find rating and rating_count\")\n",
    "\n",
    "        print(\"rating --> \", rating)\n",
    "        print(\"rating_count --> \", rating_count)\n",
    "\n",
    "\n",
    "        # find img_path\n",
    "        # img_path = scrape_img(restaurant_page_driver)\n",
    "        # print(\"cur img path -> \", img_path)\n",
    "\n",
    "\n",
    "        # convert restaurant url to adjust page url\n",
    "        # for example: from 'https://th.tripadvisor.com/Restaurant_Review-g1210687-d1792735-Reviews-Kwong_Shop_Seafood-Kata_Beach_Karon_Phuket.html' to 'https://th.tripadvisor.com/ImproveListing-d586602.html'\n",
    "        link_to_adjust_page = 'https://th.tripadvisor.com/ImproveListing-%s.html' % (link_to_restaurant.split('-')[2])\n",
    "\n",
    "        # ** find lat/long, location data and openingHours (there are in another page of current restaurant)\n",
    "        # ** if this restaurant not have lat/long\n",
    "        # ** don't continue to scrape\n",
    "        lat, long, openingHours, types, facilities = scrape_adjust_page(\n",
    "            restaurant_page_driver = restaurant_page_driver,\n",
    "            link_to_adjust_page = link_to_adjust_page\n",
    "        )\n",
    "        \n",
    "        # **if can't find lat/long --> don't scrape this attaction\n",
    "        if(lat == 0 and long == 0):\n",
    "            print(\"in scrape_single_restaurant --> can't find lat/long --> don't scrape this restaurant ...\")\n",
    "            restaurant_page_driver.quit()\n",
    "            return Restaurant()\n",
    "        \n",
    "\n",
    "        # find location\n",
    "        location = scrape_location(\n",
    "            restaurant_page_driver = restaurant_page_driver,\n",
    "            latitude = lat,\n",
    "            longitude = long,\n",
    "            province_th = province_th\n",
    "        )\n",
    "        print(\"province :\", location.get_province_code(), location.get_province())\n",
    "        print(\"District :\", location.get_district_code(), location.get_district())\n",
    "        print(\"Address : \", location.get_address())\n",
    "\n",
    "\n",
    "        # set some of \"restaurant\" object properties\n",
    "        restaurant.set_name(name)\n",
    "        # restaurant.set_description(description)\n",
    "        restaurant.set_phone(phone)\n",
    "        restaurant.set_latitude(lat)\n",
    "        restaurant.set_longitude(long)\n",
    "        # restaurant.set_imgPath(img_path)\n",
    "        restaurant.set_website(link_to_restaurant)\n",
    "        restaurant.set_openingHour(openingHours)\n",
    "        restaurant.set_type(types)\n",
    "        restaurant.set_facility(facilities)\n",
    "        restaurant.set_priceRange(priceRange)\n",
    "        restaurant.set_location(\n",
    "            address = location.get_address(),\n",
    "            province = location.get_province(),\n",
    "            district = location.get_district(),\n",
    "            sub_district = location.get_sub_district(),\n",
    "            province_code = location.get_province_code(),\n",
    "            district_code = location.get_district_code(),\n",
    "            sub_district_code = location.get_sub_district_code()\n",
    "        )\n",
    "        restaurant.set_rating(\n",
    "            score = rating,\n",
    "            rating_count = rating_count\n",
    "        )\n",
    "\n",
    "        restaurant_page_driver.quit()\n",
    "        break\n",
    "\n",
    "    return restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_url_by_page(query_url: str, page: int) -> list[str]:\n",
    "\n",
    "    res_url_by_page = []\n",
    "\n",
    "    cnt_retry = 0\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        if(cnt_retry == 10):\n",
    "            print(\"max retry for scrape data by page ...\")\n",
    "            break\n",
    "\n",
    "        # formulate the proxy url with authentication\n",
    "        # os.environ['proxy_port']\n",
    "        proxy_url = f\"http://{os.environ['proxy_username']}:{os.environ['proxy_password']}@{os.environ['proxy_address']}:{os.environ['proxy_port']}\"\n",
    "        \n",
    "        # set selenium-wire options to use the proxy\n",
    "        seleniumwire_options = {\n",
    "            \"proxy\": {\n",
    "                \"http\": proxy_url,\n",
    "                \"https\": proxy_url\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # set Chrome options to run in headless mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--lang=th-TH\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "      \n",
    "        # initialize the Chrome driver with service, selenium-wire options, and chrome options\n",
    "        driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            seleniumwire_options=seleniumwire_options,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        # just check for ip\n",
    "        # print(\"just check for ip :\")\n",
    "        # driver.get(\"https://httpbin.io/ip\")\n",
    "        # print(driver.page_source)\n",
    "\n",
    "        # find group of restaurant on the nth page\n",
    "        all_restaurants_card = []\n",
    "\n",
    "        # retry in case of web restrictions and some elements not loaded\n",
    "        try:\n",
    "            query_url_by_page = convert_url_by_page(\n",
    "                link_to_restaurant = query_url,\n",
    "                page = page\n",
    "            )\n",
    "            driver.get(query_url_by_page)\n",
    "            # scroll and wait for some msec\n",
    "            driver.execute_script('window.scrollBy(0, document.body.scrollHeight)')\n",
    "            \n",
    "            print(\"check current page url --> \", driver.current_url)\n",
    "\n",
    "            # wait for div (each restaurant section) to be present and visible\n",
    "            print(\"b1 part 1\")\n",
    "            print(\"debug get_all_url_by_page: restaurant by one page section\")\n",
    "            WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CLASS_NAME, 'yJIls')))\n",
    "            all_restaurants_card = driver.find_elements(By.CLASS_NAME, 'yJIls')\n",
    "\n",
    "\n",
    "            # check if all accomodation card can get tag a and its attribute for url\n",
    "            print(\"b2\")\n",
    "            print(\"check in loop ...\")\n",
    "            for cur_restaurant_card in all_restaurants_card:\n",
    "\n",
    "                cur_restaurant_url = cur_restaurant_card.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                print(\"cur_restaurant_url : \", cur_restaurant_url)\n",
    "                res_url_by_page.append(cur_restaurant_url)\n",
    "            \n",
    "            driver.quit()\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"retry find get_all_url_by_page ...\")\n",
    "            cnt_retry += 1\n",
    "            driver.quit()\n",
    "            continue\n",
    "\n",
    "    return res_url_by_page.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_restaurants_by_province(page: int, province_url: str, province: str) -> pd.DataFrame:\n",
    "    # res_restaurant_df = pd.DataFrame()\n",
    "    res_restaurant_df = create_restaurant_df(Restaurant())\n",
    "    \n",
    "    cnt_for_debug = 0\n",
    "        \n",
    "    print(\"scraping restaurant | province --> %s | page --> %s\" % (province, page))\n",
    "\n",
    "    all_url_by_page = get_all_url_by_page(query_url = province_url, page = page)\n",
    "\n",
    "    # use data from 'res_get_data_by_page' to retrive data of specific restaurant\n",
    "    for cur_restaurant_url in all_url_by_page:\n",
    "        # just use to limit amount of place --> will be removed \n",
    "        if(cnt_for_debug == 2):\n",
    "            break\n",
    "\n",
    "        # continue scraping data for a specific resgtaurant\n",
    "        cur_restaurant = scrape_single_restaurant(\n",
    "            link_to_restaurant = cur_restaurant_url,\n",
    "            province_th = province\n",
    "        )\n",
    "\n",
    "        cnt_for_debug += 1\n",
    "\n",
    "        # create data frame represent data scrape from current restaurant card\n",
    "        # cur_restaurant_df = create_restaurant_df(restaurant=cur_restaurant)\n",
    "\n",
    "        # concat all data frame result\n",
    "        # res_restaurant_df = pd.concat([res_restaurant_df, cur_restaurant_df])\n",
    "    \n",
    "    return res_restaurant_df.iloc[1:, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping restaurant | province --> ภูเก็ต | page --> 1\n",
      "check current page url -->  https://th.tripadvisor.com/Restaurants-g1215781-oa0-Phuket_Town_Phuket.html\n",
      "b1 part 1\n",
      "debug get_all_url_by_page: restaurant by one page section\n",
      "b2\n",
      "check in loop ...\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d13349831-Reviews-Tantitium_Restaurant-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d15059216-Reviews-Three_Monkeys_Restaurant-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315818-d21238792-Reviews-Day_Night_of_Phuket-Talat_Yai_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d21173913-Reviews-Baikingu_Japanese_Buffet_Garden_Restaurant-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d1759741-Reviews-Blue_Elephant_Cooking_School_Restaurant_Phuket-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315818-d5818564-Reviews-One_Chun_Cafe_and_Restaurant-Talat_Yai_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g297930-d25432456-Reviews-Atrio_Restaurant-Patong_Kathu_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d9722296-Reviews-Surf_and_Turf_by_Soul_Kitchen-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1389361-d1874082-Reviews-Kan_Eang_Pier-Chalong_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d13349831-Reviews-Tantitium_Restaurant-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d23839140-Reviews-Flamingo_Phuket-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315817-d12852621-Reviews-Piset_Restaurant_Phuket-Talat_Nuea_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d21138123-Reviews-Hong_Bao_Ramada_Phuket-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315818-d1172082-Reviews-Tukabkhao_Phuket_Local_Food_Restaurant-Talat_Yai_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1389361-d19622658-Reviews-The_Distillery_Phuket_Home_of_Chalong_Bay-Chalong_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315818-d13307440-Reviews-Amore_Mexican_Tapas_Bar-Talat_Yai_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d13989518-Reviews-Raya_Restaurant_Phuket_Old-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d11810229-Reviews-The_Tent-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d3168082-Reviews-Kopitiam_by_Wilai-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d23682010-Reviews-AKOYA_Star_Lounge-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d27469669-Reviews-Numnum_Cafe_Restaurant_Phuket_Old_Town-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1389361-d2714713-Reviews-Pizzeria_Agli_Amici_da_Michele_Jimmy-Chalong_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1389361-d7267941-Reviews-Baan_Noy_Restaurant-Chalong_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d23708006-Reviews-Sriwara_Bistro_Cafe-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d11810229-Reviews-The_Tent-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d23680636-Reviews-Yon_Ocean_House-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1389361-d25403279-Reviews-Lucha_Cantina_Phuket-Chalong_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d6155935-Reviews-Flavor_Phuket-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d1090092-Reviews-La_Gaetana-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d2218722-Reviews-Tunk_Ka_Cafe-Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1893005-d3148567-Reviews-Laem_Hin_Seafood-Koh_Kaew_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g2315814-d6697901-Reviews-Bollywood_Phuket_Restaurant_Bar-Wichit_Phuket_Town_Phuket.html\n",
      "cur_restaurant_url :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d7107051-Reviews-Bang_Mud_Sea_Food_Phuket_floating_restaurant-Phuket_Town_Phuket.html\n",
      "******************************************************\n",
      "scrape single restaurant...\n",
      "for restaurant :  https://th.tripadvisor.com/Restaurant_Review-g1215781-d13349831-Reviews-Tantitium_Restaurant-Phuket_Town_Phuket.html\n",
      "debug scrape_single_restaurant: top info component section\n",
      "debug scrape_single_restaurant: bottom info component section\n",
      "debug scrape_single_attraction: common component section\n",
      "name ->  Tantitium Restaurant\n",
      "check price cur_text -->  รายละเอียด\n",
      "check price cur_text -->  รายละเอียด\n",
      "check price cur_text -->  \n",
      "check price cur_text -->  \n",
      "check price cur_text -->  \n",
      "check price cur_text -->  \n",
      "check price cur_text -->  \n",
      "check price cur_text -->  \n",
      "check price cur_text -->  \n",
      "check price cur_text -->  ช่วงราคา\n",
      "฿301.00 – ฿1,003.00\n",
      "found priceRange -->  ฿301.00 – ฿1,003.00\n",
      "priceRange -->  ฿301.00 – ฿1,003.00\n",
      "phone -->  +66 81 797 3329\n",
      "rating -->  4.5\n",
      "rating_count -->  74\n",
      "scrape data in adjust restaurant page...\n",
      "for link :  https://th.tripadvisor.com/ImproveListing-d13349831.html\n",
      "debug option of adjust page: \n",
      "found target dropdown btn ...\n",
      "lat :  7.88663\n",
      "long :  98.38574\n",
      "types -->  ['ฟิวชั่น', 'ไทย']\n",
      "debug faci 1\n",
      "debug faci 2\n",
      "debug faci 3\n",
      "check facility cur_text -->  ราคา: ร้านอาหารแห่งนี้ราคาแพงเท่าใด\n",
      "(ไม่บังคับ)\n",
      "check facility cur_text -->  ร้านอาหารแห่งนี้มีคุณลักษณะและบริการและสิ่งอำนวยความสะดวกอะไรบ้าง\n",
      "(ไม่บังคับ)\n",
      "debug faci 4\n",
      "debug faci 5\n",
      "debug faci 6\n",
      "check len -->  6\n",
      "debug faci 7\n",
      "debug faci 8\n",
      "debug faci 9\n",
      "debug faci 7\n",
      "debug faci 8\n",
      "debug faci 9\n",
      "debug faci 10\n",
      "debug faci 7\n",
      "debug faci 8\n",
      "debug faci 9\n",
      "debug faci 10\n",
      "debug faci 7\n",
      "debug faci 8\n",
      "debug faci 9\n",
      "debug faci 10\n",
      "debug faci 7\n",
      "debug faci 8\n",
      "debug faci 9\n",
      "debug faci 7\n",
      "debug faci 8\n",
      "debug faci 9\n",
      "debug faci 10\n",
      "facilities -->  ['Mastercard', 'Visa', 'Wifi ฟรี', 'การรับประทานอาหารแบบส่วนตัว']\n",
      "openingHours :  {'จันทร์': '12:00-00:00', 'อังคาร': '12:00-00:00', 'พุธ': '12:00-00:00', 'พฤหัสบดี': '12:00-00:00', 'ศุกร์': '12:00-00:00', 'เสาร์': '12:00-00:00', 'อาทิตย์': '12:00-00:00'}\n"
     ]
    }
   ],
   "source": [
    "# *** select one province from 'ALL_RESTAURANTS_TRIPADVISOR_DATA'\n",
    "# *** so, change \"Idx_of_region\" everytime when scrape another province\n",
    "Idx_of_region = 0\n",
    "cur_region_data = ALL_RESTAURANTS_TRIPADVISOR_DATA[Idx_of_region]\n",
    "\n",
    "# select first and last page to scrape\n",
    "# but in this module will not have any effect (just some dummy number to use with file name)\n",
    "# will have effect on module \"mulProcess_restaurant_scraping_proxy\"\n",
    "first_page = 444\n",
    "last_page = 888\n",
    "\n",
    "cur_province_en = cur_region_data[0]\n",
    "cur_province_th = cur_region_data[1]\n",
    "cur_province_url = cur_region_data[2]\n",
    "\n",
    "# cur_res_allRestaurants_df = create_restaurant_df(Restaurant())\n",
    "\n",
    "# get dataframe result of all restaurant in current province\n",
    "cur_res_allRestaurants_df = scrape_restaurants_by_province(\n",
    "    page = 1,\n",
    "    province_url = cur_province_url,\n",
    "    province = cur_province_th\n",
    ")\n",
    "\n",
    "# don't forget to remove row with lat/long be zero\n",
    "\n",
    "# remove duplicate restaurant\n",
    "cur_res_allRestaurants_df.drop_duplicates(subset=['name'], inplace=True)\n",
    "# set new index\n",
    "cur_res_allRestaurants_df.set_index(['name'], inplace=True)\n",
    "\n",
    "# create directory to store result of scraping restaurant\n",
    "# for example: 'restaurant_scraping\\res_restaurant_scraping\\res_restaurant_Phuket'\n",
    "createDirectory(fh.STORE_ACCOMM_SCRAPING, os.path.join('res_restaurant_scraping', 'res_restaurant_%s' % (cur_province_en)))\n",
    "\n",
    "# save result dataframe to .csv\n",
    "# for example: 'res_restaurant_Phuket_page_1_44.csv'\n",
    "res_file_name = 'res_restaurant_%s_page_%s_%s.csv' % (cur_province_en, first_page, last_page)\n",
    "res_path = os.path.join(fh.STORE_ACCOMM_SCRAPING, 'res_restaurant_scraping', 'res_restaurant_%s' % (cur_province_en), res_file_name)\n",
    "cur_res_allRestaurants_df.to_csv(res_path, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://th.tripadvisor.com/Hotel_Review-g297930-d315568-Reviews-Phuket_Marriott_Resort_Spa_Merlin_Beach-Patong_Kathu_Phuket.html#/media/315568/?albumid=101&type=0&category=101\n",
    "# https://th.tripadvisor.com/Hotel_Review-g1210687-d1379794-Reviews-Chanalai_Romantica_Resort-Kata_Beach_Karon_Phuket.html#/media/1379794/?albumid=101&type=0&category=101\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
